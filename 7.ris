Provider: American Psychological Association
Database: PsycINFO
Content: application/x-research-info-systems

TY  - THES
DESCRIPTORS  - *Computers;  *Facial Expressions;  *Human Computer Interaction;  *Emotion Recognition;  *Affective Computing; Probability; Robotics; Electronic Learning
ID  - 2022-70650-282
T1  - Incorporating emotion recognition in co-adaptive systems.
A1  - Al-Omair, Osamah M.
VL  - 83
SP  - No Pagination Specified
EP  - No Pagination Specified
Y1  - 2022
CY  - US
PB  - ProQuest Information & Learning
SN  - 0419-4217(Print)
N2  - The collaboration between human and computer systems has grown astronomically over the past few years. The ability of software systems adapting to human's input is critical in the symbiosis of human-system co-adaptation, where human and software-based systems work together in a close partnership to achieve synergetic goals. However, it is not always clear what kinds of human's input should be considered to enhance the effectiveness of human and system co-adaptation. To address this issue, this research describes an approach that focuses on incorporating human emotion to improve human-computer co-adaption. The key idea is to provide a formal framework that incorporates human emotions as a foundation for explainability into co-adaptive systems, especially, how software systems recognize human emotions and adapt the system's behaviors accordingly. Detecting and recognizing optimum human emotion is a first step towards human and computer symbiosis. As the first step of this research, we conduct a comparative review for a number of technologies and methods for emotion recognition. Specifically, testing the detection accuracy of facial expression recognition of different cloud-services, algorithms, and methods.Secondly, we study the application of emotion recognition within the areas of e-learning, robotics, and explainable artificial intelligence (XAI). We propose a formal framework that incorporates human emotions into an adaptive e-learning system, to create a more personalized learning experience for higher quality of learning outcomes. In addition, we propose a framework for a co-adaptive Emotional Support Robot. This human-centric framework adopts a reinforced learning approach where the system assesses its own emotional re-actions.Finally, we present a formal probabilistic framework that incorporates emotion recognition for explanations and predicting human performance in a co-adaptive scenario. We illustrate the operability of our framework using a Decision Support System with a human operator supervising the system's decisions. We model our approach using a Stock Prediction Engine that was developed in our research lab to predict the price direction of a stock. We use probabilistic model checking to determine how complex an explanation needs to be based on how confused the human is for the purpose of improving the system's overall utility. In addition, we conduct a web-based human experiment to measure the effectiveness of incorporating emotions in improving the outcome of a co-adaptive system. Our study shows that considering human emotions in co-adaptive systems' explanation is one of the important factors for improving the overall systems performance and utility functions. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - *Computers
KW  - *Facial Expressions
KW  - *Human Computer Interaction
KW  - *Emotion Recognition
KW  - *Affective Computing
KW  - Probability
KW  - Robotics
KW  - Electronic Learning
ER  -
TY  - JOUR
DESCRIPTORS  - *Face Perception;  *Facial Expressions;  *Sensory Feedback;  *Emotion Recognition; Automatism; Emotionality (Personality); Role Perception
PMID  - 31274394
ID  - 2019-61136-003
T1  - Out of focus: Facial feedback manipulation modulates automatic processing of unattended emotional faces.
JF  - Journal of Cognitive Neuroscience
A1  - Kuehne, Maria
A1  - Siwy, Isabelle
A1  - Zaehle, Tino
A1  - Heinze, Hans-Jochen
A1  - Lobmaier, Janek S.
VL  - 31
SP  - 1631
EP  - 1640
Y1  - 2019
CY  - US
AD  - Kuehne, Maria: Department of Neurology, Otto-von-Guericke-University, Leipziger Str. 44, Magdeburg, Germany, 39120, maria.kuehne@med.ovgu.de
PB  - MIT Press
SN  - 1530-8898(Electronic),0898-929X(Print)
N2  - Facial expressions provide information about an individual’s intentions and emotions and are thus an important medium for nonverbal communication. Theories of embodied cognition assume that facial mimicry and resulting facial feedback plays an important role in the perception of facial emotional expressions. Although behavioral and electrophysiological studies have confirmed the influence of facial feedback on the perception of facial emotional expressions, the influence of facial feedback on the automatic processing of such stimuli is largely unexplored. The automatic processing of unattended facial expressions can be investigated by visual expression-related MMN. The expression-related MMN reflects a differential ERP of automatic detection of emotional changes elicited by rarely presented facial expressions (deviants) among frequently presented facial expressions (standards). In this study, we investigated the impact of facial feedback on the automatic processing of facial expressions. For this purpose, participants (n = 19) performed a centrally presented visual detection task while neutral (standard), happy, and sad faces (deviants) were presented peripherally. During the task, facial feedback was manipulated by different pen holding conditions (holding the pen with teeth, lips, or nondominant hand). Our results indicate that automatic processing of facial expressions is influenced and thus dependent on the own facial feedback. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - *Face Perception
KW  - *Facial Expressions
KW  - *Sensory Feedback
KW  - *Emotion Recognition
KW  - Automatism
KW  - Emotionality (Personality)
KW  - Role Perception
M3  - doi:10.1162/jocn_a_01445
DO  - 10.1162/jocn_a_01445
ER  -
TY  - CHAP
DESCRIPTORS  - *Emotional Responses;  *Face Perception;  *Neurosciences; Robotics
ID  - 2011-30587-002
T1  - What the robot sees, what the human feels: Robotic face detection and the human emotional response.
T2  - Emotional expression: The brain and the face, Vol. 3
T3  - Studies in brain, face, and emotion.
A1  - Montoya, Daniel
A1  - Baker-Oglesbee, Alissa
A1  - Bhattacharya, Sambit
SP  - 43
EP  - 71
Y1  - 2011
CY  - Porto,  Portugal
AD  - Montoya, Daniel: dmontoya@uncfsu.edu
PB  - Edições Universidade Fernando Pessoa
SN  - 978-989-643-084-9 (Paperback)
N2  - New developments in robotics have brought humans and robots interacting in human environments. Research has focused its attention on the development of human-like virtual displays and robotics, while parallel lines of research have focused on the study of human responses to robotic agents with special emphasis in human’s emotional reaction. This chapter explores the intersection between robotics and neurosciences with special emphasis in human-robot interactions (HRI). We briefly present recent innovations in the context of robotic face detection and recognition as well as human physiological and cognitive response to the presence of artificial agents. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - *Emotional Responses
KW  - *Face Perception
KW  - *Neurosciences
KW  - Robotics
ER  -
TY  - JOUR
DESCRIPTORS  - *Child Attitudes;  *Childhood Development;  *Emotions;  *Facial Features;  *Robotics; Facial Expressions; Perception; Posture; Social Robotics
ID  - 2014-10508-001
T1  - Child’s perception of robot’s emotions: Effects of platform, context and experience.
JF  - International Journal of Social Robotics
A1  - Cohen, I.
A1  - Looije, R.
A1  - Neerincx, M. A.
VL  - 6
SP  - 507
EP  - 518
Y1  - 2014
CY  - Germany
AD  - Cohen, I.: Delft University of Technology, Delft, Netherlands, iris.cohen@tno.nl
PB  - Springer
SN  - 1875-4805(Electronic),1875-4791(Print)
N2  - Social robots may comfort and support children who have to cope with chronic diseases like diabetes. In social interactions, it is important to be able to express recognizable emotions. Studies show that the iCat robot, with its humanoid facial features, has this capability. In this paper we look if a Nao robot, without humanoid facial features, but with a body and colored eyes is also able to express recognizable emotions. We compare the recognition rates of the emotions between the Nao and the iCat. First a set of bodily expressions of the Nao for five basic emotions (angry, fear, happy, sad, surprise) was created and evaluated. With a signal detection task, the best recognizable bodily expression for each emotion was chosen for the final set. Then, fourteen children between 8 and 9 years old interacted both with the Nao and iCat to recognize the emotions within context, in a story-telling session, and without context. These interactions were repeated one week later to study the learning effect. For both robots, recognition rates for the expressions were relatively high (between 68 and 99 % accuracy). Only for the emotional state of sadness, the recognition was significantly higher for the iCat (95 %) than for the Nao (68 %). The emotions shown within context had higher recognition rates than those without context and during the second interaction the emotion recognition was also significantly higher than during the first session for both robots. To conclude: we succeeded to design a set of well-recognized dynamic emotional expressions for a robot platform, the Nao, without facial features. These expressions were better recognized when placed in a context, and when shown a week later. This set provides useful ingredients of social robot dialogs with children. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - *Child Attitudes
KW  - *Childhood Development
KW  - *Emotions
KW  - *Facial Features
KW  - *Robotics
KW  - Facial Expressions
KW  - Perception
KW  - Posture
KW  - Social Robotics
M3  - doi:10.1007/s12369-014-0230-6
DO  - 10.1007/s12369-014-0230-6
ER  -
TY  - JOUR
DESCRIPTORS  - *Emotions;  *Facial Expressions;  *Major Depression; Sadness
PMID  - 22370153
ID  - 2012-05509-001
T1  - Sensitivity to posed and genuine facial expressions of emotion in severe depression.
JF  - Psychiatry Research
A1  - Douglas, Katie M.
A1  - Porter, Richard J.
A1  - Johnston, Lucy
VL  - 196
SP  - 72
EP  - 78
Y1  - 2012
CY  - Netherlands
AD  - Douglas, Katie M.: Department of Psychological Medicine, University of Otago, P.O. Box 4345, Christchurch, New Zealand, 8011, katie.douglas@otago.ac.nz
PB  - Elsevier Science
SN  - 1872-7123(Electronic),0165-1781(Print)
N2  - The aim of the current study was to investigate whether the ability to distinguish genuine from non-genuine (neutral or posed) facial expressions of emotion (happiness, sadness, fear and disgust) is impaired in depression, and whether improvement in this ability occurs with treatment response. Sixty-eight depressed inpatients and 50 matched healthy controls performed the Emotion Categorisation Task three times over 6 weeks. All participants showed some sensitivity to the meaningful differences between genuine and non-genuine expressions of emotion, with an increasing percentage of faces labelled as genuinely feeling the emotion from neutral to posed to genuine presentations. Depressed patients showed significantly less sensitivity in differentiating non-genuine from genuine expressions of sadness, compared with healthy controls. Performance on the Emotion Categorisation Task did not change over time in treatment responders compared with treatment non-responders. These findings have implications for understanding why depressed individuals may have difficulties in social interactions. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - *Emotions
KW  - *Facial Expressions
KW  - *Major Depression
KW  - Sadness
M3  - doi:10.1016/j.psychres.2011.10.019
DO  - 10.1016/j.psychres.2011.10.019
ER  -
TY  - JOUR
DESCRIPTORS  - *Intelligent Agents;  *Metaphor;  *Semantics;  *Emotion Recognition; Emotionality (Personality); Gestures
ID  - 2013-37145-003
T1  - Towards a semantic-based approach for affect and metaphor detection.
JF  - International Journal of Distance Education Technologies
A1  - Zhang, Li
A1  - Barnden, John
VL  - 11
SP  - 48
EP  - 65
Y1  - 2013
CY  - US
PB  - IGI Global
SN  - 1539-3119(Electronic),1539-3100(Print)
N2  - Affect detection from open-ended virtual improvisational contexts is a challenging task. To achieve this research goal, the authors developed an intelligent agent which was able to engage in virtual improvisation and perform sentence-level affect detection from user inputs. This affect detection development was efficient for the improvisational inputs with strong emotional indicators. However, it can also be fooled by the diversity of emotional expressions such as expressions with weak or no affect indicators or metaphorical affective inputs. Moreover, since the improvisation often involves multi-party conversations with several threads of discussions happening simultaneously, the previous development was unable to identify the different discussion contexts and the most intended audiences to inform affect detection. Therefore, in this paper, the authors employ latent semantic analysis to find the underlying semantic structures of the emotional expressions and identify topic themes and target audiences especially for those inputs without strong affect indicators to improve affect detection performance. They also discuss how such semantic interpretation of dialog contexts is used to identify metaphorical phenomena. Initial exploration on affect detection from gestures is also discussed to interpret users’ experience of using the system and provide an extra channel to detect affect embedded in the virtual improvisation. Their work contributes to the journal themes on affect sensing from text, semantic-based dialogue processing and emotional gesture recognition. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - *Intelligent Agents
KW  - *Metaphor
KW  - *Semantics
KW  - *Emotion Recognition
KW  - Emotionality (Personality)
KW  - Gestures
M3  - doi:10.4018/jdet.2013040103
DO  - 10.4018/jdet.2013040103
ER  -
TY  - JOUR
DESCRIPTORS  - *Facial Expressions;  *Human Computer Interaction;  *Motion Perception;  *Robotics;  *Tracking; Emotions; Human Robot Interaction
ID  - 2009-18644-004
T1  - Facial expression recognition and tracking for intelligent human-robot interaction.
JF  - Intelligent Service Robotics
A1  - Yang, Y.
A1  - Ge, S. S.
A1  - Lee, T. H.
A1  - Wang, C.
VL  - 1
SP  - 143
EP  - 157
Y1  - 2008
CY  - Germany
AD  - Ge, S. S.: Social Robotics Lab, Interactive Digital Media Institute, National University of Singapore, Singapore, Singapore, 117576, samge@nus.edu.sg
PB  - Springer
SN  - 1861-2784(Electronic),1861-2776(Print)
N2  - For effective interaction between humans and socially adept, intelligent service robots, a key capability required by this class of sociable robots is the successful interpretation of visual data. In addition to crucial techniques like human face detection and recognition, an important next step for enabling intelligence and empathy within social robots is that of emotion recognition. In this paper, an automated and interactive computer vision system is investigated for human facial expression recognition and tracking based on the facial structure features and movement information. Twenty facial features are adopted since they are more informative and prominent for reducing the ambiguity during classification. An unsupervised learning algorithm, distributed locally linear embedding (DLLE), is introduced to recover the inherent properties of scattered data lying on a manifold embedded in high-dimensional input facial images. The selected person dependent facial expression images in a video are classified using the DLLE. In addition, facial expression motion energy is introduced to describe the facial muscle’s tension during the expressions for person-independent tracking for person independent recognition. This method takes advantage of the optical flow which tracks the feature points’ movement information. Finally, experimental results show that our approach is able to separate different expressions successfully. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - *Facial Expressions
KW  - *Human Computer Interaction
KW  - *Motion Perception
KW  - *Robotics
KW  - *Tracking
KW  - Emotions
KW  - Human Robot Interaction
M3  - doi:10.1007/s11370-007-0014-z
DO  - 10.1007/s11370-007-0014-z
ER  -
TY  - JOUR
DESCRIPTORS  - *Empathy;  *Medical Students;  *Psychiatric Evaluation;  *Psychiatric Training;  *Psychodiagnostic Interview; Intelligent Agents; Major Depression; Medical Patients
PMID  - 31818765
ID  - 2020-07413-001
T1  - Evaluation of a virtual agent to train medical students conducting psychiatric interviews for diagnosing major depressive disorders.
JF  - Journal of Affective Disorders
A1  - Dupuy, Lucile
A1  - Micoulaud-Franchi, Jean-Arthur
A1  - Cassoudesalle, Hélène
A1  - Ballot, Orlane
A1  - Dehail, Patrick
A1  - Aouizerate, Bruno
A1  - Cuny, Emmanuel
A1  - de Sevin, Etienne
A1  - Philip, Pierre
VL  - 263
SP  - 1
EP  - 8
Y1  - 2020
CY  - Netherlands
AD  - Dupuy, Lucile: University of Bordeaux, USR 3413 SANPSY Addiction et Neuropsychiatrie, Site Carreire – Zone Nord, Bat 3B, 3rd floor, Bordeaux, France, 33076, Cedex, lucile.dupuy@u-bordeaux.fr
PB  - Elsevier Science
SN  - 1573-2517(Electronic),0165-0327(Print)
N2  - Background: A psychiatric diagnosis involves the physician's ability to create an empathic interaction with the patient in order to accurately extract semiology (i.e., clinical manifestations). Virtual patients (VPs) can be used to train these skills but need to be evaluated in terms of accuracy, and to be perceived positively by users. Methods: We recruited 35 medical students who interacted in a 35-min psychiatric interview with a VP simulating major depressive disorders. Semiology extraction, verbal and non-verbal empathy were measured objectively during the interaction. The students were then debriefed to collect their experience with the VP. Results: The VP was able to simulate the conduction of a psychiatric interview realistically, and was effective to discriminate students depending on their psychiatric knowledge. Results suggest that students managed to keep an emotional distance during the interview and show the added value of emotion recognition software to measure empathy in psychiatry training. Students provided positive feedback regarding pedagogic usefulness, realism and enjoyment in the interaction. Limitations: Our sample was relatively small. As a first prototype, the measures taken by the VP would need improvement (subtler empathic questions, levels of difficulty). The face-tracking technique might induce errors in detecting non-verbal empathy. Conclusion: This study is the first to simulate a realistic psychiatric interview and to measure both skills needed by future psychiatrists: semiology extraction and empathic communication. Results provide evidence that VPs are acceptable by medical students, and highlight their relevance to complement existing training and evaluation tools in the field of affective disorders. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - *Empathy
KW  - *Medical Students
KW  - *Psychiatric Evaluation
KW  - *Psychiatric Training
KW  - *Psychodiagnostic Interview
KW  - Intelligent Agents
KW  - Major Depression
KW  - Medical Patients
M3  - doi:10.1016/j.jad.2019.11.117
DO  - 10.1016/j.jad.2019.11.117
ER  -
TY  - JOUR
DESCRIPTORS  - *Drug Therapy;  *Emotional Regulation;  *Impulsiveness;  *Lithium;  *Risk Taking; Emotional Processing
PMID  - 27256357
ID  - 2016-28003-001
T1  - Effects of the potential lithium-mimetic, ebselen, on impulsivity and emotional processing.
JF  - Psychopharmacology
A1  - Masaki, Charles
A1  - Sharpley, Ann L.
A1  - Cooper, Charlotte M.
A1  - Godlewska, Beata R.
A1  - Singh, Nisha
A1  - Vasudevan, Sridhar R.
A1  - Harmer, Catherine J.
A1  - Churchill, Grant C.
A1  - Sharp, Trevor
A1  - Rogers, Robert D.
A1  - Cowen, Philip J.
VL  - 233
SP  - 2655
EP  - 2661
Y1  - 2016
CY  - Germany
AD  - Cowen, Philip J.: Department of Psychiatry, University of Oxford, Warneford Hospital, Oxford, United Kingdom, OX3 7JX, phil.cowen@psych.ox.ac.uk
PB  - Springer
SN  - 1432-2072(Electronic),0033-3158(Print)
N2  - Rationale: Lithium remains the most effective treatment for bipolar disorder and also has important effects to lower suicidal behaviour, a property that may be linked to its ability to diminish impulsive, aggressive behaviour. The antioxidant drug, ebselen, has been proposed as a possible lithium-mimetic based on its ability in animals to inhibit inositol monophosphatase (IMPase), an action which it shares with lithium. Objectives: The aim of the study was to determine whether treatment with ebselen altered emotional processing and diminished measures of risk-taking behaviour. Methods: We studied 20 healthy participants who were tested on two occasions receiving either ebselen (3600 mg over 24 h) or identical placebo in a double-blind, randomized, cross-over design. Three hours after the final dose of ebselen/placebo, participants completed the Cambridge Gambling Task (CGT) and a task that required the detection of emotional facial expressions (facial emotion recognition task (FERT)). Results: On the CGT, relative to placebo, ebselen reduced delay aversion while on the FERT, it increased the recognition of positive vs negative facial expressions. Conclusions: The study suggests that at the dosage used, ebselen can decrease impulsivity and produce a positive bias in emotional processing. These findings have implications for the possible use of ebselen in the disorders characterized by impulsive behaviour and dysphoric mood. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - *Drug Therapy
KW  - *Emotional Regulation
KW  - *Impulsiveness
KW  - *Lithium
KW  - *Risk Taking
KW  - Emotional Processing
M3  - doi:10.1007/s00213-016-4319-5
DO  - 10.1007/s00213-016-4319-5
ER  -
TY  - JOUR
DESCRIPTORS  - *Corticotropin;  *Depression (Emotion);  *Drug Therapy;  *Human Information Storage;  *Visual Thresholds; Geriatric Patients; Letters (Alphabet)
PMID  - 2993946
ID  - 1986-20380-001
T1  - Early visual information processing in depressive patients treated with ORG 2766 (an ACTH 4–9 analogue).
JF  - Neuropsychobiology
A1  - d'Elia, Giacomo
A1  - Frederiksen, Svend-Otto
A1  - Bengtsson, Bengt-Olof
VL  - 13
SP  - 63
EP  - 68
Y1  - 1985
CY  - Switzerland
PB  - Karger
SN  - 1423-0224(Electronic),0302-282X(Print)
N2  - 21 patients (aged 50–72 yrs) who met Diagnostic and Statistical Manual of Mental Disorders (DSM-III) criteria for dysthymic or major depressive disorder participated in an inter- and intraindividual double-blind comparative study between the adrenocorticotropic hormone (ACTH) 4–9 analog ORG 2766 (80 mg/day for 14 or 28 days) and placebo. Temporal thresholds for detection and recognition of rapidly presented single letters were assessed by a tachistoscopic technique. Results indicate that the peptide had a reducing effect on recognition threshold and appeared to improve automatic, preattentive levels of information processing possibly involved in the transfer of information from the peripheral icon to the cortical center (i.e., from iconic storage to short-term memory). (43 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - *Corticotropin
KW  - *Depression (Emotion)
KW  - *Drug Therapy
KW  - *Human Information Storage
KW  - *Visual Thresholds
KW  - Geriatric Patients
KW  - Letters (Alphabet)
M3  - doi:10.1159/000118164
DO  - 10.1159/000118164
ER  -
TY  - JOUR
DESCRIPTORS  - *Human Computer Interaction;  *Mental Disorders;  *Psychiatric Symptoms;  *Psychological Assessment;  *Virtual Reality; Mental Health; Posttraumatic Stress Disorder
ID  - 2017-04044-011
T1  - Detection and computational analysis of psychological signals using a virtual human interviewing agent.
JF  - Journal of Pain Management
A1  - Rizzo, Albert
A1  - Scherer, Stefan
A1  - DeVault, David
A1  - Gratch, Jonathan
A1  - Artsteui, Ronald
A1  - Hartholt, Arno
A1  - Lucas, Gale
A1  - Marsella, Stacey
A1  - Morbini, Fabrizio
A1  - Nazarian, Angela
A1  - Stratou, Giota
A1  - Traum, David
A1  - Wood, Rachel
A1  - Boberg, Jill
A1  - Morency, Louis-Philippe
VL  - 9
SP  - 311
EP  - 321
Y1  - 2016
CY  - US
AD  - Rizzo, Albert: Institute for Creative Technologies, University of Southern California, 12015 East Waterfront Dr, Playa Vista, CA, US, 90094, arizzo@usc.edu
PB  - Nova Science Publishers, Inc.
SN  - 1939-5914(Print)
N2  - It has long been recognized that facial expressions, body posture/gestures and vocal parameters play an important role in human communication and the implicit signalling of emotion. Recent advances in low cost computer vision and behavioral sensing technologies can now be applied to the process of making meaningful inferences as to user state when a person interacts with a computational device. Effective use of this additive information could serve to promote human interaction with virtual human (VH) agents that may enhance diagnostic assessment. This paper will focus on our current research in these areas within the DARPA-funded "Detection and Computational Analysis of Psychological Signals" project, with specific attention to the SimSensei application use case. SimSensei is a virtual human interaction platform that is able to sense and interpret real-time audiovisual behavioral signals from users interacting with the system. It is specifically designed for health care support and leverages years of virtual human research and development at USC-ICT. The platform enables an engaging face-to-face interaction where the virtual human automatically reacts to the state and inferred intent of the user through analysis of behavioral signals gleaned from facial expressions, body gestures and vocal parameters. Akin to how non-verbal behavioral signals have an impact on human to human interaction and communication, SimSensei aims to capture and infer from user non-verbal communication to improve engagement between a VH and a user. The system can also quantify and interpret sensed behavioral signals longitudinally that can be used to inform diagnostic assessment within a clinical context. (PsycINFO Database Record (c) 2017 APA, all rights reserved)
KW  - *Human Computer Interaction
KW  - *Mental Disorders
KW  - *Psychiatric Symptoms
KW  - *Psychological Assessment
KW  - *Virtual Reality
KW  - Mental Health
KW  - Posttraumatic Stress Disorder
ER  -
TY  - JOUR
DESCRIPTORS  - *Cognition;  *Gerontology;  *Technology; Computational Modeling
ID  - 2016-17846-001
T1  - Cognitively-inspired computing for gerontechnology.
JF  - Cognitive Computation
A1  - Fernández-Caballero, Antonio
A1  - González, Pascual
A1  - Navarro, Elena
VL  - 8
SP  - 297
EP  - 298
Y1  - 2016
CY  - Germany
AD  - Fernández-Caballero, Antonio: Instituto de Investigacion en Informatica de Albacete, Universidad de Castilla-La Mancha, Albacete, Spain, 02071, antonio.fdez@uclm.es
PB  - Springer
SN  - 1866-9964(Electronic),1866-9956(Print)
N2  - This article provides an overview of the papers presented in the special issue Cognitive Computation. This Special Issue focuses on all aspects of cognitive agents, addressed by current practices and future trends in Gerontechnology. These include perception, action, affective and cognitive learning and memory, attention, decision making and control, social cognition, language processing and communication, reasoning, problem solving, and consciousness. The Special Issue on "Cognitively-Inspired Computing for Gerontechnology" attracted numerous papers of the highest quality, of which five were finally accepted. The articles that make up this Special Issue cover most of the initial topics outlined during the call for papers, namely, cognitively-inspired computing for assistive technologies and devices; cognitively-inspired computing for household accident detection; emotion/affect/mood recognition and regulation; personalized ambient adaptation; social/carecognitive agents; intelligent telehealth, telemedicine and communication services; social networks for the elderly; lifelong learning for mental health. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - *Cognition
KW  - *Gerontology
KW  - *Technology
KW  - Computational Modeling
M3  - doi:10.1007/s12559-016-9392-x
DO  - 10.1007/s12559-016-9392-x
ER  -
TY  - CHAP
DESCRIPTORS  - *Coping Behavior;  *Positive Psychology;  *Stress Management;  *Telemedicine;  *Mobile Devices; Empathy; Motor Processes; Social Support; Stress; Technology; Avatars
ID  - 2017-06727-006
T1  - Tools for eMental-health: A coping processor for adaptive and interactive mobile systems for stress management.
T2  - Integrating technology in positive psychology practice.
T3  - Advances in psychology, mental health, and behavioral studies (APMHBS).
A1  - Kavakli, Manolya
A1  - Ranjbartabar, Hedieh
A1  - Maddah, Amir
A1  - Ranjbartabar, Kiumars
SP  - 127
EP  - 160
Y1  - 2016
CY  - Hershey,  PA,  US
PB  - Information Science Reference/IGI Global
SN  - 978-1-4666-9986-1 (Hardcover); 978-1-4666-9987-8 (Digital (undefined format))
N2  - This chapter focuses on how to develop tools for positive technology and more specifically, mobile e-mental health systems using a virtual stress counselor. The main objective is to develop a framework for mobile e-mental health systems reviewing existing e-mental health apps and discussing necessary system requirements. The chapter states that current e-mental health apps do not offer any facilities to promote social interaction between the counselor and the user. The proposed framework requires personalized interactions between a virtual counselor and a student. It provides personalized feedback to reduce stress level and enhances personal stress management strategies. This requires integration of technologies for facial expression detection, speech and emotion recognition as well as other psycho-physiological feedback. A prototype system for e-mental health has been developed and the components of the system architecture have been widely discussed including the need for a coping processor. Finally, conclusions are drawn regarding the tools for positive technology. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - *Coping Behavior
KW  - *Positive Psychology
KW  - *Stress Management
KW  - *Telemedicine
KW  - *Mobile Devices
KW  - Empathy
KW  - Motor Processes
KW  - Social Support
KW  - Stress
KW  - Technology
KW  - Avatars
M3  - doi:10.4018/978-1-4666-9986-1.ch006
DO  - 10.4018/978-1-4666-9986-1.ch006
ER  -
TY  - JOUR
DESCRIPTORS  - *Firearms;  *Individual Differences;  *Judgment;  *Signal Detection (Perception); Personality Traits
PMID  - 33196931
ID  - 2020-87428-001
T1  - Wielding a gun increases judgments of others as holding guns: A randomized controlled trial.
JF  - Cognitive Research: Principles and Implications
A1  - Witt, Jessica K.
A1  - Parnes, Jamie E.
A1  - Tenhundfeld, Nathan L.
VL  - 5
Y1  - 2020
CY  - Germany
AD  - Witt, Jessica K.: Department of Psychology, Colorado State University, Fort Collins, CO, US, 80523, Jessica.Witt@colostate.edu
PB  - Springer
SN  - 2365-7464(Electronic)
N2  - The gun embodiment effect is the consequence caused by wielding a gun on judgments of whether others are also holding a gun. This effect could be responsible for real-world instances when police officers shoot an unarmed person because of the misperception that the person had a gun. The gun embodiment effect is an instance of embodied cognition for which a person’s tool-augmented body affects their judgments. The replication crisis in psychology has raised concern about embodied cognition effects in particular, and the issue of low statistical power applies to the original research on the gun embodiment effect. Thus, the first step was to conduct a high-powered replication. We found a significant gun embodiment effect in participants’ reaction times and in their proportion of correct responses, but not in signal detection measures of bias, as had been originally reported. To help prevent the gun embodiment effect from leading to fatal encounters, it would be useful to know whether individuals with certain traits are less prone to the effect and whether certain kinds of experiences help alleviate the effect. With the new and reliable measure of the gun embodiment effect, we tested for moderation by individual differences related to prior gun experience, attitudes, personality, and factors related to emotion regulation and impulsivity. Despite the variety of these measures, there was little evidence for moderation. The results were more consistent with the idea of the gun embodiment effect being a universal, fixed effect, than being a flexible, malleable effect. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - *Firearms
KW  - *Individual Differences
KW  - *Judgment
KW  - *Signal Detection (Perception)
KW  - Personality Traits
M3  - doi:10.1186/s41235-020-00260-3
DO  - 10.1186/s41235-020-00260-3
ER  -
TY  - JOUR
DESCRIPTORS  - *Adaptive Behavior;  *Artificial Intelligence;  *Machine Learning;  *Robotics;  *Social Behavior; Decision Making; Feedback
ID  - 2016-18302-001
T1  - Adaptive artificial companions learning from users’ feedback.
JF  - Adaptive Behavior
A1  - Karami, Abir B.
A1  - Sehaba, Karim
A1  - Encelle, Benoit
VL  - 24
SP  - 69
EP  - 86
Y1  - 2016
CY  - US
AD  - Karami, Abir B.: Universite Lille Nord de France, Ecole des Mines de Douai, 764 Boulevard Lahure, Douai, France, F-59500, abir.karami@mines-douai.fr
PB  - Sage Publications
SN  - 1741-2633(Electronic),1059-7123(Print)
N2  - Until recently, propositions on the subject of intelligent service companions, like robots, were mostly user and environment independent. Our work is part of the FUI-RoboPopuli project, which concentrates on endowing entertainment companion robots with adaptive and social behavior. More precisely, we focus on the capacity of an intelligent system to learn how to personalize and adapt its behavior/actions according to its interaction situation that describes (a) the current user attributes, and (b) the current environment attributes. Our approach is based on models of the type of Markov decision processes (MDPs) that are largely used for adaptive robot applications. In order to have, as quickly as possible, a relevant adaptive behavior whatever the interaction situation, several approaches were proposed to decrease the sample complexity required to learn the MDP model, including its reward function. We argue that systems that are based on detecting important attributes for each decision are more likely to converge faster than others. To this end, we present two algorithms to learn the MDP reward function through analyzing interaction traces (i.e., the interaction history between the robot and its users including their feedback regarding the robot actions). The first algorithm is direct, certain and does not particularly exploit its knowledge to adapt to unknown situations (i.e., unknown users’ and/or environment settings). The second is able to detect the importance of certain situation attributes in the adaptation process. The detection of important attributes is used to speed up the learning process and helps by generalizing the learned reward function to unknown situations. In this paper, we present both learning algorithms, simulated experiments and an experiment with the EMOX (EMOtion eXchange) robot that was upgraded during the FUI-RoboPopuli project. The results of those experiments prove that when dealing with adaptive decision making, the detection of important attributes for each decision speeds up the learning process and help in achieving convergence using fewer samples. We also present a scaling analysis through the simulated experiments. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - *Adaptive Behavior
KW  - *Artificial Intelligence
KW  - *Machine Learning
KW  - *Robotics
KW  - *Social Behavior
KW  - Decision Making
KW  - Feedback
M3  - doi:10.1177/1059712316634062
DO  - 10.1177/1059712316634062
ER  -
TY  - JOUR
DESCRIPTORS  - *Posture;  *Virtual Reality;  *Visual Displays;  *Multimedia;  *Animation; Facial Expressions
ID  - 2013-42530-005
T1  - The role of body postures in the recognition of emotions in contextually rich scenarios.
JF  - International Journal of Human-Computer Interaction
A1  - Buisine, Stéphanie
A1  - Courgeon, Matthieu
A1  - Charles, Aurélien
A1  - Clavel, Céline
A1  - Martin, Jean-Claude
A1  - Tan, Ning
A1  - Grynszpan, Ouriel
VL  - 30
SP  - 52
EP  - 62
Y1  - 2014
CY  - United Kingdom
AD  - Buisine, Stéphanie: Arts et Metiers ParisTech, LCPI, 151 bd Hopital, Paris, France, 75013, stephanie.buisine@ensam.eu
PB  - Taylor & Francis
SN  - 1532-7590(Electronic),1044-7318(Print)
N2  - In this article the role of different categories of postures in the detection, recognition, and interpretation of emotion in contextually rich scenarios, including ironic items, is investigated. Animated scenarios are designed with 3D virtual agents in order to test 3 conditions: In the “still” condition, the narrative content was accompanied by emotional facial expressions without any body movements; in the "idle" condition, emotionally neutral body movements were introduced; and in the "congruent" condition, emotional body postures congruent with the character’s facial expressions were displayed. Those conditions were examined by 27 subjects, and their impact on the viewers’ attentional and emotional processes was assessed. The results highlight the importance of the contextual information to emotion recognition and irony interpretation. It is also shown that both idle and emotional postures improve the detection of emotional expressions. Moreover, emotional postures increase the perceived intensity of emotions and the realism of the animations. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - *Posture
KW  - *Virtual Reality
KW  - *Visual Displays
KW  - *Multimedia
KW  - *Animation
KW  - Facial Expressions
M3  - doi:10.1080/10447318.2013.802200
DO  - 10.1080/10447318.2013.802200
ER  -
TY  - JOUR
DESCRIPTORS  - *Cognition;  *Schizophrenia;  *Scientific Communication; Neuropsychopharmacology
PMID  - 23706575
ID  - 2013-29974-009
T1  - Comment on "Cognition in schizophrenia: Summary Nice Consultation Meeting 2012".
JF  - European Neuropsychopharmacology
A1  - Goldberg, Terry E.
A1  - Gomar, Jesus J.
VL  - 23
SP  - 788
EP  - 789
Y1  - 2013
CY  - Netherlands
AD  - Goldberg, Terry E.: Psychiatry Research and Litwin Zucker Alzheimer's Disease Center, Feinstein Institute for Medical Research, Hofstra North Shore-LIJ School of Medicine, Manhasset, NY, US, 11030, tgoldber@nshs.edu
PB  - Elsevier Science
SN  - 1873-7862(Electronic),0924-977X(Print)
N2  - Comments on an editorial by David Nutt et al. (see record 2013-29974-004). It has been noted that a treatment that impacts both positive symptoms and cognitive impairments would probably be accused of pseudo-specificity, and thus have a more difficult regulatory approval path. Nutt et al. strongly disagree, as do the current authors. They do not think that it is a coincidence that both symptoms and worsening of cognitive status arise in tandem during a prodromal period. In fact, they think that it is plausible that there is some shared, but yet to be determined pathophysiology and view the latter as an underappreciated “holy grail.” If this is indeed the case, then a treatment might impact both these domains. One area not covered in depth in the report by Nutt et al. relates to a large US based initiative, called CNTRICS that was designed to develop tests that reflect advances in cognitive neuroscience, and as such might more precisely assay neural systems or neurochemical status. (PsycINFO Database Record (c) 2017 APA, all rights reserved)
KW  - *Cognition
KW  - *Schizophrenia
KW  - *Scientific Communication
KW  - Neuropsychopharmacology
M3  - doi:10.1016/j.euroneuro.2013.04.009
DO  - 10.1016/j.euroneuro.2013.04.009
ER  -
TY  - CHAP
DESCRIPTORS  - *Automated Information Processing;  *Emotions;  *Experimentation;  *Intention;  *Robotics; Electroencephalography
ID  - 2016-59877-010
T1  - The status of research into intention recognition.
T2  - Improving the quality of life for dementia patients through progressive detection, treatment, and care.
T3  - Advances in psychology, mental health, and behavioral studies (APMHBS) book series.
A1  - Wang, Luyao
A1  - Li, Chunlin
A1  - Wu, Jinglong
SP  - 201
EP  - 221
Y1  - 2017
CY  - Hershey,  PA,  US
PB  - Medical Information Science Reference/IGI Global
SN  - 978-1-522-50925-7 (Hardcover); 978-1-522-50926-4 (Digital (undefined format))
N2  - In recent years, service robots have been widely used in many fields, especially for assisting the elderly and disabled. For example, the medical care of patients with Alzheimer’s disease has become a worldwide problem. Existing service robots with some intelligence quotient can perform actions that are programmed by a human. However, the robot cannot understand human intentions or communicate with people naturally. Understanding the intent of the service object could allow the robot to provide better service. Therefore, the most critical component of human-computer interactions is intention recognition. There are currently many methods by which intention recognition can be achieved, such as EMG, EOG and EEG. In addition, emotion is one of the important factors during intention recognition, and this has been a breakthrough notion. This chapter summarizes the current status of research into intention recognition and gives a brief description of the relationship between emotion and intention. We hope to provide more ideas for optimizing human-computer interactions. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - *Automated Information Processing
KW  - *Emotions
KW  - *Experimentation
KW  - *Intention
KW  - *Robotics
KW  - Electroencephalography
M3  - doi:10.4018/978-1-5225-0925-7.ch010
DO  - 10.4018/978-1-5225-0925-7.ch010
ER  -
TY  - JOUR
DESCRIPTORS  - *Levodopa;  *Parkinson's Disease;  *Deep Brain Stimulation;  *Subthalamic Nucleus; Emotion Recognition
PMID  - 22944002
ID  - 2012-28014-019
T1  - The combined effect of subthalamic nuclei deep brain stimulation and L-dopa increases emotion recognition in Parkinson’s disease.
JF  - Neuropsychologia
A1  - Mondillon, Laurie
A1  - Mermillod, Martial
A1  - Musca, Serban C.
A1  - Rieu, Isabelle
A1  - Vidal, Tiphaine
A1  - Chambres, Patrick
A1  - Auxiette, Catherine
A1  - Dalens, Hélène
A1  - Marie Coulangeon, Louise
A1  - Jalenques, Isabelle
A1  - Lemaire, Jean-Jacques
A1  - Ulla, Miguel
A1  - Derost, Philippe
A1  - Marques, Ana
A1  - Durif, Franck
VL  - 50
SP  - 2869
EP  - 2879
Y1  - 2012
CY  - Netherlands
AD  - Mondillon, Laurie: LAPSCO, Blaise Pascal University, Clermont-Ferrand, France, 63000, Laurie.Mondillon@univ-savoie.fr
PB  - Elsevier Science
SN  - 1873-3514(Electronic),0028-3932(Print)
N2  - Deep brain stimulation of the subthalamic nucleus (DBS) is a widely used surgical technique to suppress motor symptoms in Parkinson’s disease (PD), and as such improves patients’ quality of life. However, DBS may produce emotional disorders such as a reduced ability to recognize emotional facial expressions (EFE). Previous studies have not considered the fact that DBS and L-dopa medication can have differential, common, or complementary consequences on EFE processing. A thorough way of investigating the effect of DBS and L-dopa medication in greater detail is to compare patients’ performances after surgery, with the two therapies either being administered (‘on’) or not administered (‘off’). We therefore used a four-condition (L-dopa ‘on’/DBS ‘on’, L-dopa ‘on’/DBS ‘off’, L-dopa ‘off’/DBS ‘on’, and L-dopa ‘off’/DBS ‘off’) EFE recognition paradigm and compared implanted PD patients to healthy controls. The results confirmed those of previous studies, yielding a significant impairment in the detection of some facial expressions relative to controls. Disgust recognition was impaired when patients were ‘off’ L-dopa and ‘on’ DBS, and fear recognition impaired when ‘off’ of both therapies. More interestingly, the combined effect of both DBS and L-dopa administration seems much more beneficial for EFE recognition than the separate administration of each individual therapy. We discuss the implications of these findings in the light of the inverted U curve function that describes the differential effects of dopamine level on the right orbitofrontal cortex (OFC). We propose that, while L-dopa could "overdose" in dopamine the ventral stream of the OFC, DBS would compensate for this over-activation by decreasing OFC activity, thereby restoring the necessary OFC–amygdala interaction. Another finding is that, when collapsing over all treatment conditions, PD patients recognized more neutral faces than the matched controls, a result that concurs with embodiment theories. (PsycINFO Database Record (c) 2018 APA, all rights reserved)
KW  - *Levodopa
KW  - *Parkinson's Disease
KW  - *Deep Brain Stimulation
KW  - *Subthalamic Nucleus
KW  - Emotion Recognition
M3  - doi:10.1016/j.neuropsychologia.2012.08.016
DO  - 10.1016/j.neuropsychologia.2012.08.016
ER  -
TY  - JOUR
DESCRIPTORS  - *Facial Expressions;  *Graphical Displays;  *Human Computer Interaction;  *Intelligent Tutoring Systems;  *Emotion Recognition; Algorithms; Avatars
ID  - 2015-09096-002
T1  - Dynamic facial emotion recognition oriented to HCI applications.
JF  - Interacting with Computers
A1  - Pablos, Samuel Marcos
A1  - García-Bermejo, Jaime Gómez
A1  - Casanova, Eduardo Zalama
A1  - López, Joaquín
VL  - 27
SP  - 99
EP  - 119
Y1  - 2015
CY  - United Kingdom
AD  - Pablos, Samuel Marcos: CARTIF Foundation, Division of Robotics and Computer Vision, Parque Tecnologico de Boecillo, 205 Boecillo, Valladolid, Spain, 47151, sammar@cartif.es
PB  - Oxford University Press
SN  - 1873-7951(Electronic),0953-5438(Print)
N2  - As part of a multimodal animated avatar previously presented in Marcos-Pablos et al. ((2010) A realistic, virtual head for human-computer interaction. Interact. Comput., 22, 176–192, ISSN 0953-5438), in this paper we describe a method for dynamic recognition of displayed facial emotions on low-resolution streaming images. First, we address the detection of action units (AUs) of the facial action coding system using active shape models and Gabor filters. Normalized outputs of the AU recognition step are then used as inputs for a neural network that consists of an habituation network plus a competitive network. Both the competitive and the habituation layer use differential equations, thus taking into account the dynamic information of facial expressions through time. Experimental results carried out on live video sequences and on the Cohn-Kanade face database show that the proposed method provides high recognition hit rates. To assess the suitability of the developed emotional recognition system for human–computer interaction applications, it has been successfully integrated in the architecture of an avatar and we have conducted a preliminary experiment on empathy. The experiment showed promising results, as the avatar that made use of the emotional recognition system obtained a clear increase in the positivity of the rating when compared with the same avatar with no emotional response. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - *Facial Expressions
KW  - *Graphical Displays
KW  - *Human Computer Interaction
KW  - *Intelligent Tutoring Systems
KW  - *Emotion Recognition
KW  - Algorithms
KW  - Avatars
M3  - doi:10.1093/iwc/iwt057
DO  - 10.1093/iwc/iwt057
ER  -
TY  - JOUR
DESCRIPTORS  - *Facial Expressions;  *Feedback;  *Emotion Recognition; Cognition
ID  - 2015-45397-004
T1  - Facial feedback affects perceived intensity but not quality of emotional expressions.
JF  - Brain Sciences
A1  - Lobmaier, Janek S.
A1  - Fischer, Martin H.
VL  - 5
SP  - 357
EP  - 368
Y1  - 2015
CY  - Switzerland
AD  - Lobmaier, Janek S.: Institute of Psychology, University of Bern, Fabrikstrasse 8, Bern, Switzerland, 3012, janek.lobmaier@psy.unibe.ch
PB  - Multidisciplinary Digital Publishing Institute
SN  - 2076-3425(Electronic)
N2  - Motivated by conflicting evidence in the literature, we re-assessed the role of facial feedback when detecting quantitative or qualitative changes in others’ emotional expressions. Fifty-three healthy adults observed self-paced morph sequences where the emotional facial expression either changed quantitatively (i.e., sad-to-neutral, neutral-to-sad, happy-to-neutral, neutral-to-happy) or qualitatively (i.e. from sad to happy, or from happy to sad). Observers held a pen in their own mouth to induce smiling or frowning during the detection task. When morph sequences started or ended with neutral expressions we replicated a congruency effect: Happiness was perceived longer and sooner while smiling; sadness was perceived longer and sooner while frowning. Interestingly, no such congruency effects occurred for transitions between emotional expressions. These results suggest that facial feedback is especially useful when evaluating the intensity of a facial expression, but less so when we have to recognize which emotion our counterpart is expressing. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - *Facial Expressions
KW  - *Feedback
KW  - *Emotion Recognition
KW  - Cognition
M3  - doi:10.3390/brainsci5030357
DO  - 10.3390/brainsci5030357
ER  -
TY  - JOUR
DESCRIPTORS  - *Attention;  *Emotions;  *Object Recognition;  *Visual Search;  *Artificial Neural Networks; Perceptual Motor Processes; Robotics
ID  - 2018-17743-001
T1  - Emotional metacontrol of attention: Top-down modulation of sensorimotor processes in a robotic visual search task.
JF  - PLoS ONE
A1  - Belkaid, Marwen
A1  - Cuperlier, Nicolas
A1  - Gaussier, Philippe
VL  - 12
Y1  - 2017
CY  - US
AD  - Belkaid, Marwen: marwen.belkaid@ensea.fr
PB  - Public Library of Science
SN  - 1932-6203(Electronic)
N2  - Emotions play a significant role in internal regulatory processes. In this paper, we advocate four key ideas. First, novelty detection can be grounded in the sensorimotor experience and allow higher order appraisal. Second, cognitive processes, such as those involved in self-assessment, influence emotional states by eliciting affects like boredom and frustration. Third, emotional processes such as those triggered by self-assessment influence attentional processes. Last, close emotion-cognition interactions implement an efficient feedback loop for the purpose of top-down behavior regulation. The latter is what we call 'Emotional Metacontrol'. We introduce a model based on artificial neural networks. This architecture is used to control a robotic system in a visual search task. The emotional metacontrol intervenes to bias the robot visual attention during active object recognition. Through a behavioral and statistical analysis, we show that this mechanism increases the robot performance and fosters the exploratory behavior to avoid deadlocks. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - *Attention
KW  - *Emotions
KW  - *Object Recognition
KW  - *Visual Search
KW  - *Artificial Neural Networks
KW  - Perceptual Motor Processes
KW  - Robotics
M3  - doi:10.1371/journal.pone.0184960
DO  - 10.1371/journal.pone.0184960
ER  -
TY  - JOUR
DESCRIPTORS  - *Amitriptyline;  *Antidepressant Drugs;  *Depression (Emotion);  *Memory;  *Side Effects (Drug); Drug Therapy
PMID  - 6438686
ID  - 1985-26177-001
T1  - Effects of two antidepressants on memory performance in depressed outpatients: A double-blind study.
JF  - Psychopharmacology
A1  - Lamping, Donna L.
A1  - Spring, Bonnie
A1  - Gelenberg, Alan J.
VL  - 84
SP  - 254
EP  - 261
Y1  - 1984
CY  - Germany
PB  - Springer
SN  - 1432-2072(Electronic),0033-3158(Print)
N2  - Outpatients with primary depression received either amitriptyline (50–243 mg/day [n = 9, mean age 37.95 yrs]) or clovoxamine (60–238 mg/day [n = 21, mean age 31.19 yrs]) in a double-blind study of the effects of the 2 antidepressants on memory performance. During the 28-day study period, Ss were administered a signal-detection-recognition-memory task, the Benton Visual Retention Test, and the Wechsler Memory Scale on Days 4, 7, and 28. Results from the signal-detection task suggest an amitriptyline-associated memory impairment. The conventional memory measures failed to detect differential effects on memory of the 2 antidepressants. (44 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - *Amitriptyline
KW  - *Antidepressant Drugs
KW  - *Depression (Emotion)
KW  - *Memory
KW  - *Side Effects (Drug)
KW  - Drug Therapy
M3  - doi:10.1007/BF00427455
DO  - 10.1007/BF00427455
ER  -
TY  - JOUR
DESCRIPTORS  - *Emotions;  *Facial Expressions;  *Feedback; Emotion Recognition
PMID  - 30405497
ID  - 2018-55617-001
T1  - A false trail to follow: Differential effects of the facial feedback signals from the upper and lower face on the recognition of micro-expressions.
JF  - Frontiers in Psychology
A1  - Zeng, Xuemei
A1  - Wu, Qi
A1  - Zhang, Siwei
A1  - Liu, Zheying
A1  - Zhou, Qing
A1  - Zhang, Meishan
VL  - 9
Y1  - 2018
CY  - Switzerland
AD  - Wu, Qi: sandwich624@yeah.net
PB  - Frontiers Media S.A.
SN  - 1664-1078(Electronic)
N2  - Micro-expressions, as fleeting facial expressions, are very important for judging people’s true emotions, thus can provide an essential behavioral clue for lie and dangerous demeanor detection. From embodied accounts of cognition, we derived a novel hypothesis that facial feedback from upper and lower facial regions has differential effects on micro-expression recognition. This hypothesis was tested and supported across three studies. Specifically, the results of Study 1 showed that people became better judges of intense micro-expressions with a duration of 450 ms when the facial feedback from upper face was enhanced via a restricting gel. Additional results of Study 2 showed that the recognition accuracy of subtle micro-expressions was significantly impaired under all duration conditions (50, 150, 333, and 450 ms) when facial feedback from lower face was enhanced. In addition, the results of Study 3 also revealed that blocking the facial feedback of lower face, significantly boosted the recognition accuracy of subtle and intense micro-expressions under all duration conditions (150 and 450 ms). Together, these results highlight the role of facial feedback in judging the subtle movements of micro-expressions. (PsycINFO Database Record (c) 2018 APA, all rights reserved)
KW  - *Emotions
KW  - *Facial Expressions
KW  - *Feedback
KW  - Emotion Recognition
M3  - doi:10.3389/fpsyg.2018.02015
DO  - 10.3389/fpsyg.2018.02015
ER  -
TY  - JOUR
DESCRIPTORS  - *Human Sex Differences;  *Neural Networks;  *Neurology;  *Somatosensory Cortex;  *Transcranial Magnetic Stimulation; Smiles
PMID  - 26211433
ID  - 2015-39377-011
T1  - Gender differences in the neural network of facial mimicry of smiles—An rTMS study.
JF  - Cortex: A Journal Devoted to the Study of the Nervous System and Behavior
A1  - Korb, Sebastian
A1  - Malsert, Jennifer
A1  - Rochas, Vincent
A1  - Rihs, Tonia A.
A1  - Rieger, Sebastian W.
A1  - Schwab, Samir
A1  - Niedenthal, Paula M.
A1  - Grandjean, Didier
VL  - 70
SP  - 101
EP  - 114
Y1  - 2015
CY  - France
AD  - Korb, Sebastian: SISSA, Via Bonomea 265, Trieste, Italy, 34136, skorb@sissa.it
PB  - Elsevier Masson SAS
SN  - 1973-8102(Electronic),0010-9452(Print)
N2  - Under theories of embodied emotion, exposure to a facial expression triggers facial mimicry. Facial feedback is then used to recognize and judge the perceived expression. However, the neural bases of facial mimicry and of the use of facial feedback remain poorly understood. Furthermore, gender differences in facial mimicry and emotion recognition suggest that different neural substrates might accompany the production of facial mimicry, and the processing of facial feedback, in men and women. Here, repetitive transcranial magnetic stimulation (rTMS) was applied to the right primary motor cortex (M1), the right primary somatosensory cortex (S1), or, in a control condition, the vertex (VTX). Facial mimicry of smiles and emotion judgments were recorded in response to video clips depicting changes from neutral or angry to happy facial expressions. While in females rTMS over M1 and S1 compared to VTX led to reduced mimicry and, in the case of M1, delayed detection of smiles, there was no effect of TMS condition for males. We conclude that in female participants M1 and S1 play a role in the mimicry and in the use of facial feedback for accurate processing of smiles. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - *Human Sex Differences
KW  - *Neural Networks
KW  - *Neurology
KW  - *Somatosensory Cortex
KW  - *Transcranial Magnetic Stimulation
KW  - Smiles
M3  - doi:10.1016/j.cortex.2015.06.025
DO  - 10.1016/j.cortex.2015.06.025
ER  -
TY  - JOUR
DESCRIPTORS  - *Parietal Lobe;  *Prefrontal Cortex;  *Reasoning;  *Simulation;  *Medial Prefrontal Cortex; False Beliefs; Theory of Mind
PMID  - 24038516
ID  - 2014-17724-001
T1  - Amygdala excitability to subliminally presented emotional faces distinguishes unipolar and bipolar depression: An fMRI and pattern classification study.
JF  - Human Brain Mapping
A1  - Grotegerd, Dominik
A1  - Stuhrmann, Anja
A1  - Kugel, Harald
A1  - Schmidt, Simone
A1  - Redlich, Ronny
A1  - Zwanzger, Peter
A1  - Rauch, Astrid Veronika
A1  - Heindel, Walter
A1  - Zwitserlood, Pienie
A1  - Arolt, Volker
A1  - Suslow, Thomas
A1  - Dannlowski, Udo
VL  - 35
SP  - 2995
EP  - 3007
Y1  - 2014
CY  - US
AD  - Dannlowski, Udo: Department of Psychiatry, University of Munster, Albert-Schweitzer-Campus 1 A9, Munster, Germany, 48149, dannlow@uni-muenster.de
PB  - John Wiley & Sons
SN  - 1097-0193(Electronic),1065-9471(Print)
N2  - Bipolar disorder and Major depressive disorder are difficult to differentiate during depressive episodes, motivating research for differentiating neurobiological markers. Dysfunctional amygdala responsiveness during emotion processing has been implicated in both disorders, but the important rapid and automatic stages of emotion processing in the amygdala have so far never been investigated in bipolar patients. Methods fMRI data of 22 bipolar depressed patients (BD), 22 matched unipolar depressed patients (MDD), and 22 healthy controls (HC) were obtained during processing of subliminal sad, happy and neutral faces. Amygdala responsiveness was investigated using standard univariate analyses as well as pattern‐recognition techniques to differentiate the two clinical groups. Furthermore, medication effects on amygdala responsiveness were explored. Results All subjects were unaware of the emotional faces. Univariate analysis revealed a significant group × emotion interaction within the left amygdala. Amygdala responsiveness to sad > neutral faces was increased in MDD relative to BD. In contrast, responsiveness to happy > neutral faces showed the opposite pattern, with higher amygdala activity in BD than in MDD. Most of the activation patterns in both clinical groups differed significantly from activation patterns of HC—and therefore represent abnormalities. Furthermore, pattern classification on amygdala activation to sad > happy faces yielded almost 80% accuracy differentiating MDD and BD patients. Medication had no significant effect on these findings. Conclusions Distinct amygdala excitability during automatic stages of the processing of emotional faces may reflect differential pathophysiological processes in BD versus MDD depression, potentially representing diagnosis‐specific neural markers mostly unaffected by current psychotropic medication. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - *Parietal Lobe
KW  - *Prefrontal Cortex
KW  - *Reasoning
KW  - *Simulation
KW  - *Medial Prefrontal Cortex
KW  - False Beliefs
KW  - Theory of Mind
M3  - doi:10.1002/hbm.22380
DO  - 10.1002/hbm.22380
ER  -
TY  - JOUR
DESCRIPTORS  - *Corticosteroids;  *Emotions;  *Facial Expressions;  *Oral Contraceptives; Clinical Trials; Estrogens; Progesterone
PMID  - 25224104
ID  - 2014-39018-001
T1  - Oral contraceptives may alter the detection of emotions in facial expressions.
JF  - European Neuropsychopharmacology
A1  - Hamstra, Danielle A.
A1  - De Rover, Mischa
A1  - De Rijk, Roel H.
A1  - Van der Does, Willem
VL  - 24
SP  - 1855
EP  - 1859
Y1  - 2014
CY  - Netherlands
AD  - Van der Does, Willem: Institute of Psychology, Clinical Psychology Department, Leiden University, Wassenaarseweg 52, Leiden, Netherlands, 2333 AK, vanderdoes@fsw.leidenuniv.nl
PB  - Elsevier Science
SN  - 1873-7862(Electronic),0924-977X(Print)
N2  - A possible effect of oral contraceptives on emotion recognition was observed in the context of a clinical trial with a corticosteroid. Users of oral contraceptives detected significantly fewer facial expressions of sadness, anger and disgust than non-users. This was true for trial participants overall as well as for those randomized to placebo. Although it is uncertain whether this is an effect of oral contraceptives or a pre-existing difference, future studies on the effect of interventions should control for the effects of oral contraceptives on emotional and cognitive outcomes. (PsycINFO Database Record (c) 2017 APA, all rights reserved)
KW  - *Corticosteroids
KW  - *Emotions
KW  - *Facial Expressions
KW  - *Oral Contraceptives
KW  - Clinical Trials
KW  - Estrogens
KW  - Progesterone
M3  - doi:10.1016/j.euroneuro.2014.08.015
DO  - 10.1016/j.euroneuro.2014.08.015
ER  -