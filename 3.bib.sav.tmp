@Misc{,
  author    = {Zhang, Kunpeng and Xie, Yusheng and Yang, Yi and Sun, Aaron and Liu, Hengchang and Choudhary, Alok},
  title     = {Incorporating conditional random fields and active learning to improve sentiment identification.},
  doi       = {10.1016/j.neunet.2014.04.005},
  abstract  = {Many machine learning, statistical, and computational linguistic methods have been developed to identify sentiment of sentences in documents, yielding promising results. However, most of state-of-the-art methods focus on individual sentences and ignore the impact of context on the meaning of a sentence. In this paper, we propose a method based on conditional random fields to incorporate sentence structure and context information in addition to syntactic information for improving sentiment identification. We also investigate how human interaction affects the accuracy of sentiment labeling using limited training data. We propose and evaluate two different active learning strategies for labeling sentiment data. Our experiments with the proposed approach demonstrate a 5%-15% improvement in accuracy on Amazon customer reviews compared to existing supervised learning and rule-based methods. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Xie, Yusheng: yxi389@eecs.northwestern.edu},
  issn      = {1879-2782(Electronic),0893-6080(Print)},
  journal   = {Neural Networks},
  keywords  = {*Internet, *Linguistics, *Machine Learning, *Sentence Structure, *Verbal Meaning, Consumer Psychology, Emotional Content, Labeling, Semantics, Sentences, Syntax, Computational Modeling, Social Media, Sentiment Analysis},
  pages     = {60--67},
  publisher = {Elsevier Science},
  refid     = {2014-21666-001},
  volume    = {58},
  year      = {2014},
}

@Misc{,
  author    = {Paltoglou, Georgios and Thelwall, Mike},
  title     = {Sensing social media: A range of approaches for sentiment analysis.},
  doi       = {10.1007/978-3-319-43639-5_6},
  abstract  = {In this chapter, we discuss a range of different approaches to solve the problem of accurately predicting the nature of private states expressed in social media. Section 6.2 will focus on machine learning solutions, i.e., solutions that require some pre-annotated data to automatically extract the underlying patterns that characterise different affective content. Section 6.3 will present the lexicon-based solutions that were investigated within the project, that is, algorithms that rely on sentiment dictionaries. Lastly, the chapter concludes with a summary and a discussion of the potential future directions of the field in Sect. 6.4. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Paltoglou, Georgios: School of Mathematics and Computer Science, University of Wolverhampton, Wulfruna Street, Wolverhampton, United Kingdom, WV1 1LY, g.paltoglou@wlv.ac.uk},
  issn      = {978-3-319-43637-1 (Hardcover); 978-3-319-43639-5 (Digital (undefined format))},
  journal   = {Cyberemotions: Collective emotions in cyberspace.},
  keywords  = {*Emotions, *Machine Learning, *Mental Lexicon, *Social Media, *Sentiment Analysis, Text Structure},
  pages     = {97--117},
  publisher = {Springer International Publishing},
  refid     = {2016-55885-006},
  series    = {Understanding complex systems.},
  year      = {2017},
}

@Misc{,
  author    = {Bens, Jonas},
  title     = {The ethnography of affect in discourse practice: Performing sentiment in the time machine.},
  doi       = {10.4324/9780429424366-11},
  abstract  = {An ethnography of discourse practice investigates the workings of discourse practice events through participant observation, writing about this as 'thickly' as possible. This chapter proposes an ethnographic approach to discourse practice as it is embedded in affective dynamics. At the heart of such ethnographies is the observation of discourse practice events--most broadly defined as a specific place in the world and a specific moment in time in which people communicate in order to be heard by a public. Such events have usually been investigated with a focus on language and speech. Without abandoning the analysis of talk, the chapter makes the argument that it is crucial to strive for an ethnographic investigation that captures quite broadly the affective dimension of the events in which such talk takes place. To that end, it lays out what the author means by an ethnography of discourse practice and then argues two points on methodology. The chapter begins by discussing discourse practice events and describes as affective arrangements. It also describes discourse practice events as time machines in which sentiments become manifest, are enacted, invoked, mobilized, shaped and transformed. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {New York, NY, US},
  issn      = {9781138388796 (Hardcover); 978-0-429-42436-6 (Digital (undefined format))},
  journal   = {Analyzing affective societies: Methods and methodologies.},
  keywords  = {*Discourse Analysis, *Emotions, *Ethnography, *Language, *Oral Communication, Methodology},
  pages     = {199--213},
  publisher = {Routledge/Taylor & Francis Group},
  refid     = {2019-21155-011},
  series    = {Routledge studies in affective societies.},
  year      = {2019},
}

@Misc{,
  author    = {Calix, Ricardo A. and Javadpour, Leili and Knapp, Gerald M.},
  title     = {Detection of affective states from text and speech for real-time human-computer interaction.},
  doi       = {10.1177/0018720811425922},
  abstract  = {Objective: The goal of this work is to develop and test an automated system methodology that can detect emotion from text and speech features. Background: Affective human-computer interaction will be critical for the success of new systems that will be prevalent in the 21st century. Such systems will need to properly deduce human emotional state before they can determine how to best interact with people. Method: Corpora and machine learning classification models are used to train and test a methodology for emotion detection. The methodology uses a stepwise approach to detect sentiment in sentences by first filtering out neutral sentences, then distinguishing among positive, negative, and five emotion classes. Results: Results of the classification between emotion and neutral sentences achieved recall accuracies as high as 77% in the University of Illinois at Urbana-Champaign (UIUC) corpus and 61% in the Louisiana State University medical drama (LSU-MD) corpus for emotion samples. Once neutral sentences were filtered out, the methodology achieved accuracy scores for detecting negative sentences as high as 92.3%. Conclusion: Results of the feature analysis indicate that speech spectral features are better than speech prosodic features for emotion detection. Accumulated sentiment composition text features appear to be very important as well. This work contributes to the study of human communication by providing a better understanding of how language factors help to best convey human emotion and how to best automate this process. Application: Results of this study can be used to develop better automated assistive systems that interpret human language and respond to emotions through 3-D computer graphics. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Knapp, Gerald M.: Louisiana State University, 3128 Patrick F. Taylor Hall, Baton Rouge, LA, US, 70803, gknapp@lsu.edu},
  issn      = {1547-8181(Electronic),0018-7208(Print)},
  journal   = {Human Factors},
  keywords  = {*Emotional States, *Human Computer Interaction, *Speech Perception, *Text Analysis, Automated Speech Recognition, Emotions},
  pages     = {530--545},
  publisher = {Sage Publications},
  refid     = {2012-18918-005},
  volume    = {54},
  year      = {2012},
}

@Misc{,
  author    = {Cambria, Erik and Hussain, Amir},
  title     = {Sentic computing: A common-sense-based framework for concept-level sentiment analysis.},
  doi       = {10.1007/978-3-319-23654-4},
  abstract  = {This volume presents a knowledge-based approach to concept-level sentiment analysis at the crossroads between affective computing, information extraction, and commonsense reasoning, which exploits both computer and human sciences to better interpret and process social information on the Web. Concept-level sentiment analysis goes beyond a mere word-level analysis of text in order to enable a more efficient passage from (unstructured) textual information to (structured) machine-processable data, in potentially any domain. Readers will discover the following key novelties, that make this approach so unique and avant-garde, being reviewed and discussed: Sentic Computing's multi-disciplinary approach to sentiment analysis--evidenced by the concomitant use of AI, linguistics and psychology for knowledge representation and reasoning; Sentic Computing's shift from syntax to semantics--enabled by the adoption of the bag-of-concepts model instead of simply counting word co-occurrence frequencies in text; and Sentic Computing's shift from statistics to linguistics--implemented by allowing sentiments to flow from concept to concept based on the dependency relation between clauses. This volume is the first in the Series Socio-Affective Computing edited by Prof Amir Hussain and Dr Erik Cambria and will be of interest to researchers in the fields of socially intelligent, affective and multimodal human-machine interaction and systems. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Cham, Switzerland},
  issn      = {978-3-319-23653-7 (Hardcover); 978-3-319-23654-4 (Digital (undefined format))},
  journal   = {Sentic computing: A common-sense-based framework for concept-level sentiment analysis.},
  keywords  = {*Artificial Intelligence, *Emotions, *Human Computer Interaction, *Human Machine Systems Design, *Models, Concepts, Social Processes, Sentiment Analysis},
  pages     = {xxii, 176--xxii, 176},
  publisher = {Springer International Publishing},
  refid     = {2016-02101-000},
  series    = {Socio-affective computing.},
  year      = {2015},
}

@Misc{,
  author    = {Sarkar, Kamal},
  title     = {A stacked ensemble approach to Bengali sentiment analysis.},
  doi       = {10.1007/978-3-030-44689-5_10},
  abstract  = {Sentiment analysis is a crucial step in the social media data analysis. The majority of research works on sentiment analysis focus on sentiment polarity detection which identifies whether an input text is positive, negative or neutral. In this paper, we have implemented a stacked ensemble approach to sentiment polarity detection in Bengali tweets. The basic concept of stacked generalization is to fuse the outputs of the first level base classifiers using a second-level Meta classifier in an ensemble. In our ensemble method, we have used two types of base classifiers- multinomial Naïve Bayes classifiers and SVM that make use of a diverse set of features. Our proposed approach shows an improvement over some existing Bengali sentiment analysis approaches reported in the literature. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Sarkar, Kamal: Computer Science and Engineering Department, Jadavpur University, Kolkata, India, 700032, jukamal2001@yahoo.com},
  issn      = {978-3-030-44688-8 (Hardcover); 978-3-030-44689-5 (Digital (undefined format))},
  journal   = {Intelligent Human Computer Interaction: 11th international conference, IHCI 2019, Allahabad, India, December 12-14, 2019, proceedings.},
  keywords  = {*Machine Learning, *Social Media, Sentiment Analysis},
  pages     = {102--111},
  publisher = {Springer Nature Switzerland AG},
  refid     = {2020-89396-011},
  series    = {Lecture notes in computer science.},
  year      = {2020},
}

@Misc{,
  author    = {Liu, Bowen and Xing, Wanli and Zeng, Yifang and Wu, Yonghe},
  title     = {Quantifying the influence of achievement emotions for student learning in MOOCs.},
  doi       = {10.1177/0735633120967318},
  abstract  = {Massive Open Online Courses (MOOCs) have become a popular tool for worldwide learners. However, a lack of emotional interaction and support is an important reason for learners to abandon their learning and eventually results in poor learning performance. This study applied an integrative framework of achievement emotions to uncover their holistic influence on students’ learning by analyzing more than 400,000 forum posts from 13 MOOCs. Six machine-learning models were first built to automatically identify achievement emotions, including K-Nearest Neighbor, Logistic Regression, Naïve Bayes, Decision Tree, Random Forest, and Support Vector Machines. Results showed that Random Forest performed the best with a kappa of 0.83 and an ROC_AUC of 0.97. Then, multilevel modeling with the “Stepwise Build-up” strategy was used to quantify the effect of achievement emotions on students’ academic performance. Results showed that different achievement emotions influenced students’ learning differently. These findings allow MOOC platforms and instructors to provide relevant emotional feedback to students automatically or manually, thereby improving their learning in MOOCs. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Xing, Wanli: College of Education, University of Florida, 2-215, Normal Hall, Gainesville, FL, US, 32611, wanli.xing@coe.ufl.edu},
  issn      = {1541-4140(Electronic),0735-6331(Print)},
  journal   = {Journal of Educational Computing Research},
  keywords  = {*Academic Achievement, *Distance Education, *Educational Programs, *Open Classroom Method, *School Learning, Emotions, Machine Learning, Simulation, Sentiment Analysis},
  pages     = {429--452},
  publisher = {Sage Publications},
  refid     = {2021-44943-003},
  volume    = {59},
  year      = {2021},
}

@Misc{,
  author    = {Das, Dipankar and Bandyopadhyay, Sivaji},
  title     = {Sentence-level emotion and valence tagging.},
  doi       = {10.1007/s12559-012-9173-0},
  abstract  = {The paper proposes the tagging of sentence-level emotion and valence based on the word-level constituents on the SemEval 2007 affect sensing news corpus. The baseline system for each emotion class assigns the class label to each word, while the WordNet Affect lists updated using the SentiWordNet were also used as the lexicon-based system. Though the inclusion of morphology into the lexicon-based system improves the performance of the word-level emotion tagging, the Conditional Random Field-based machine-learning framework was employed for the word-level emotion-tagging system, and it outperforms both the baseline- and lexicon-based systems. Six separate sense scores for six emotion types are calculated from the SentiWordNet and applied to word-level emotion tagged constituents for identifying sentential emotion scores. Three emotion scoring methods followed by a post-processing technique were employed for identifying the sentence-level emotion tags. In addition to that, the best two emotion tags corresponding to the maximum obtained sense scores are assigned to the sentences, whereas the sentence-level valence is identified based on the total sense scores of the word-level emotion tags along with their polarity. Evaluation was carried out with respect to the best two emotion tags on 250 gold standard test sentences and achieved satisfactory results for sentence-level emotion and valence tagging. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Das, Dipankar: Department of Computer Science and Engineering, Jadavpur University, Kolkata, India, dipankar.dipnil2005@gmail.com},
  issn      = {1866-9964(Electronic),1866-9956(Print)},
  journal   = {Cognitive Computation},
  keywords  = {*Expert Systems, *Machine Learning, Emotions, Sentences, Affective Valence},
  pages     = {420--435},
  publisher = {Springer},
  refid     = {2012-32116-005},
  volume    = {4},
  year      = {2012},
}

@Misc{,
  author    = {Provoost, Simon and Ruwaard, Jeroen and van Breda, Ward and Riper, Heleen and Bosse, Tibor},
  title     = {Validating automated sentiment analysis of online cognitive behavioral therapy patient texts: An exploratory study.},
  doi       = {10.3389/fpsyg.2019.01065},
  abstract  = {Introduction: Sentiment analysis may be a useful technique to derive a user’s emotional state from free text input, allowing for more empathic automated feedback in online cognitive behavioral therapy (iCBT) interventions for psychological disorders such as depression. As guided iCBT is considered more effective than unguided iCBT, such automated feedback may help close the gap between the two. The accuracy of automated sentiment analysis is domain dependent, and it is unclear how well the technology is applicable to iCBT. This paper presents an empirical study in which automated sentiment analysis by an algorithm for the Dutch language is validated against human judgment. Methods: A total of 493 iCBT user texts were evaluated on overall sentiment and the presence of five specific emotions by an algorithm, and by 52 psychology students who evaluated 75 randomly selected texts each, providing about eight human evaluations per text. Inter-rater agreement (IRR) between algorithm and humans, and humans among each other, was analyzed by calculating the intra-class correlation under a numerical interpretation of the data, and Cohen’s kappa, and Krippendorff’s alpha under a categorical interpretation. Results: All analyses indicated moderate agreement between the algorithm and average human judgment with respect to evaluating overall sentiment, and low agreement for the specific emotions. Somewhat surprisingly, the same was the case for the IRR among human judges, which means that the algorithm performed about as well as a randomly selected human judge. Thus, considering average human judgment as a benchmark for the applicability of automated sentiment analysis, the technique can be considered for practical application. Discussion/Conclusion: The low human-human agreement on the presence of emotions may be due to the nature of the texts, it may simply be difficult for humans to agree on the presence of the selected emotions, or perhaps trained therapists would have reached more consensus. Future research may focus on validating the algorithm against a more solid benchmark, on applying the algorithm in an application in which empathic feedback is provided, for example, by an embodied conversational agent, or on improving the algorithm for the iCBT domain with a bottom-up machine learning approach. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Provoost, Simon: s.j.provoost@vu.nl},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Algorithms, *Cognitive Behavior Therapy, *Emotions, *Feedback, *Sentiment Analysis, Empathy, Judgment, Online Therapy},
  publisher = {Frontiers Media S.A.},
  refid     = {2019-29859-001},
  volume    = {10},
  year      = {2019},
}

@Misc{,
  author    = {Brady, William J. and McLoughlin, Killian and Crockett, M. J.},
  title     = {Theory‑driven measurement of emotion (expressions) in social media text.},
  abstract  = {With over 3 billion users across the world, social media platforms provide researchers with an opportunity to study a massive volume of emotion expressions unfolding in real-time social interactions. As new computational tools are becoming available, social media data increasingly afford researchers the ability to study psychological questions about emotions at an unprecedented scale. This chapter provides psychologists with a practical guide for making use of advances in sentiment analysis for the purpose of measuring specific emotions in social media text, and ultimately testing hypotheses. It begins with an examination of the type of emotion related information researchers can expect to measure in social media text. Next, it presents a practical guide for the theory-driven application of supervised machine learning to measure specific emotion expressions on social media using the measurement of moral outrage expression as a case study. The chapter ends with considerations for researchers who wish to apply sentiment analysis of social media text for the purposes of psychological hypothesis testing. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {New York, NY, US},
  issn      = {978-1-4625-4843-9 (Hardcover)},
  journal   = {Handbook of language analysis in psychology.},
  keywords  = {*Discourse Analysis, *Expressed Emotion, *Social Media, *Text Messaging, *Sentiment Analysis, Hypothesis Testing, Machine Learning, Psychologists, Social Interaction},
  pages     = {377--388},
  publisher = {The Guilford Press},
  refid     = {2022-08338-019},
  year      = {2022},
}

@Misc{,
  author    = {Poria, Soujanya and Cambria, Erik and Winterstein, Grégoire and Huang, Guang-Bin},
  title     = {Sentic patterns: Dependency-based rules for concept-level sentiment analysis.},
  doi       = {10.1016/j.knosys.2014.05.005},
  abstract  = {The Web is evolving through an era where the opinions of users are getting increasingly important and valuable. The distillation of knowledge from the huge amount of unstructured information on the Web can be a key factor for tasks such as social media marketing, branding, product positioning, and corporate reputation management. These online social data, however, remain hardly accessible to computers, as they are specifically meant for human consumption. The automatic analysis of online opinions involves a deep understanding of natural language text by machines, from which we are still very far. To this end, concept-level sentiment analysis aims to go beyond a mere word-level analysis of text and provide novel approaches to opinion mining and sentiment analysis that enable a more efficient passage from (unstructured) textual information to (structured) machine-processable data. A recent knowledge-based technology in this context is sentic computing, which relies on the ensemble application of common-sense computing and the psychology of emotions to infer the conceptual and affective information associated with natural language. Sentic computing, however, is limited by the richness of the knowledge base and by the fact that the bag-of-concepts model, despite more sophisticated than bag-of-words, misses out important discourse structure information that is key for properly detecting the polarity conveyed by natural language opinions. In this work, we introduce a novel paradigm to concept-level sentiment analysis that merges linguistics, common-sense computing, and machine learning for improving the accuracy of tasks such as polarity detection. By allowing sentiments to flow from concept to concept based on the dependency relation of the input sentence, in particular, we achieve a better understanding of the contextual role of each concept within the sentence and, hence, obtain a polarity detection engine that outperforms state-of-the-art statistical methods. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Cambria, Erik: cambria@ntu.edu.sg},
  issn      = {1872-7409(Electronic),0950-7051(Print)},
  journal   = {Knowledge-Based Systems},
  keywords  = {*Algorithms, *Artificial Intelligence, *Linguistics, *Machine Learning, *Natural Language, Concepts, Sentiment Analysis, Natural Language Processing},
  pages     = {45--63},
  publisher = {Elsevier Science},
  refid     = {2014-34186-001},
  volume    = {69},
  year      = {2014},
}

@Misc{,
  author    = {Kagan, Vadim and Rossini, Edward and Sapounas, Demetrios},
  title     = {Sentiment analysis for PTSD signals.},
  doi       = {10.1007/978-1-4614-3097-1},
  abstract  = {The book provides background information on PTSD and related psychological signals; details the technology developed, the data flows, the processing and results; and a sample system implementation. More specifically the subsequent chapters cover: (1) An introduction to PTSD that will explain the notion of PTSD-related psychological signals, and will also present the categorization of PTSD symptoms, the sources and methodologies used to unify clinical and colloquial terms into a PTSD ontology, and the resulting ontology. (2) A description of the selection of data sources serving as inputs to the system for training and testing the text analysis algorithms. This section will also cover the selection criteria applied to web forums and blogs, and will explain the role the materials from the psychological library play in the project. Further, the data collection and pre-processing workflow before the data is stored in a database and submitted to the text analysis engine for processing will be described. (3) As part of the discussion on text analysis of PTSD text, a description of the general approach taken with the extraction and quantification of PTSD-related signals with an overview of the relevant natural language processing techniques, focusing on sentiment mining, and the role of the annotated corpus. Additionally, the human annotation process and tools developed for creating algorithm training and testing data sets will be outlined. (4) An overview of the SentiMetrix® SentiGrade™ scoring engine, and a description of the enhancements made to the engine and the training that was necessary to tune it for the detection of PTSD-related signals. (5) A sample system implementation integrating all the tools into a cohesive environment, implementing an automated end-to-end process, including social networking features used for collecting data from anonymous user participation. The system architecture, including the data flow and feedback loops, as well as the reports generated by the system will also be outlined. 6. Finally, the project findings are presented. These findings compare and contrast the results produced by the automated system with evaluation of the same anonymous data set by a team of clinical psychologists. The analysis presents strong supporting evidence of viability of automated detection of psychological signals associated with PTSD. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {New York, NY, US},
  issn      = {978-1-4614-3096-4 (Paperback); 978-1-4614-3097-1 (PDF)},
  journal   = {Sentiment analysis for PTSD signals.},
  keywords  = {*Automated Information Processing, *Emotional Content, *Human Machine Systems Design, *Posttraumatic Stress Disorder, *Sentiment Analysis, Data Collection, Ontology (Philosophy), Psychodynamics, Symptoms, Technology},
  pages     = {x, 81--x, 81},
  publisher = {Springer Science + Business Media},
  refid     = {2013-40929-000},
  series    = {Springer briefs in computer science.},
  year      = {2013},
}

@Article{,
  author    = {Ratajczyk, Dawid},
  title     = {Shape of the uncanny valley and emotional attitudes toward robots assessed by an analysis of youtube comments.},
  doi       = {10.1007/s12369-022-00905-x},
  issn      = {1875-4805(Electronic),1875-4791(Print)},
  pages     = {No Pagination Specified--No Pagination Specified},
  abstract  = {The uncanny valley hypothesis (UVH) suggests that almost, but not fully, humanlike artificial characters elicit a feeling of eeriness or discomfort in observers. This study used Natural Language Processing of YouTube comments to provide ecologically-valid, non-laboratory results about people’s emotional reactions toward robots. It contains analyses of 224,544 comments from 1515 videos showing robots from a wide humanlikeness spectrum. The humanlikeness scores were acquired from the Anthropomorphic roBOT database. The analysis showed that people use words related to eeriness to describe very humanlike robots. Humanlikeness was linearly related to both general sentiment and perceptions of eeriness---more humanlike robots elicit more negative emotions. One of the subscales of humanlikeness, Facial Features, showed a UVH-like relationship with both sentiment and eeriness. The exploratory analysis demonstrated that the most suitable words for measuring the self-reported uncanny valley effect are: ‘scary’ and ‘creepy’. In contrast to theoretical expectations, the results showed that humanlikeness was not related to either pleasantness or attractiveness. Finally, it was also found that the size of robots influences sentiment toward the robots. According to the analysis, the reason behind this is the perception of smaller robots as more playable (as toys), although the prediction that bigger robots would be perceived as more threatening was not supported. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Ratajczyk, Dawid: dawid.ratajczyk@amu.edu.pl},
  journal   = {International Journal of Social Robotics},
  publisher = {Springer},
  refid     = {2022-92814-001},
  year      = {2022},
}

@Misc{,
  author    = {Carrillo-de-Albornoz, Jorge and Rodríguez Vidal, Javier and Plaza, Laura},
  title     = {Feature engineering for sentiment analysis in e-health forums.},
  doi       = {10.1371/journal.pone.0207996},
  abstract  = {Introduction: Exploiting information in health-related social media services is of great interest for patients, researchers and medical companies. The challenge is, however, to provide easy, quick and relevant access to the vast amount of information that is available. One step towards facilitating information access to online health data is opinion mining. Even though the classification of patient opinions into positive and negative has been previously tackled, most works make use of machine learning methods and bags of words. Our first contribution is an extensive evaluation of different features, including lexical, syntactic, semantic, network-based, sentiment-based and word embeddings features to represent patient-authored texts for polarity classification. The second contribution of this work is the study of polar facts (i.e. objective information with polar connotations). Traditionally, the presence of polar facts has been neglected and research in polarity classification has been bounded to opinionated texts. We demonstrate the existence and importance of polar facts for the polarity classification of health information. Material and methods: We annotate a set of more than 3500 posts to online health forums of breast cancer, crohn and different allergies, respectively. Each sentence in a post is manually labeled as “experience”, “fact” or “opinion”, and as “positive”, “negative” and “neutral”. Using this data, we train different machine learning algorithms and compare traditional bags of words representations with word embeddings in combination with lexical, syntactic, semantic, network- based and emotional properties of texts to automatically classify patient-authored contents into positive, negative and neutral. Beside, we experiment with a combination of textual and semantic representations by generating concept embeddings using the UMLS Metathesaurus. Results: We reach two main results: first, we find that it is possible to predict polarity of patient-authored contents with a very high accuracy (≈ 70 percent) using word embeddings, and that this considerably outperforms more traditional representations like bags of words; and second, when dealing with medical information, negative and positive facts (i.e. objective information) are nearly as frequent as negative and positive opinions and experiences (i.e. subjective information), and their importance for polarity classification is crucial. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Carrillo-de-Albornoz, Jorge: jcalbornoz@lsi.uned.es},
  issn      = {1932-6203(Electronic)},
  journal   = {PLoS ONE},
  keywords  = {*Health Behavior, *Social Media, Engineering},
  publisher = {Public Library of Science},
  refid     = {2018-61911-001},
  volume    = {13},
  year      = {2018},
}

@Article{,
  author    = {Pan, Xianglin and Hu, Bihao and Zhou, Zihao and Feng, Xiang},
  title     = {Are students happier the more they learn? - research on the influence of course progress on academic emotion in online learning.},
  doi       = {10.1080/10494820.2022.2052110},
  issn      = {1744-5191(Electronic),1049-4820(Print)},
  pages     = {No Pagination Specified--No Pagination Specified},
  abstract  = {Academic emotions of learners are important for academic achievement. For the online learning platform, it is of great value to gain insight into the academic emotion of the course in appropriate time interval from the platform. We crawled a large number of student comment texts from MOOC, and used deep learning algorithms (BERT models) to perform aspect-oriented sentiment classification on the comment texts. We conducted statistical analysis and identified keywords to explore the changes of academic emotions in the online learning environment in different aspect dimensions. The results show that academic emotions are significantly improved in the first and second period of the course schedule, and tend to be stable in the second and third period of the course schedule. From the word frequency statistics, in the dimension of the teacher, students’ concerns mainly focus on two aspects: One is whether they can acquire knowledge, the other is the characteristics of teachers; in the course dimension, students attach more importance to the learning; in the dimension of the platform, students’ negative emotions mainly focus on four aspects: certificate, learning record, prompt and subtitle. Our research aims at providing suggestions for course design, platform improvement, and teachers’ practice. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Feng, Xiang: xfeng@eec.ecnu.edu.cn},
  journal   = {Interactive Learning Environments},
  publisher = {Taylor & Francis},
  refid     = {2022-50333-001},
  year      = {2022},
}

@Misc{,
  author    = {Zhou, Xinyu and Song, Yi and Jiang, Hao and Wang, Qian and Qu, Zhiqiang and Zhou, Xiaoyu and Jit, Mark and Hou, Zhiyuan and Lin, Leesa},
  title     = {Comparison of public responses to containment measures during the initial outbreak and resurgence of COVID-19 in China: Infodemiology study.},
  doi       = {10.2196/26518},
  abstract  = {Background: COVID-19 cases resurged worldwide in the second half of 2020. Not much is known about the changes in public responses to containment measures from the initial outbreak to resurgence. Monitoring public responses is crucial to inform policy measures to prepare for COVID-19 resurgence. Objective: This study aimed to assess and compare public responses to containment measures during the initial outbreak and resurgence of COVID-19 in China. Methods: We curated all COVID-19-related posts from Sina Weibo (China’s version of Twitter) during the initial outbreak and resurgence of COVID-19 in Beijing, China. With a Python script, we constructed subsets of Weibo posts focusing on 3 containment measures: lockdown, the test-trace-isolate strategy, and suspension of gatherings. The Baidu open-source sentiment analysis model and latent Dirichlet allocation topic modeling, a widely used machine learning algorithm, were used to assess public engagement, sentiments, and frequently discussed topics on each containment measure. Results: A total of 8,985,221 Weibo posts were curated. In China, the containment measures evolved from a complete lockdown for the general population during the initial outbreak to a more targeted response strategy for high-risk populations during COVID-19 resurgence. Between the initial outbreak and resurgence, the average daily proportion of Weibo posts with negative sentiments decreased from 57% to 47% for the lockdown, 56% to 51% for the test-trace-isolate strategy, and 55% to 48% for the suspension of gatherings. Among the top 3 frequently discussed topics on lockdown measures, discussions on containment measures accounted for approximately 32% in both periods, but those on the second-most frequently discussed topic shifted from the expression of negative emotions (11%) to its impacts on daily life or work (26%). The public expressed a high level of panic (21%) during the initial outbreak but almost no panic (1%) during resurgence. The more targeted test-trace-isolate measure received the most support (60%) among all 3 containment measures in the initial outbreak, and its support rate approached 90% during resurgence. Conclusions: Compared to the initial outbreak, the public expressed less engagement and less negative sentiments on containment measures and were more supportive toward containment measures during resurgence. Targeted test-trace-isolate strategies were more acceptable to the public. Our results indicate that when COVID-19 resurges, more targeted test-trace-isolate strategies for high-risk populations should be promoted to balance pandemic control and its impact on daily life and the economy. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Hou, Zhiyuan: School of Public Health, Fudan University, Mailbox 250, 138# Yixueyuan Road, Xuhui District, Shanghai, China, 200032, zyhou@fudan.edu.cn},
  issn      = {1438-8871(Electronic),1439-4456(Print)},
  journal   = {Journal of Medical Internet Research},
  keywords  = {*Panic, *Responses, *Simulation, *Social Media, *COVID-19, Pandemics, Negative Emotions, Psychological Engagement},
  publisher = {JMIR Publications},
  refid     = {2021-58443-001},
  volume    = {23},
  year      = {2021},
}

@Misc{,
  author    = {Aladağ, Ahmet Emre and Muderrisoglu, Serra and Akbas, Naz Berfu and Zahmacioglu, Oguzhan and Bingol, Haluk O.},
  title     = {Detecting suicidal ideation on forums: Proof-of-concept study.},
  doi       = {10.2196/jmir.9840},
  abstract  = {Background: In 2016, 44,965 people in the United States died by suicide. It is common to see people with suicidal ideation seek help or leave suicide notes on social media before attempting suicide. Many prefer to express their feelings with longer passages on forums such as Reddit and blogs. Because these expressive posts follow regular language patterns, potential suicide attempts can be prevented by detecting suicidal posts as they are written. Objective: This study aims to build a classifier that differentiates suicidal and nonsuicidal forum posts via text mining methods applied on post titles and bodies. Methods: A total of 508,398 Reddit posts longer than 100 characters and posted between 2008 and 2016 on SuicideWatch, Depression, Anxiety, and ShowerThoughts subreddits were downloaded from the publicly available Reddit dataset. Of these, 10,785 posts were randomly selected and 785 were manually annotated as suicidal or nonsuicidal. Features were extracted using term frequency-inverse document frequency, linguistic inquiry and word count, and sentiment analysis on post titles and bodies. Logistic regression, random forest, and support vector machine (SVM) classification algorithms were applied on resulting corpus and prediction performance is evaluated. Results: The logistic regression and SVM classifiers correctly identified suicidality of posts with 80% to 92% accuracy and F1 score, respectively, depending on different data compositions closely followed by random forest, compared to baseline ZeroR algorithm achieving 50% accuracy and 66% F1 score. Conclusions: This study demonstrated that it is possible to detect people with suicidal ideation on online forums with high accuracy. The logistic regression classifier in this study can potentially be embedded on blogs and forums to make the decision to offer real-time online counseling in case a suicidal post is being written. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Aladağ, Ahmet Emre: Department of Computer Engineering, Bogazici University, Istanbul, Turkey, 34342, emre.aladag@boun.edu.tr},
  issn      = {1438-8871(Electronic),1439-4456(Print)},
  journal   = {Journal of Medical Internet Research},
  keywords  = {*Suicidal Ideation, *Suicide, *Suicide Prevention, *Online Social Networks, *Suicidality, Artificial Intelligence, Machine Learning},
  publisher = {JMIR Publications},
  refid     = {2018-57167-001},
  volume    = {20},
  year      = {2018},
}

@Misc{,
  author    = {Ortigosa, Alvaro and Martín, José M. and Carro, Rosa M.},
  title     = {Sentiment analysis in facebook and its application to e-learning.},
  doi       = {10.1016/j.chb.2013.05.024},
  abstract  = {This paper presents a new method for sentiment analysis in Facebook that, starting from messages written by users, supports: (i) to extract information about the users’ sentiment polarity (positive, neutral or negative), as transmitted in the messages they write; and (ii) to model the users’ usual sentiment polarity and to detect significant emotional changes. We have implemented this method in SentBuk, a Facebook application also presented in this paper. SentBuk retrieves messages written by users in Facebook and classifies them according to their polarity, showing the results to the users through an interactive interface. It also supports emotional change detection, friend’s emotion finding, user classification according to their messages, and statistics, among others. The classification method implemented in SentBuk follows a hybrid approach: it combines lexical-based and machine-learning techniques. The results obtained through this approach show that it is feasible to perform sentiment analysis in Facebook with high accuracy (83.27%). In the context of e-learning, it is very useful to have information about the users’ sentiments available. On one hand, this information can be used by adaptive e-learning systems to support personalized learning, by considering the user’s emotional state when recommending him/her the most suitable activities to be tackled at each time. On the other hand, the students’ sentiments towards a course can serve as feedback for teachers, especially in the case of online learning, where face-to-face contact is less frequent. The usefulness of this work in the context of e-learning, both for teachers and for adaptive systems, is described too. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Ortigosa, Alvaro: Department of Computer Science, Universidad Autonoma de Madrid, Francisco Tomás y Valiente 11, Madrid, Spain, 28049, alvaro.ortigosa@uam.es},
  issn      = {1873-7692(Electronic),0747-5632(Print)},
  journal   = {Computers in Human Behavior},
  keywords  = {*Educational Psychology, *School Learning, *Online Social Networks, *Sentiment Analysis, Emotional States, Messages},
  pages     = {527--541},
  publisher = {Elsevier Science},
  refid     = {2013-28731-001},
  volume    = {31},
  year      = {2014},
}

@Misc{,
  author    = {Wawer, Aleksander and Chojnicka, Izabela and Okruszek, Lukasz and Sarzynska-Wawer, Justyna},
  title     = {Single and cross-disorder detection for autism and schizophrenia.},
  doi       = {10.1007/s12559-021-09834-9},
  abstract  = {Detection of mental disorders from textual input is an emerging field for applied machine and deep learning methods. Here, we explore the limits of automated detection of autism spectrum disorder (ASD) and schizophrenia (SCZ). We compared the performance of: (1) dedicated diagnostic tools that involve collecting textual data, (2) automated methods applied to the data gathered by these tools, and (3) psychiatrists. Our article tests the effectiveness of several baseline approaches, such as bag of words and dictionary-based vectors, followed by a machine learning model. We employed two more refined Sentic text representations using affective features and concept-level analysis on texts. Further, we applied selected state-of-the-art deep learning methods for text representation and inference, as well as experimented with transfer and zero-shot learning. Finally, we also explored few-shot methods dedicated to low data size scenarios, which is a typical problem for the clinical setting. The best breed of automated methods outperformed human raters (psychiatrists). Cross-dataset approaches turned out to be useful (only from SCZ to ASD) despite different data types. The few-shot learning methods revealed promising results on the SCZ dataset. However, more effort is needed to explore the approaches to efficiently training models, given the very limited amounts of labeled clinical data. Psychiatry is one of the few medical fields in which the diagnosis of most disorders is based on the subjective assessment of a psychiatrist. Therefore, the introduction of objective tools supporting diagnostics seems to be pivotal. This paper is a step in this direction. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Wawer, Aleksander: Institute of Computer Science, Polish Academy of Sciences, Jana Kazimierza 5, Warszawa, Poland, 01 248, axw@ipipan.waw.pl},
  issn      = {1866-9964(Electronic),1866-9956(Print)},
  journal   = {Cognitive Computation},
  keywords  = {*Autism Spectrum Disorders, *Machine Learning, *Schizophrenia, *Transfer (Learning), Sentiment Analysis},
  pages     = {461--473},
  publisher = {Springer},
  refid     = {2021-12746-001},
  volume    = {14},
  year      = {2022},
}

@Misc{,
  author    = {Yang, Fan},
  title     = {False textual information detection-towards building a truth machine.},
  abstract  = {With social media growing dominant, false information, such as questionable claims and fake news, diffuses fast. Detecting false information is one of the most elusive and long-standing challenges. With social media growing dominant, falsehood can diffuse faster and broader than truth. This calls for building a ``truth machine" that automatically debunks false information. Although existing works have developed methods to prevent false information, challenges still remain. For example, previous works demand a large amount of annotated data and related evidence, underestimating the difficulty of evidence linking and the cost of manual annotation. Besides, since a large number of works rely on evidence to determine the credibility of claims, we need to carefully address situations when no evidence or noisy evidence is provided. This thesis aims to improve detecting false textual information from four aspects: 1. we first target sentiment classification because previous works show that leveraging sentiment can boost content-based rumor detection. We propose a representation learning framework that incorporates both labeled and unlabeled data. We show that our model learns robust features across domains and removes domain-specific features. 2. we develop a hierarchical model with attention mechanism so that our model reveals important insights at the paragraph level or at the sentence level. We evaluate our model on news satire detection and find that our model can effectively discover satirical cues at different levels. 3. we extend evidence-aware claim verification from supervised learning to positive-unlabeled learning. This setting requires a comparatively small number of true claims, and more claims can be unlabeled. We adopt the generative adversarial network to generate pseudo negative examples and conduct a thorough analysis of selected models. 4. we pay special attention to analyzing whether estimating entailment between evidence and claim helps not only to verify it but also to the preliminary step of retrieving the necessary evidence. We find that entailment indeed improves evidence ranking, as far as the entailment model produces reliable outputs. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4217(Print)},
  keywords  = {*Credibility, *Learning, *Sentences, *Truth, *Social Media, Deception, Gossip, Labeling, News Media},
  pages     = {No Pagination Specified--No Pagination Specified},
  publisher = {ProQuest Information & Learning},
  refid     = {2021-27921-031},
  volume    = {82},
  year      = {2021},
}

@Misc{,
  author    = {Mohammad, Saif M.},
  title     = {Sentiment analysis: Automatically detecting valence, emotions, and other affectual states from text.},
  doi       = {10.1016/B978-0-12-821124-3.00011-9},
  abstract  = {Recent advances in machine learning have led to computer systems that are humanlike in behavior. Sentiment analysis, the automatic determination of emotions in text, is allowing us to capitalize on substantial previously unattainable opportunities in commerce, public health, government policy, social sciences, and art. Further, analysis of emotions in text, from news to social media posts, is improving our understanding of not just how people convey emotions through language but also how emotions shape our behavior. This article presents a sweeping overview of sentiment analysis research that includes: the origins of the field, the rich landscape of tasks, challenges, a survey of the methods and resources used, and applications. We also discuss how, without careful fore-thought, sentiment analysis has the potential for harmful outcomes. We outline the latest lines of research in pursuit of fairness in sentiment analysis. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Mohammad, Saif M.: saif.mohammad@nrc-cnrc.gc.ca},
  issn      = {978-0-12-821125-0 (Hardcover)},
  journal   = {Emotion measurement, 2nd ed.},
  keywords  = {*Emotional States, *Affective Valence, *Sentiment Analysis, Text Analysis},
  pages     = {323--379},
  publisher = {Elsevier},
  refid     = {2021-46839-011},
  year      = {2021},
}

@Misc{,
  author    = {Iacus, S. M. and Porro, G.},
  title     = {Subjective well-being and social media.},
  doi       = {10.1201/9780429401435},
  abstract  = {This book presents an overview of the most recent projects on the estimation of subjective well-being through social media data. In particular, it focuses on a new project, aimed at constructing a Twitter Subjective Well-Being Index, which started in 2012-almost at the same time of expansion of sentiment analysis to Twitter data-and grew slowly till the present days. The project was originally conceived at the University of Milan (Italy) and then embraced later in 2015 by the University of Insubria (Como, Italy), the University of Tokyo and the University of Waseda in Japan. The book reviews the different approaches to the estimation of well-being, from traditional macro-economic definition-both one-dimensional and multidimensional-to survey analysis and finally to big data and social networking sites (SNS) in particular. It introduces briefly the most commonly used machine learning and statistical techniques for textual analysis. It also serves two scopes: to explain how machines transforms text into meaningful statistics, and also to convey the idea that human supervision is an essential step of this process whatever technique is used. The book presents different SNS-based subjective well-being indexes that have been proposed in the literature, with a special focus on the one proposed by the authors. Among all positive aspects of SNS data, there are also some pitfalls which are quite easy to imagine, and well known to the experts in the field. The main one is that social media accounts/users/data cannot be considered statistically representative of the demographic population. The book presents a possible approach to tackle the selection bias problem by anchoring social media indexes to official statistics. It focuses on the analysis of the impact of the COVID-19 pandemic, that hit the world in 2020, on the social media indexes of subjective well-being. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Boca Raton, FL, US},
  issn      = {9781138393929 (Hardcover); 978-1-032-04316-6 (Paperback); 978-0-429-40143-5 (Digital (undefined format))},
  journal   = {Subjective well-being and social media.},
  keywords  = {*Subjectivity, *Well Being, *Social Media, Biased Sampling, Machine Learning, Social Networks, Statistical Analysis, Big Data},
  pages     = {xiii, 206--xiii, 206},
  publisher = {CRC Press/Routledge/Taylor & Francis Group},
  refid     = {2021-80251-000},
  year      = {2021},
}

@Misc{,
  author    = {Liu, Tony and Meyerhoff, Jonah and Eichstaedt, Johannes C. and Karr, Chris J. and Kaiser, Susan M. and Kording, Konrad P. and Mohr, David C. and Ungar, Lyle H.},
  title     = {The relationship between text message sentiment and self-reported depression.},
  doi       = {10.1016/j.jad.2021.12.048},
  abstract  = {Background: Personal sensing has shown promise for detecting behavioral correlates of depression, but there is little work examining personal sensing of cognitive and affective states. Digital language, particularly through personal text messages, is one source that can measure these markers. Methods: We correlated privacy-preserving sentiment analysis of text messages with self-reported depression symptom severity. We enrolled 219 U.S. adults in a 16 week longitudinal observational study. Participants installed a personal sensing app on their phones, which administered self-report PHQ-8 assessments of their depression severity, collected phone sensor data, and computed anonymized language sentiment scores from their text messages. We also trained machine learning models for predicting end-of-study self-reported depression status using on blocks of phone sensor and text features. Results: In correlation analyses, we find that degrees of depression, emotional, and personal pronoun language categories correlate most strongly with self-reported depression, validating prior literature. Our classification models which predict binary depression status achieve a leave-one-out AUC of 0.72 when only considering text features and 0.76 when combining text with other networked smartphone sensors. Limitations: Participants were recruited from a panel that over-represented women, caucasians, and individuals with self-reported depression at baseline. As language use differs across demographic factors, generalizability beyond this population may be limited. The study period also coincided with the initial COVID-19 outbreak in the United States, which may have affected smartphone sensor data quality. Conclusions: Effective depression prediction through text message sentiment, especially when combined with other personal sensors, could enable comprehensive mental health monitoring and intervention. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Liu, Tony: liutony@seas.upenn.edu},
  issn      = {1573-2517(Electronic),0165-0327(Print)},
  journal   = {Journal of Affective Disorders},
  keywords  = {*Major Depression, *Self-Report, *Text Messaging, Machine Learning},
  pages     = {7--14},
  publisher = {Elsevier Science},
  refid     = {2022-35007-003},
  volume    = {302},
  year      = {2022},
}

@Misc{,
  author    = {Sun, Bing and Mao, Hongying and Yin, Chengshun},
  title     = {Male and female users’ differences in online technology community based on text mining.},
  doi       = {10.3389/fpsyg.2020.00806},
  abstract  = {With the emergence of online communities, more and more people are participating in online technology communities to meet personalized learning needs. This study aims to investigate whether and how male and female users behave differently in online technology communities. Using text data from the Python Technology Community, through the LDA (Latent Dirichlet Allocation) model, sentiment analysis, and regression analysis, this paper reveals the different topics of male and female users in the online technology community, their sentimental tendencies and activity under different topics, and their correlation and mutual influence. The results show the following: (1) Male users tend to provide information help, while female users prefer to participate in the topic of making friends and advertising. (2) When communicating in the technology community, male and female users mostly express positive emotions, but female users express positive emotions more frequently. (3) Different emotional tendencies of male and female users under different topics have different effects on their activity in the community. The activity of female users is more susceptible to emotional orientation. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Sun, Bing: heusun@hotmail.com},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Computer Assisted Instruction, *Human Sex Differences, *Technology, *Positive Emotions, *Online Community, Computer Software