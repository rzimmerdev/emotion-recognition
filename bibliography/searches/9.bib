@Misc{,
  author    = {Papakostas, Michalis},
  title     = {From body to brain: Using artificial intelligence to identify user skills & intentions in interactive scenarios.},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {Artificial Intelligence has probably been the most rapidly evolving field of science during the last decade. Its numerous real-life applications have radically altered the way we experience daily-living with great impact in some of the most basic aspects of human lives including but not limited to health and well-being, communication and interaction, education, driving, daily, and entertainment.Human-Computer Interaction (HCI) is the field of Computer Science lying in the epicenter of this evolution and is responsible for transforming rudimentary research findings and theoretical principles into intuitive tools, responsible for enhancing human performance, increasing productivity and ensuring safety. Two of the core questions that HCI research tries to address relate to a) what does user want? and b) what can the user do?Multi-modal user monitoring has shown great potential towards answering those questions. Modeling and tracking different parameters of user's behavior has provided groundbreaking solutions in several fields such as smart rehabilitation, smart driving, and workplace-safety.Two of the dominant modalities that have been extensively deployed for such systems are speech and vision-based approaches with a special focus on activity and emotion recognition. Despite the great amount of research that has been done in these domains, there are numerous other implicit and explicit types of user-feedback produced during an HCI scenario, that are very informative but have attracted very limited research interest. This is usually due to the great levels of inherent noise that such signals tend to carry, or due to the highly invasive equipment that is required to capture this kind of information. These factors make most real-life applications almost impossible to implement.This research aims to investigate the potentials of multi-modal user monitoring towards designing personalized scenarios and interactive interfaces that focus on two different research axis. Firstly we explore the advantages of reusing existing knowledge across different information domains, application areas, and individual users in an effort to create predictive models that can expand their functionalities between distinct HCI scenarios. Secondly, we try to enhance multi-modal interaction by accessing information that stems from more sophisticated and less explored sources such as Electroencephalogram (EEG) and Electromyogram (EMG) analysis using minimally invasive sensors. We achieve this by designing a series of end-to-end experiments (from data collection to analysis and application) and by performing an extensive evaluation on various Machine Learning (ML) and Deep-Learning (DL) approaches on their ability to model diverge signals of interaction. As an outcome of this in-depth investigation and experimentation, we propose CogBeacon. A multi-modal dataset and data-collection platform, to our knowledge the first of its kind, towards predicting events of cognitive fatigue and understanding its impact on human performance. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4209(Print)},
  keywords  = {*Artificial Intelligence, *Electroencephalography, *Electromyography, *Human Computer Interaction, *Models, Experimentation, Intention, Sciences},
  pages     = {No Pagination Specified--No Pagination Specified},
  priority  = {prio2},
  publisher = {ProQuest Information & Learning},
  refid     = {2020-04053-171},
  volume    = {81},
  year      = {2020},
}

@Misc{,
  author    = {Bolonkin, Maksim},
  title     = {Exploiting group structures to infer social interactions from videos.},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {In this thesis, we consider the task of inferring the social interactions between humans by analyzing multi-modal data. Specifically, we attempt to solve some of the problems in interaction analysis, such as long-term deception detection, political deception detection, and impression prediction. In this work, we emphasize the importance of using knowledge about the group structure of the analyzed interactions. Previous works on the matter mostly neglected this aspect and analyzed a single subject at a time.Using the new Resistance dataset, collected by our collaborators, we approach the problem of long-term deception detection by designing a class of histogram-based features and a novel class of meta-features we call LiarRank. We develop a LiarOrNot model to identify spies in Resistance videos. We achieve AUCs of over 0.70 outperforming our baselines by 3% and human judges by 12%.For the problem of political deception, we first collect a dataset of videos and transcripts of 76 politicians from 18 countries making truthful and deceptive statements. We call it the Global Political Deception Dataset. We then show how to analyze the statements in a broader context by building a Video-Article-Topic graph. From this graph, we create a novel class of features called Deception Score that captures how controversial each topic is and how it affects the truthfulness of each statement. We show that our approach achieves 0.775 AUC outperforming competing baselines.Finally, we use the Resistance data to solve the problem of dyadic impression prediction. Our proposed Dyadic Impression Prediction System (DIPS) contains four major innovations: a novel class of features called emotion ranks, sign imbalance features derived from signed graphs theory, a novel method to align the facial expressions of subjects, and finally, we propose the concept of a multilayered stochastic network we call Temporal Delayed Network. Our DIPS architecture beats eight baselines from the literature, yielding statistically significant improvements of 19.9-30.8% in AUC. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4217(Print)},
  keywords  = {*Computers, *Group Structure, *Machine Learning, *Social Interaction, *Sociology, Deception, Polygraphs, Prediction},
  pages     = {No Pagination Specified--No Pagination Specified},
  priority  = {prio3},
  publisher = {ProQuest Information & Learning},
  refid     = {2021-94599-272},
  volume    = {83},
  year      = {2022},
}

@Misc{,
  author    = {Zhang, Jie and Duan, Yingjing and Gu, Xiaoqing},
  title     = {Research on emotion analysis of Chinese literati painting images based on deep learning.},
  doi       = {10.3389/fpsyg.2021.723325},
  status    = {Rejeitado - Escopo},
  abstract  = {Starting from a pure-image perspective, using machine learning in emotion analysis methods to study artwork is a new cross-cutting approach in the field of literati painting and is an effective supplement to research conducted from the perspectives of aesthetics, philosophy, and history. This study constructed a literati painting emotion dataset. Five classic deep learning models were used to test the dataset and select the most suitable model, which was then improved upon for literati painting emotion analysis based on accuracy and model characteristics. The final training accuracy rate of the improved model was 54.17%. This process visualizes the salient feature areas of the picture in machine vision, analyzes the visualization results, and summarizes the connection law between the picture content of the Chinese literati painting and the emotion expressed by the painter. This study validates the possibility of combining deep learning with Chinese cultural research, provides new ideas for the combination of new technology and traditional Chinese literati painting research, and provides a better understanding of the Chinese cultural spirit and advanced factors. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Zhang, Jie: zhangjie@jiangnan.edu.cn},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Computers, *Emotions, *Machine Learning, *Painting (Art), Imagery, Learning, Models},
  publisher = {Frontiers Media S.A.},
  refid     = {2021-75851-001},
  volume    = {12},
  year      = {2021},
}

@Misc{,
  author    = {Pons, Gerard and Masip, David},
  title     = {Supervised committee of convolutional neural networks in automated facial expression analysis.},
  doi       = {10.1109/TAFFC.2017.2753235},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {Automated emotion recognition from facial images is an unsolved problem in computer vision. Although recent methods achieve close to human accuracy in controlled scenarios, the recognition of emotions in the wild remains a challenging problem. Recent advances in Deep learning have supposed a significant breakthrough in many computer vision tasks, including facial expression analysis. Particularly, the use of Deep Convolutional Neural Networks has attained the best results in the recent public challenges. The current state-of-the-art algorithms suggest that the use of ensembles of CNNs can outperform individual CNN classifiers. Two key considerations influence these results: (i) The design of CNNs involves the adjustment of parameters that allow diversity and complementarity in the partial classification results, and (ii) the final classification rule that assembles the result of the committee. In this paper we propose to improve the assembling of the committee by introducing supervised learning on the ensemble computation.We train a CNN on the posterior-class probabilities resulting from the individual members allowing to capture non-linear dependencies among committee members, and to learn this combination from data. The validation shows an accuracy 5 percent higher with respect to previous state-of-the art results based on averaging classifiers, and 4 percent to the majority voting rule. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Pons, Gerard: Department of Computer Science, Universitat Oberta de Catalunya, Barcelona, Spain, 08018, gponsro@uoc.edu},
  issn      = {1949-3045(Electronic)},
  journal   = {IEEE Transactions on Affective Computing},
  keywords  = {*Artificial Neural Networks, *Machine Learning Algorithms, Facial Affect Recognition},
  pages     = {343--350},
  priority  = {prio1},
  publisher = {IEEE Publications Office},
  refid     = {2019-18885-004},
  volume    = {9},
  year      = {2018},
}

@Misc{,
  author    = {Küntzler, Theresa and Höfling, T. Tim A. and Alpers, Georg W.},
  title     = {Automatic facial expression recognition in standardized and non-standardized emotional expressions.},
  doi       = {10.3389/fpsyg.2021.627561},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {Emotional facial expressions can inform researchers about an individual's emotional state. Recent technological advances open up new avenues to automatic Facial Expression Recognition (FER). Based on machine learning, such technology can tremendously increase the amount of processed data. FER is now easily accessible and has been validated for the classification of standardized prototypical facial expressions. However, applicability to more naturalistic facial expressions still remains uncertain. Hence, we test and compare performance of three different FER systems (Azure Face API, Microsoft; Face++, Megvii Technology; FaceReader, Noldus Information Technology) with human emotion recognition (A) for standardized posed facial expressions (from prototypical inventories) and (B) for non-standardized acted facial expressions (extracted from emotional movie scenes). For the standardized images, all three systems classify basic emotions accurately (FaceReader is most accurate) and they are mostly on par with human raters. For the non-standardized stimuli, performance drops remarkably for all three systems, but Azure still performs similarly to humans. In addition, all systems and humans alike tend to misclassify some of the non-standardized emotional facial expressions as neutral. In sum, emotion recognition by automated facial expression recognition can be an attractive alternative to human emotion recognition for standardized and non-standardized emotional facial expressions. However, we also found limitations in accuracy for specific facial expressions; clearly there is need for thorough empirical evaluation to guide future developments in computer vision of emotional facial expressions. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Küntzler, Theresa: theresa.kuentzler@uni-konstanz.de},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Emotionality (Personality), *Facial Expressions, *Machine Learning, *Emotion Recognition, Computer Assisted Instruction, Films},
  priority  = {prio1},
  publisher = {Frontiers Media S.A.},
  refid     = {2021-48254-001},
  volume    = {12},
  year      = {2021},
}

@Misc{,
  author    = {Siam, Ali I. and Soliman, Naglaa F. and Algarni, Abeer D. and Abd El-Samie, Fathi E. and Sedik, Ahmed},
  title     = {Deploying machine learning techniques for human emotion detection.},
  doi       = {10.1155/2022/8032673},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {Emotion recognition is one of the trending research fields. It is involved in several applications. Its most interesting applications include robotic vision and interactive robotic communication. Human emotions can be detected using both speech and visual modalities. Facial expressions can be considered as ideal means for detecting the persons' emotions. This paper presents a real-time approach for implementing emotion detection and deploying it in the robotic vision applications. The proposed approach consists of four phases: preprocessing, key point generation, key point selection and angular encoding, and classification. The main idea is to generate key points using MediaPipe face mesh algorithm, which is based on real-time deep learning. In addition, the generated key points are encoded using a sequence of carefully designed mesh generator and angular encoding modules. Furthermore, feature decomposition is performed using Principal Component Analysis (PCA). This phase is deployed to enhance the accuracy of emotion detection. Finally, the decomposed features are enrolled into a Machine Learning (ML) technique that depends on a Support Vector Machine (SVM), k-Nearest Neighbor (KNN), Naïve Bayes (NB), Logistic Regression (LR), or Random Forest (RF) classifier. Moreover, we deploy a Multilayer Perceptron (MLP) as an efficient deep neural network technique. The presented techniques are evaluated on different datasets with different evaluation metrics. The simulation results reveal that they achieve a superior performance with a human emotion detection accuracy of 97%, which ensures superiority among the efforts in this field. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Soliman, Naglaa F.: nfsoliman@pnu.edu.sa},
  issn      = {1687-5273(Electronic),1687-5265(Print)},
  journal   = {Computational Intelligence and Neuroscience},
  keywords  = {*Artificial Intelligence, *Machine Learning, Emotions, Facial Expressions, Robotics, Deep Neural Networks},
  priority  = {prio3},
  publisher = {Hindawi Limited},
  refid     = {2022-42340-001},
  volume    = {2022},
  year      = {2022},
}

@Misc{,
  author    = {Srinivasan, Ramprakash},
  title     = {Computational models of the production and perception of facial expressions.},
  url       = {https://etd.ohiolink.edu/apexprod/rws_olink/r/1501/10?clear=10&p10_accession_num=osu1531239299392184},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {By combining different facial muscle actions, called Action Units (AUs), humans can produce an extraordinarily large number of facial expressions. Computational models and studies in cognitive science have long hypothesized the brain needs to visually interpret these action units to understand other people's actions and intentions. Surprisingly, no studies have identified the neural basis of the visual recognition of these action units. Here, using functional Magnetic Resonance Imaging (fMRI) and an innovative machine learning analysis approach, we identify a consistent and differential coding of action units in the brain. Crucially, in a brain region thought to be responsible for the processing of changeable aspects of the face, pattern analysis could decode the presence of specific action units in an image. This coding was found to be consistent across people, facilitating the estimation of the perceived action units on participants not used to train the pattern analysis decoder. Research in face perception and emotion theory requires very large annotated databases of images of facial expressions of emotion. Useful annotations include AUs and their intensities, as well as emotion category. This process cannot be practically achieved manually. Herein, we present a novel computer vision algorithm to annotate a large database of a million images of facial expressions of emotion from the wild (i.e., face images downloaded from the Internet). Comparisons with state-of-the-art algorithms demonstrate the algorithm's high accuracy. We further use WordNet to download 1,000,000 images of facial expressions with associated emotion keywords from the Internet. The downloaded images are then automatically annotated with AUs, AU intensities and emotion categories by our algorithm. The result is a highly useful database that can be readily queried using semantic descriptions for applications in computer vision, affective computing, social and cognitive psychology. Color is a fundamental image feature of facial expressions. For example, when we furrow our eyebrows in anger, blood rushes in and a reddish color becomes apparent around that area of the face. Surprisingly, these image properties have not been exploited to recognize the facial action units (AUs) associated with these expressions. Herein, we present the first system to do recognition of AUs and their intensities using these functional color changes. These color features are shown to be robust to changes in identity, gender, race, ethnicity and skin color. Because these image changes are given by functions rather than vectors, we use a functional classifiers to identify the most discriminant color features of an AU and its intensities. We demonstrate that, using these discriminant color features, one can achieve results superior to those of the state-of-the-art. Lastly, the study of emotion has reached an impasse that can only be addressed once we know which facial expressions are used within and across cultures in the wild, not in controlled lab conditions. Yet, no such studies exist. Here, we present the first large-scale study of the production and visual perception of facial expressions of emotion in the wild. We find that of the 16,384 possible facial configurations that people can produce, only 35 are successfully used to transmit emotive information across cultures, and 8 within a smaller number of cultures. Cross-cultural expressions successfully transmit emotion category and valence, but not arousal. Cultural-specific expressions successfully transmit valence and arousal, but not categories. These unexpected findings cannot be fully explained by current models of emotion. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4217},
  keywords  = {*Facial Expressions, *Models, Emotions},
  pages     = {147},
  priority  = {prio3},
  publisher = {ProQuest Information & Learning},
  refid     = {2019-00353-065},
  volume    = {80},
  year      = {2019},
}

@Misc{,
  author    = {Jonauskaite, Domicele and Abu-Akel, Ahmad and Dael, Nele and Oberfeld, Daniel and Abdel-Khalek, Ahmed M. and Al-Rasheed, Abdulrahman S. and Antonietti, Jean-Philippe and Bogushevskaya, Victoria and Chamseddine, Amer and Chkonia, Eka and Corona, Violeta and Fonseca-Pedrero, Eduardo and Griber, Yulia A. and Grimshaw, Gina and Hasan, Aya Ahmed and Havelka, Jelena and Hirnstein, Marco and Karlsson, Bodil S. A. and Laurent, Eric and Lindeman, Marjaana and Marquardt, Lynn and Mefoh, Philip and Papadatou-Pastou, Marietta and Pérez-Albéniz, Alicia and Pouyan, Niloufar and Roinishvili, Maya and Romanyuk, Lyudmyla and Salgado Montejo, Alejandro and Schrag, Yann and Sultanova, Aygun and Uusküla, Mari and Vainio, Suvi and Wąsowicz, Grażyna and Zdravković, Sunčica and Zhang, Meng and Mohr, Christine},
  title     = {Universal patterns in color-emotion associations are further shaped by linguistic and geographic proximity.},
  doi       = {10.1177/0956797620948810},
  status    = {Rejeitado - Escopo},
  abstract  = {Many of us “see red,” “feel blue,” or “turn green with envy.” Are such color-emotion associations fundamental to our shared cognitive architecture, or are they cultural creations learned through our languages and traditions? To answer these questions, we tested emotional associations of colors in 4,598 participants from 30 nations speaking 22 native languages. Participants associated 20 emotion concepts with 12 color terms. Pattern-similarity analyses revealed universal color-emotion associations (average similarity coefficient r = .88). However, local differences were also apparent. A machine-learning algorithm revealed that nation predicted color-emotion associations above and beyond those observed universally. Similarity was greater when nations were linguistically or geographically close. This study highlights robust universal color-emotion associations, further modulated by linguistic and geographic factors. These results pose further theoretical and empirical questions about the affective properties of color and may inform practice in applied domains, such as well-being and design. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Jonauskaite, Domicele: University of Lausanne, Institute of Psychology, Quartier Mouline, Batiment Geopolis, Lausanne, Switzerland, CH-1015, domicele.jonauskaite@unil.ch},
  issn      = {1467-9280(Electronic),0956-7976(Print)},
  journal   = {Psychological Science},
  keywords  = {*Color, *Emotions, *Linguistics, *Well Being, Universality},
  pages     = {1245--1260},
  publisher = {Sage Publications},
  refid     = {2020-76782-004},
  volume    = {31},
  year      = {2020},
}

@Misc{,
  author    = {Martinez, Aleix M.},
  title     = {The promises and perils of automated facial action coding in studying children’s emotions.},
  doi       = {10.1037/dev0000728},
  procedure = {Avaliação Psicólogica},
  status    = {Aceito},
  abstract  = {Computer vision algorithms have made tremendous advances in recent years. We now have algorithms that can detect and recognize objects, faces, and even facial actions in still images and video sequences. This is wonderful news for researchers that need to code facial articulations in large data sets of images and videos, because this task is time consuming and can only be completed by expert coders, making it very expensive. The availability of computer algorithms that can automatically code facial actions in extremely large data sets also opens the door to studies in psychology and neuroscience that were not previously possible, for example, to study the development of the production of facial expressions from infancy to adulthood within and across cultures. Unfortunately, there is a lack of methodological understanding on how these algorithms should and should not be used, and on how to select the most appropriate algorithm for each study. This article aims to address this gap in the literature. Specifically, we present several methodologies for use in hypothesis-based and exploratory studies, explain how to select the computer algorithms that best fit to the requirements of our experimental design, and detail how to evaluate whether the automatic annotations provided by existing algorithms are trustworthy. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Martinez, Aleix M.: Department of Electrical and Computer Engineering, and the Center for Cognitive and Brain Sciences, The Ohio State University, 205 Dreese Labs, 2015 Neil Avenue, Columbus, OH, US, 43210, martinez.158@osu.edu},
  issn      = {1939-0599(Electronic),0012-1649(Print)},
  journal   = {Developmental Psychology},
  keywords  = {*Algorithms, *Automated Information Coding, *Computer Programming, *Emotions, *Facial Expressions, Computers, Cross Cultural Differences, Experimental Design, Machine Learning, Neurosciences, Visual Perception},
  pages     = {1965--1981},
  priority  = {prio1},
  publisher = {American Psychological Association},
  refid     = {2019-50498-012},
  volume    = {55},
  year      = {2019},
}

@Misc{,
  author    = {Schultebraucks, Katharina and Yadav, Vijay and Shalev, Arieh Y. and Bonanno, George A. and Galatzer-Levy, Isaac R.},
  title     = {Deep learning-based classification of posttraumatic stress disorder and depression following trauma utilizing visual and auditory markers of arousal and mood.},
  doi       = {10.1017/S0033291720002718},
  procedure = {Terapia},
  status    = {Aceito},
  abstract  = {Background: Visual and auditory signs of patient functioning have long been used for clinical diagnosis, treatment selection, and prognosis. Direct measurement and quantification of these signals can aim to improve the consistency, sensitivity, and scalability of clinical assessment. Currently, we investigate if machine learning-based computer vision (CV), semantic, and acoustic analysis can capture clinical features from free speech responses to a brief interview 1 month post-trauma that accurately classify major depressive disorder (MDD) and posttraumatic stress disorder (PTSD). Methods: N = 81 patients admitted to an emergency department (ED) of a Level-1 Trauma Unit following a life-threatening traumatic event participated in an open-ended qualitative interview with a para-professional about their experience 1 month following admission. A deep neural network was utilized to extract facial features of emotion and their intensity, movement parameters, speech prosody, and natural language content. These features were utilized as inputs to classify PTSD and MDD cross-sectionally. Results: Both video- and audio-based markers contributed to good discriminatory classification accuracy. The algorithm discriminates PTSD status at 1 month after ED admission with an AUC of 0.90 (weighted average precision = 0.83, recall = 0.84, and f1-score = 0.83) as well as depression status at 1 month after ED admission with an AUC of 0.86 (weighted average precision = 0.83, recall = 0.82, and f1-score = 0.82). Conclusions: Direct clinical observation during post-trauma free speech using deep learning identifies digital markers that can be utilized to classify MDD and PTSD status. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Schultebraucks, Katharina: ks3796@cumc.columbia.edu},
  issn      = {1469-8978(Electronic),0033-2917(Print)},
  journal   = {Psychological Medicine},
  keywords  = {*Biological Markers, *Major Depression, *Physiological Arousal, *Posttraumatic Stress Disorder, *Deep Neural Networks, Auditory Perception, Auditory Thresholds, Emotional States, Learning, Visual Perception, Visual Thresholds, Voice, Computer Assisted Therapy, Emergency Medicine},
  pages     = {957--967},
  priority  = {prio1},
  publisher = {Cambridge University Press},
  refid     = {2020-57463-001},
  volume    = {52},
  year      = {2022},
}
