@Misc{,
  author    = {Al-Omair, Osamah M.},
  title     = {Incorporating emotion recognition in co-adaptive systems.},
  doi       = {10.1145/3425329.3425343},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {The collaboration between human and computer systems has grown astronomically over the past few years. The ability of software systems adapting to human's input is critical in the symbiosis of human-system co-adaptation, where human and software-based systems work together in a close partnership to achieve synergetic goals. However, it is not always clear what kinds of human's input should be considered to enhance the effectiveness of human and system co-adaptation. To address this issue, this research describes an approach that focuses on incorporating human emotion to improve human-computer co-adaption. The key idea is to provide a formal framework that incorporates human emotions as a foundation for explainability into co-adaptive systems, especially, how software systems recognize human emotions and adapt the system's behaviors accordingly. Detecting and recognizing optimum human emotion is a first step towards human and computer symbiosis. As the first step of this research, we conduct a comparative review for a number of technologies and methods for emotion recognition. Specifically, testing the detection accuracy of facial expression recognition of different cloud-services, algorithms, and methods.Secondly, we study the application of emotion recognition within the areas of e-learning, robotics, and explainable artificial intelligence (XAI). We propose a formal framework that incorporates human emotions into an adaptive e-learning system, to create a more personalized learning experience for higher quality of learning outcomes. In addition, we propose a framework for a co-adaptive Emotional Support Robot. This human-centric framework adopts a reinforced learning approach where the system assesses its own emotional re-actions.Finally, we present a formal probabilistic framework that incorporates emotion recognition for explanations and predicting human performance in a co-adaptive scenario. We illustrate the operability of our framework using a Decision Support System with a human operator supervising the system's decisions. We model our approach using a Stock Prediction Engine that was developed in our research lab to predict the price direction of a stock. We use probabilistic model checking to determine how complex an explanation needs to be based on how confused the human is for the purpose of improving the system's overall utility. In addition, we conduct a web-based human experiment to measure the effectiveness of incorporating emotions in improving the outcome of a co-adaptive system. Our study shows that considering human emotions in co-adaptive systems' explanation is one of the important factors for improving the overall systems performance and utility functions. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4217(Print)},
  keywords  = {*Computers, *Facial Expressions, *Human Computer Interaction, *Emotion Recognition, *Affective Computing, Probability, Robotics, Electronic Learning},
  pages     = {No Pagination Specified--No Pagination Specified},
  priority  = {prio1},
  publisher = {ProQuest Information & Learning},
  refid     = {2022-70650-282},
  volume    = {83},
  year      = {2022},
}

@Misc{,
  author    = {Kuehne, Maria and Siwy, Isabelle and Zaehle, Tino and Heinze, Hans-Jochen and Lobmaier, Janek S.},
  title     = {Out of focus: Facial feedback manipulation modulates automatic processing of unattended emotional faces.},
  doi       = {10.1162/jocn_a_01445},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {Facial expressions provide information about an individual’s intentions and emotions and are thus an important medium for nonverbal communication. Theories of embodied cognition assume that facial mimicry and resulting facial feedback plays an important role in the perception of facial emotional expressions. Although behavioral and electrophysiological studies have confirmed the influence of facial feedback on the perception of facial emotional expressions, the influence of facial feedback on the automatic processing of such stimuli is largely unexplored. The automatic processing of unattended facial expressions can be investigated by visual expression-related MMN. The expression-related MMN reflects a differential ERP of automatic detection of emotional changes elicited by rarely presented facial expressions (deviants) among frequently presented facial expressions (standards). In this study, we investigated the impact of facial feedback on the automatic processing of facial expressions. For this purpose, participants (n = 19) performed a centrally presented visual detection task while neutral (standard), happy, and sad faces (deviants) were presented peripherally. During the task, facial feedback was manipulated by different pen holding conditions (holding the pen with teeth, lips, or nondominant hand). Our results indicate that automatic processing of facial expressions is influenced and thus dependent on the own facial feedback. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Kuehne, Maria: Department of Neurology, Otto-von-Guericke-University, Leipziger Str. 44, Magdeburg, Germany, 39120, maria.kuehne@med.ovgu.de},
  issn      = {1530-8898(Electronic),0898-929X(Print)},
  journal   = {Journal of Cognitive Neuroscience},
  keywords  = {*Face Perception, *Facial Expressions, *Sensory Feedback, *Emotion Recognition, Automatism, Emotionality (Personality), Role Perception},
  pages     = {1631--1640},
  priority  = {prio3},
  publisher = {MIT Press},
  refid     = {2019-61136-003},
  volume    = {31},
  year      = {2019},
}

@Misc{,
  author    = {Montoya, Daniel and Baker-Oglesbee, Alissa and Bhattacharya, Sambit},
  title     = {What the robot sees, what the human feels: Robotic face detection and the human emotional response.},
  url       = {https://www.researchgate.net/publication/254256959_What_the_Robot_Sees_What_the_Human_Feels_Robotic_Face_Detection_and_the_Human_Emotional_Response},
  procedure = {Avaliação Psicológica},
  status    = {Aceito},
  abstract  = {New developments in robotics have brought humans and robots interacting in human environments. Research has focused its attention on the development of human-like virtual displays and robotics, while parallel lines of research have focused on the study of human responses to robotic agents with special emphasis in human’s emotional reaction. This chapter explores the intersection between robotics and neurosciences with special emphasis in human-robot interactions (HRI). We briefly present recent innovations in the context of robotic face detection and recognition as well as human physiological and cognitive response to the presence of artificial agents. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Montoya, Daniel: dmontoya@uncfsu.edu},
  issn      = {978-989-643-084-9},
  journal   = {Emotional expression: The brain and the face, Vol. 3},
  keywords  = {*Emotional Responses, *Face Perception, *Neurosciences, Robotics},
  pages     = {43--71},
  priority  = {prio1},
  publisher = {Edições Universidade Fernando Pessoa},
  refid     = {2011-30587-002},
  series    = {Studies in brain, face, and emotion.},
  year      = {2011},
}

@Misc{,
  author    = {Cohen, I. and Looije, R. and Neerincx, M. A.},
  title     = {Child’s perception of robot’s emotions: Effects of platform, context and experience.},
  doi       = {10.1007/s12369-014-0230-6},
  procedure = {Avaliação Psicológica},
  status    = {Aceito},
  abstract  = {Social robots may comfort and support children who have to cope with chronic diseases like diabetes. In social interactions, it is important to be able to express recognizable emotions. Studies show that the iCat robot, with its humanoid facial features, has this capability. In this paper we look if a Nao robot, without humanoid facial features, but with a body and colored eyes is also able to express recognizable emotions. We compare the recognition rates of the emotions between the Nao and the iCat. First a set of bodily expressions of the Nao for five basic emotions (angry, fear, happy, sad, surprise) was created and evaluated. With a signal detection task, the best recognizable bodily expression for each emotion was chosen for the final set. Then, fourteen children between 8 and 9 years old interacted both with the Nao and iCat to recognize the emotions within context, in a story-telling session, and without context. These interactions were repeated one week later to study the learning effect. For both robots, recognition rates for the expressions were relatively high (between 68 and 99 % accuracy). Only for the emotional state of sadness, the recognition was significantly higher for the iCat (95 %) than for the Nao (68 %). The emotions shown within context had higher recognition rates than those without context and during the second interaction the emotion recognition was also significantly higher than during the first session for both robots. To conclude: we succeeded to design a set of well-recognized dynamic emotional expressions for a robot platform, the Nao, without facial features. These expressions were better recognized when placed in a context, and when shown a week later. This set provides useful ingredients of social robot dialogs with children. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Cohen, I.: Delft University of Technology, Delft, Netherlands, iris.cohen@tno.nl},
  issn      = {1875-4805(Electronic),1875-4791(Print)},
  journal   = {International Journal of Social Robotics},
  keywords  = {*Child Attitudes, *Childhood Development, *Emotions, *Facial Features, *Robotics, Facial Expressions, Perception, Posture, Social Robotics},
  pages     = {507--518},
  priority  = {prio1},
  publisher = {Springer},
  refid     = {2014-10508-001},
  volume    = {6},
  year      = {2014},
}

@Misc{,
  author    = {Douglas, Katie M. and Porter, Richard J. and Johnston, Lucy},
  title     = {Sensitivity to posed and genuine facial expressions of emotion in severe depression.},
  doi       = {10.1016/j.psychres.2011.10.019},
  procedure = {Terapia},
  status    = {Aceito},
  abstract  = {The aim of the current study was to investigate whether the ability to distinguish genuine from non-genuine (neutral or posed) facial expressions of emotion (happiness, sadness, fear and disgust) is impaired in depression, and whether improvement in this ability occurs with treatment response. Sixty-eight depressed inpatients and 50 matched healthy controls performed the Emotion Categorisation Task three times over 6 weeks. All participants showed some sensitivity to the meaningful differences between genuine and non-genuine expressions of emotion, with an increasing percentage of faces labelled as genuinely feeling the emotion from neutral to posed to genuine presentations. Depressed patients showed significantly less sensitivity in differentiating non-genuine from genuine expressions of sadness, compared with healthy controls. Performance on the Emotion Categorisation Task did not change over time in treatment responders compared with treatment non-responders. These findings have implications for understanding why depressed individuals may have difficulties in social interactions. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Douglas, Katie M.: Department of Psychological Medicine, University of Otago, P.O. Box 4345, Christchurch, New Zealand, 8011, katie.douglas@otago.ac.nz},
  issn      = {1872-7123(Electronic),0165-1781(Print)},
  journal   = {Psychiatry Research},
  keywords  = {*Emotions, *Facial Expressions, *Major Depression, Sadness},
  pages     = {72--78},
  priority  = {prio1},
  publisher = {Elsevier Science},
  refid     = {2012-05509-001},
  volume    = {196},
  year      = {2012},
}

@Misc{,
  author    = {Zhang, Li and Barnden, John},
  title     = {Towards a semantic-based approach for affect and metaphor detection.},
  doi       = {10.4018/jdet.2013040103},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {Affect detection from open-ended virtual improvisational contexts is a challenging task. To achieve this research goal, the authors developed an intelligent agent which was able to engage in virtual improvisation and perform sentence-level affect detection from user inputs. This affect detection development was efficient for the improvisational inputs with strong emotional indicators. However, it can also be fooled by the diversity of emotional expressions such as expressions with weak or no affect indicators or metaphorical affective inputs. Moreover, since the improvisation often involves multi-party conversations with several threads of discussions happening simultaneously, the previous development was unable to identify the different discussion contexts and the most intended audiences to inform affect detection. Therefore, in this paper, the authors employ latent semantic analysis to find the underlying semantic structures of the emotional expressions and identify topic themes and target audiences especially for those inputs without strong affect indicators to improve affect detection performance. They also discuss how such semantic interpretation of dialog contexts is used to identify metaphorical phenomena. Initial exploration on affect detection from gestures is also discussed to interpret users’ experience of using the system and provide an extra channel to detect affect embedded in the virtual improvisation. Their work contributes to the journal themes on affect sensing from text, semantic-based dialogue processing and emotional gesture recognition. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {US},
  issn      = {1539-3119(Electronic),1539-3100(Print)},
  journal   = {International Journal of Distance Education Technologies},
  keywords  = {*Intelligent Agents, *Metaphor, *Semantics, *Emotion Recognition, Emotionality (Personality), Gestures},
  pages     = {48--65},
  priority  = {prio2},
  publisher = {IGI Global},
  refid     = {2013-37145-003},
  volume    = {11},
  year      = {2013},
}

@Misc{,
  author    = {Yang, Y. and Ge, S. S. and Lee, T. H. and Wang, C.},
  title     = {Facial expression recognition and tracking for intelligent human-robot interaction.},
  doi       = {10.1007/s11370-007-0014-z},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {For effective interaction between humans and socially adept, intelligent service robots, a key capability required by this class of sociable robots is the successful interpretation of visual data. In addition to crucial techniques like human face detection and recognition, an important next step for enabling intelligence and empathy within social robots is that of emotion recognition. In this paper, an automated and interactive computer vision system is investigated for human facial expression recognition and tracking based on the facial structure features and movement information. Twenty facial features are adopted since they are more informative and prominent for reducing the ambiguity during classification. An unsupervised learning algorithm, distributed locally linear embedding (DLLE), is introduced to recover the inherent properties of scattered data lying on a manifold embedded in high-dimensional input facial images. The selected person dependent facial expression images in a video are classified using the DLLE. In addition, facial expression motion energy is introduced to describe the facial muscle’s tension during the expressions for person-independent tracking for person independent recognition. This method takes advantage of the optical flow which tracks the feature points’ movement information. Finally, experimental results show that our approach is able to separate different expressions successfully. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Ge, S. S.: Social Robotics Lab, Interactive Digital Media Institute, National University of Singapore, Singapore, Singapore, 117576, samge@nus.edu.sg},
  issn      = {1861-2784(Electronic),1861-2776(Print)},
  journal   = {Intelligent Service Robotics},
  keywords  = {*Facial Expressions, *Human Computer Interaction, *Motion Perception, *Robotics, *Tracking, Emotions, Human Robot Interaction},
  pages     = {143--157},
  priority  = {prio1},
  publisher = {Springer},
  refid     = {2009-18644-004},
  volume    = {1},
  year      = {2008},
}

@Misc{,
  author    = {Dupuy, Lucile and Micoulaud-Franchi, Jean-Arthur and Cassoudesalle, Hélène and Ballot, Orlane and Dehail, Patrick and Aouizerate, Bruno and Cuny, Emmanuel and de Sevin, Etienne and Philip, Pierre},
  title     = {Evaluation of a virtual agent to train medical students conducting psychiatric interviews for diagnosing major depressive disorders.},
  doi       = {10.1016/j.jad.2019.11.117},
  procedure = {Terapia},
  status    = {Aceito},
  abstract  = {Background: A psychiatric diagnosis involves the physician's ability to create an empathic interaction with the patient in order to accurately extract semiology (i.e., clinical manifestations). Virtual patients (VPs) can be used to train these skills but need to be evaluated in terms of accuracy, and to be perceived positively by users. Methods: We recruited 35 medical students who interacted in a 35-min psychiatric interview with a VP simulating major depressive disorders. Semiology extraction, verbal and non-verbal empathy were measured objectively during the interaction. The students were then debriefed to collect their experience with the VP. Results: The VP was able to simulate the conduction of a psychiatric interview realistically, and was effective to discriminate students depending on their psychiatric knowledge. Results suggest that students managed to keep an emotional distance during the interview and show the added value of emotion recognition software to measure empathy in psychiatry training. Students provided positive feedback regarding pedagogic usefulness, realism and enjoyment in the interaction. Limitations: Our sample was relatively small. As a first prototype, the measures taken by the VP would need improvement (subtler empathic questions, levels of difficulty). The face-tracking technique might induce errors in detecting non-verbal empathy. Conclusion: This study is the first to simulate a realistic psychiatric interview and to measure both skills needed by future psychiatrists: semiology extraction and empathic communication. Results provide evidence that VPs are acceptable by medical students, and highlight their relevance to complement existing training and evaluation tools in the field of affective disorders. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Dupuy, Lucile: University of Bordeaux, USR 3413 SANPSY Addiction et Neuropsychiatrie, Site Carreire - Zone Nord, Bat 3B, 3rd floor, Bordeaux, France, 33076, Cedex, lucile.dupuy@u-bordeaux.fr},
  issn      = {1573-2517(Electronic),0165-0327(Print)},
  journal   = {Journal of Affective Disorders},
  keywords  = {*Empathy, *Medical Students, *Psychiatric Evaluation, *Psychiatric Training, *Psychodiagnostic Interview, Intelligent Agents, Major Depression, Medical Patients},
  pages     = {1--8},
  priority  = {prio1},
  publisher = {Elsevier Science},
  refid     = {2020-07413-001},
  volume    = {263},
  year      = {2020},
}

@Misc{,
  author    = {Masaki, Charles and Sharpley, Ann L. and Cooper, Charlotte M. and Godlewska, Beata R. and Singh, Nisha and Vasudevan, Sridhar R. and Harmer, Catherine J. and Churchill, Grant C. and Sharp, Trevor and Rogers, Robert D. and Cowen, Philip J.},
  title     = {Effects of the potential lithium-mimetic, ebselen, on impulsivity and emotional processing.},
  doi       = {10.1007/s00213-016-4319-5},
  status    = {Rejeitado - Escopo},
  abstract  = {Rationale: Lithium remains the most effective treatment for bipolar disorder and also has important effects to lower suicidal behaviour, a property that may be linked to its ability to diminish impulsive, aggressive behaviour. The antioxidant drug, ebselen, has been proposed as a possible lithium-mimetic based on its ability in animals to inhibit inositol monophosphatase (IMPase), an action which it shares with lithium. Objectives: The aim of the study was to determine whether treatment with ebselen altered emotional processing and diminished measures of risk-taking behaviour. Methods: We studied 20 healthy participants who were tested on two occasions receiving either ebselen (3600 mg over 24 h) or identical placebo in a double-blind, randomized, cross-over design. Three hours after the final dose of ebselen/placebo, participants completed the Cambridge Gambling Task (CGT) and a task that required the detection of emotional facial expressions (facial emotion recognition task (FERT)). Results: On the CGT, relative to placebo, ebselen reduced delay aversion while on the FERT, it increased the recognition of positive vs negative facial expressions. Conclusions: The study suggests that at the dosage used, ebselen can decrease impulsivity and produce a positive bias in emotional processing. These findings have implications for the possible use of ebselen in the disorders characterized by impulsive behaviour and dysphoric mood. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Cowen, Philip J.: Department of Psychiatry, University of Oxford, Warneford Hospital, Oxford, United Kingdom, OX3 7JX, phil.cowen@psych.ox.ac.uk},
  issn      = {1432-2072(Electronic),0033-3158(Print)},
  journal   = {Psychopharmacology},
  keywords  = {*Drug Therapy, *Emotional Regulation, *Impulsiveness, *Lithium, *Risk Taking, Emotional Processing},
  pages     = {2655--2661},
  publisher = {Springer},
  refid     = {2016-28003-001},
  volume    = {233},
  year      = {2016},
}

@Misc{,
  author    = {d'Elia, Giacomo and Frederiksen, Svend-Otto and Bengtsson, Bengt-Olof},
  title     = {Early visual information processing in depressive patients treated with ORG 2766 (an ACTH 4-9 analogue).},
  doi       = {10.1159/000118164},
  procedure = {Terapia},
  status    = {Aceito},
  abstract  = {21 patients (aged 50-72 yrs) who met Diagnostic and Statistical Manual of Mental Disorders (DSM-III) criteria for dysthymic or major depressive disorder participated in an inter- and intraindividual double-blind comparative study between the adrenocorticotropic hormone (ACTH) 4-9 analog ORG 2766 (80 mg/day for 14 or 28 days) and placebo. Temporal thresholds for detection and recognition of rapidly presented single letters were assessed by a tachistoscopic technique. Results indicate that the peptide had a reducing effect on recognition threshold and appeared to improve automatic, preattentive levels of information processing possibly involved in the transfer of information from the peripheral icon to the cortical center (i.e., from iconic storage to short-term memory). (43 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Switzerland},
  issn      = {1423-0224(Electronic),0302-282X(Print)},
  journal   = {Neuropsychobiology},
  keywords  = {*Corticotropin, *Depression (Emotion), *Drug Therapy, *Human Information Storage, *Visual Thresholds, Geriatric Patients, Letters (Alphabet)},
  pages     = {63--68},
  priority  = {prio3},
  publisher = {Karger},
  refid     = {1986-20380-001},
  volume    = {13},
  year      = {1985},
}

@Misc{,
  author    = {Rizzo, Albert and Scherer, Stefan and DeVault, David and Gratch, Jonathan and Artsteui, Ronald and Hartholt, Arno and Lucas, Gale and Marsella, Stacey and Morbini, Fabrizio and Nazarian, Angela and Stratou, Giota and Traum, David and Wood, Rachel and Boberg, Jill and Morency, Louis-Philippe},
  title     = {Detection and computational analysis of psychological signals using a virtual human interviewing agent.},
  url       = {https://www.researchgate.net/publication/316476437_Detection_and_computational_analysis_of_psychological_signals_using_a_virtual_human_interviewing_agent},
  procedure = {Avaliação Psicológica},
  status    = {Aceito},
  abstract  = {It has long been recognized that facial expressions, body posture/gestures and vocal parameters play an important role in human communication and the implicit signalling of emotion. Recent advances in low cost computer vision and behavioral sensing technologies can now be applied to the process of making meaningful inferences as to user state when a person interacts with a computational device. Effective use of this additive information could serve to promote human interaction with virtual human (VH) agents that may enhance diagnostic assessment. This paper will focus on our current research in these areas within the DARPA-funded "Detection and Computational Analysis of Psychological Signals" project, with specific attention to the SimSensei application use case. SimSensei is a virtual human interaction platform that is able to sense and interpret real-time audiovisual behavioral signals from users interacting with the system. It is specifically designed for health care support and leverages years of virtual human research and development at USC-ICT. The platform enables an engaging face-to-face interaction where the virtual human automatically reacts to the state and inferred intent of the user through analysis of behavioral signals gleaned from facial expressions, body gestures and vocal parameters. Akin to how non-verbal behavioral signals have an impact on human to human interaction and communication, SimSensei aims to capture and infer from user non-verbal communication to improve engagement between a VH and a user. The system can also quantify and interpret sensed behavioral signals longitudinally that can be used to inform diagnostic assessment within a clinical context. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  address   = {Rizzo, Albert: Institute for Creative Technologies, University of Southern California, 12015 East Waterfront Dr, Playa Vista, CA, US, 90094, arizzo@usc.edu},
  issn      = {1939-5914},
  journal   = {Journal of Pain Management},
  keywords  = {*Human Computer Interaction, *Mental Disorders, *Psychiatric Symptoms, *Psychological Assessment, *Virtual Reality, Mental Health, Posttraumatic Stress Disorder},
  pages     = {311--321},
  priority  = {prio1},
  publisher = {Nova Science Publishers, Inc.},
  refid     = {2017-04044-011},
  volume    = {9},
  year      = {2016},
}

@Misc{,
  author    = {Fernández-Caballero, Antonio and González, Pascual and Navarro, Elena},
  title     = {Cognitively-inspired computing for gerontechnology.},
  doi       = {10.1007/s12559-016-9392-x},
  procedure = {Terapia},
  status    = {Aceito},
  abstract  = {This article provides an overview of the papers presented in the special issue Cognitive Computation. This Special Issue focuses on all aspects of cognitive agents, addressed by current practices and future trends in Gerontechnology. These include perception, action, affective and cognitive learning and memory, attention, decision making and control, social cognition, language processing and communication, reasoning, problem solving, and consciousness. The Special Issue on "Cognitively-Inspired Computing for Gerontechnology" attracted numerous papers of the highest quality, of which five were finally accepted. The articles that make up this Special Issue cover most of the initial topics outlined during the call for papers, namely, cognitively-inspired computing for assistive technologies and devices; cognitively-inspired computing for household accident detection; emotion/affect/mood recognition and regulation; personalized ambient adaptation; social/carecognitive agents; intelligent telehealth, telemedicine and communication services; social networks for the elderly; lifelong learning for mental health. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Fernández-Caballero, Antonio: Instituto de Investigacion en Informatica de Albacete, Universidad de Castilla-La Mancha, Albacete, Spain, 02071, antonio.fdez@uclm.es},
  issn      = {1866-9964(Electronic),1866-9956(Print)},
  journal   = {Cognitive Computation},
  keywords  = {*Cognition, *Gerontology, *Technology, Computational Modeling},
  pages     = {297--298},
  priority  = {prio3},
  publisher = {Springer},
  refid     = {2016-17846-001},
  volume    = {8},
  year      = {2016},
}

@Misc{,
  author    = {Kavakli, Manolya and Ranjbartabar, Hedieh and Maddah, Amir and Ranjbartabar, Kiumars},
  title     = {Tools for eMental-health: A coping processor for adaptive and interactive mobile systems for stress management.},
  doi       = {10.4018/978-1-4666-9986-1.ch006},
  procedure = {Terapia},
  status    = {Aceito},
  abstract  = {This chapter focuses on how to develop tools for positive technology and more specifically, mobile e-mental health systems using a virtual stress counselor. The main objective is to develop a framework for mobile e-mental health systems reviewing existing e-mental health apps and discussing necessary system requirements. The chapter states that current e-mental health apps do not offer any facilities to promote social interaction between the counselor and the user. The proposed framework requires personalized interactions between a virtual counselor and a student. It provides personalized feedback to reduce stress level and enhances personal stress management strategies. This requires integration of technologies for facial expression detection, speech and emotion recognition as well as other psycho-physiological feedback. A prototype system for e-mental health has been developed and the components of the system architecture have been widely discussed including the need for a coping processor. Finally, conclusions are drawn regarding the tools for positive technology. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Hershey, PA, US},
  issn      = {978-1-4666-9986-1 (Hardcover); 978-1-4666-9987-8 (Digital (undefined format))},
  journal   = {Integrating technology in positive psychology practice.},
  keywords  = {*Coping Behavior, *Positive Psychology, *Stress Management, *Telemedicine, *Mobile Devices, Empathy, Motor Processes, Social Support, Stress, Technology, Avatars},
  pages     = {127--160},
  priority  = {prio1},
  publisher = {Information Science Reference/IGI Global},
  refid     = {2017-06727-006},
  series    = {Advances in psychology, mental health, and behavioral studies (APMHBS).},
  year      = {2016},
}

@Misc{,
  author    = {Witt, Jessica K. and Parnes, Jamie E. and Tenhundfeld, Nathan L.},
  title     = {Wielding a gun increases judgments of others as holding guns: A randomized controlled trial.},
  doi       = {10.1186/s41235-020-00260-3},
  status    = {Rejeitado - Confuso},
  abstract  = {The gun embodiment effect is the consequence caused by wielding a gun on judgments of whether others are also holding a gun. This effect could be responsible for real-world instances when police officers shoot an unarmed person because of the misperception that the person had a gun. The gun embodiment effect is an instance of embodied cognition for which a person’s tool-augmented body affects their judgments. The replication crisis in psychology has raised concern about embodied cognition effects in particular, and the issue of low statistical power applies to the original research on the gun embodiment effect. Thus, the first step was to conduct a high-powered replication. We found a significant gun embodiment effect in participants’ reaction times and in their proportion of correct responses, but not in signal detection measures of bias, as had been originally reported. To help prevent the gun embodiment effect from leading to fatal encounters, it would be useful to know whether individuals with certain traits are less prone to the effect and whether certain kinds of experiences help alleviate the effect. With the new and reliable measure of the gun embodiment effect, we tested for moderation by individual differences related to prior gun experience, attitudes, personality, and factors related to emotion regulation and impulsivity. Despite the variety of these measures, there was little evidence for moderation. The results were more consistent with the idea of the gun embodiment effect being a universal, fixed effect, than being a flexible, malleable effect. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Witt, Jessica K.: Department of Psychology, Colorado State University, Fort Collins, CO, US, 80523, Jessica.Witt@colostate.edu},
  issn      = {2365-7464(Electronic)},
  journal   = {Cognitive Research: Principles and Implications},
  keywords  = {*Firearms, *Individual Differences, *Judgment, *Signal Detection (Perception), Personality Traits},
  publisher = {Springer},
  refid     = {2020-87428-001},
  volume    = {5},
  year      = {2020},
}

@Misc{,
  author    = {Karami, Abir B. and Sehaba, Karim and Encelle, Benoit},
  title     = {Adaptive artificial companions learning from users’ feedback.},
  doi       = {10.1177/1059712316634062},
  procedure = {Terapia},
  status    = {Aceito},
  abstract  = {Until recently, propositions on the subject of intelligent service companions, like robots, were mostly user and environment independent. Our work is part of the FUI-RoboPopuli project, which concentrates on endowing entertainment companion robots with adaptive and social behavior. More precisely, we focus on the capacity of an intelligent system to learn how to personalize and adapt its behavior/actions according to its interaction situation that describes (a) the current user attributes, and (b) the current environment attributes. Our approach is based on models of the type of Markov decision processes (MDPs) that are largely used for adaptive robot applications. In order to have, as quickly as possible, a relevant adaptive behavior whatever the interaction situation, several approaches were proposed to decrease the sample complexity required to learn the MDP model, including its reward function. We argue that systems that are based on detecting important attributes for each decision are more likely to converge faster than others. To this end, we present two algorithms to learn the MDP reward function through analyzing interaction traces (i.e., the interaction history between the robot and its users including their feedback regarding the robot actions). The first algorithm is direct, certain and does not particularly exploit its knowledge to adapt to unknown situations (i.e., unknown users’ and/or environment settings). The second is able to detect the importance of certain situation attributes in the adaptation process. The detection of important attributes is used to speed up the learning process and helps by generalizing the learned reward function to unknown situations. In this paper, we present both learning algorithms, simulated experiments and an experiment with the EMOX (EMOtion eXchange) robot that was upgraded during the FUI-RoboPopuli project. The results of those experiments prove that when dealing with adaptive decision making, the detection of important attributes for each decision speeds up the learning process and help in achieving convergence using fewer samples. We also present a scaling analysis through the simulated experiments. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Karami, Abir B.: Universite Lille Nord de France, Ecole des Mines de Douai, 764 Boulevard Lahure, Douai, France, F-59500, abir.karami@mines-douai.fr},
  issn      = {1741-2633(Electronic),1059-7123(Print)},
  journal   = {Adaptive Behavior},
  keywords  = {*Adaptive Behavior, *Artificial Intelligence, *Machine Learning, *Robotics, *Social Behavior, Decision Making, Feedback},
  pages     = {69--86},
  priority  = {prio1},
  publisher = {Sage Publications},
  refid     = {2016-18302-001},
  volume    = {24},
  year      = {2016},
}

@Misc{,
  author    = {Buisine, Stéphanie and Courgeon, Matthieu and Charles, Aurélien and Clavel, Céline and Martin, Jean-Claude and Tan, Ning and Grynszpan, Ouriel},
  title     = {The role of body postures in the recognition of emotions in contextually rich scenarios.},
  doi       = {10.1080/10447318.2013.802200},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {In this article the role of different categories of postures in the detection, recognition, and interpretation of emotion in contextually rich scenarios, including ironic items, is investigated. Animated scenarios are designed with 3D virtual agents in order to test 3 conditions: In the “still” condition, the narrative content was accompanied by emotional facial expressions without any body movements; in the "idle" condition, emotionally neutral body movements were introduced; and in the "congruent" condition, emotional body postures congruent with the character’s facial expressions were displayed. Those conditions were examined by 27 subjects, and their impact on the viewers’ attentional and emotional processes was assessed. The results highlight the importance of the contextual information to emotion recognition and irony interpretation. It is also shown that both idle and emotional postures improve the detection of emotional expressions. Moreover, emotional postures increase the perceived intensity of emotions and the realism of the animations. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Buisine, Stéphanie: Arts et Metiers ParisTech, LCPI, 151 bd Hopital, Paris, France, 75013, stephanie.buisine@ensam.eu},
  issn      = {1532-7590(Electronic),1044-7318(Print)},
  journal   = {International Journal of Human-Computer Interaction},
  keywords  = {*Posture, *Virtual Reality, *Visual Displays, *Multimedia, *Animation, Facial Expressions},
  pages     = {52--62},
  priority  = {prio2},
  publisher = {Taylor & Francis},
  refid     = {2013-42530-005},
  volume    = {30},
  year      = {2014},
}

@Misc{,
  author    = {Goldberg, Terry E. and Gomar, Jesus J.},
  title     = {Comment on "Cognition in schizophrenia: Summary Nice Consultation Meeting 2012".},
  doi       = {10.1016/j.euroneuro.2013.04.009},
  procedure = {Terapia},
  status    = {Aceito},
  abstract  = {Comments on an editorial by David Nutt et al. (see record 2013-29974-004). It has been noted that a treatment that impacts both positive symptoms and cognitive impairments would probably be accused of pseudo-specificity, and thus have a more difficult regulatory approval path. Nutt et al. strongly disagree, as do the current authors. They do not think that it is a coincidence that both symptoms and worsening of cognitive status arise in tandem during a prodromal period. In fact, they think that it is plausible that there is some shared, but yet to be determined pathophysiology and view the latter as an underappreciated “holy grail.” If this is indeed the case, then a treatment might impact both these domains. One area not covered in depth in the report by Nutt et al. relates to a large US based initiative, called CNTRICS that was designed to develop tests that reflect advances in cognitive neuroscience, and as such might more precisely assay neural systems or neurochemical status. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  address   = {Goldberg, Terry E.: Psychiatry Research and Litwin Zucker Alzheimer's Disease Center, Feinstein Institute for Medical Research, Hofstra North Shore-LIJ School of Medicine, Manhasset, NY, US, 11030, tgoldber@nshs.edu},
  issn      = {1873-7862(Electronic),0924-977X(Print)},
  journal   = {European Neuropsychopharmacology},
  keywords  = {*Cognition, *Schizophrenia, *Scientific Communication, Neuropsychopharmacology},
  pages     = {788--789},
  priority  = {prio3},
  publisher = {Elsevier Science},
  refid     = {2013-29974-009},
  volume    = {23},
  year      = {2013},
}

@Misc{,
  author    = {Wang, Luyao and Li, Chunlin and Wu, Jinglong},
  title     = {The status of research into intention recognition.},
  doi       = {10.4018/978-1-5225-0925-7.ch010},
  procedure = {Terapia},
  status    = {Aceito},
  abstract  = {In recent years, service robots have been widely used in many fields, especially for assisting the elderly and disabled. For example, the medical care of patients with Alzheimer’s disease has become a worldwide problem. Existing service robots with some intelligence quotient can perform actions that are programmed by a human. However, the robot cannot understand human intentions or communicate with people naturally. Understanding the intent of the service object could allow the robot to provide better service. Therefore, the most critical component of human-computer interactions is intention recognition. There are currently many methods by which intention recognition can be achieved, such as EMG, EOG and EEG. In addition, emotion is one of the important factors during intention recognition, and this has been a breakthrough notion. This chapter summarizes the current status of research into intention recognition and gives a brief description of the relationship between emotion and intention. We hope to provide more ideas for optimizing human-computer interactions. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Hershey, PA, US},
  issn      = {978-1-522-50925-7 (Hardcover); 978-1-522-50926-4 (Digital (undefined format))},
  journal   = {Improving the quality of life for dementia patients through progressive detection, treatment, and care.},
  keywords  = {*Automated Information Processing, *Emotions, *Experimentation, *Intention, *Robotics, Electroencephalography},
  pages     = {201--221},
  priority  = {prio3},
  publisher = {Medical Information Science Reference/IGI Global},
  refid     = {2016-59877-010},
  series    = {Advances in psychology, mental health, and behavioral studies (APMHBS) book series.},
  year      = {2017},
}

@Misc{,
  author    = {Mondillon, Laurie and Mermillod, Martial and Musca, Serban C. and Rieu, Isabelle and Vidal, Tiphaine and Chambres, Patrick and Auxiette, Catherine and Dalens, Hélène and Marie Coulangeon, Louise and Jalenques, Isabelle and Lemaire, Jean-Jacques and Ulla, Miguel and Derost, Philippe and Marques, Ana and Durif, Franck},
  title     = {The combined effect of subthalamic nuclei deep brain stimulation and L-dopa increases emotion recognition in Parkinson’s disease.},
  doi       = {10.1016/j.neuropsychologia.2012.08.016},
  status    = {Rejeitado - Escopo},
  abstract  = {Deep brain stimulation of the subthalamic nucleus (DBS) is a widely used surgical technique to suppress motor symptoms in Parkinson’s disease (PD), and as such improves patients’ quality of life. However, DBS may produce emotional disorders such as a reduced ability to recognize emotional facial expressions (EFE). Previous studies have not considered the fact that DBS and L-dopa medication can have differential, common, or complementary consequences on EFE processing. A thorough way of investigating the effect of DBS and L-dopa medication in greater detail is to compare patients’ performances after surgery, with the two therapies either being administered (‘on’) or not administered (‘off’). We therefore used a four-condition (L-dopa ‘on’/DBS ‘on’, L-dopa ‘on’/DBS ‘off’, L-dopa ‘off’/DBS ‘on’, and L-dopa ‘off’/DBS ‘off’) EFE recognition paradigm and compared implanted PD patients to healthy controls. The results confirmed those of previous studies, yielding a significant impairment in the detection of some facial expressions relative to controls. Disgust recognition was impaired when patients were ‘off’ L-dopa and ‘on’ DBS, and fear recognition impaired when ‘off’ of both therapies. More interestingly, the combined effect of both DBS and L-dopa administration seems much more beneficial for EFE recognition than the separate administration of each individual therapy. We discuss the implications of these findings in the light of the inverted U curve function that describes the differential effects of dopamine level on the right orbitofrontal cortex (OFC). We propose that, while L-dopa could "overdose" in dopamine the ventral stream of the OFC, DBS would compensate for this over-activation by decreasing OFC activity, thereby restoring the necessary OFC-amygdala interaction. Another finding is that, when collapsing over all treatment conditions, PD patients recognized more neutral faces than the matched controls, a result that concurs with embodiment theories. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  address   = {Mondillon, Laurie: LAPSCO, Blaise Pascal University, Clermont-Ferrand, France, 63000, Laurie.Mondillon@univ-savoie.fr},
  issn      = {1873-3514(Electronic),0028-3932(Print)},
  journal   = {Neuropsychologia},
  keywords  = {*Levodopa, *Parkinson's Disease, *Deep Brain Stimulation, *Subthalamic Nucleus, Emotion Recognition},
  pages     = {2869--2879},
  publisher = {Elsevier Science},
  refid     = {2012-28014-019},
  volume    = {50},
  year      = {2012},
}

@Misc{,
  author    = {Pablos, Samuel Marcos and García-Bermejo, Jaime Gómez and Casanova, Eduardo Zalama and López, Joaquín},
  title     = {Dynamic facial emotion recognition oriented to HCI applications.},
  doi       = {10.1093/iwc/iwt057},
  procedure = {Avaliação Psicológica},
  status    = {Aceito},
  abstract  = {As part of a multimodal animated avatar previously presented in Marcos-Pablos et al. ((2010) A realistic, virtual head for human-computer interaction. Interact. Comput., 22, 176-192, ISSN 0953-5438), in this paper we describe a method for dynamic recognition of displayed facial emotions on low-resolution streaming images. First, we address the detection of action units (AUs) of the facial action coding system using active shape models and Gabor filters. Normalized outputs of the AU recognition step are then used as inputs for a neural network that consists of an habituation network plus a competitive network. Both the competitive and the habituation layer use differential equations, thus taking into account the dynamic information of facial expressions through time. Experimental results carried out on live video sequences and on the Cohn-Kanade face database show that the proposed method provides high recognition hit rates. To assess the suitability of the developed emotional recognition system for human-computer interaction applications, it has been successfully integrated in the architecture of an avatar and we have conducted a preliminary experiment on empathy. The experiment showed promising results, as the avatar that made use of the emotional recognition system obtained a clear increase in the positivity of the rating when compared with the same avatar with no emotional response. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Pablos, Samuel Marcos: CARTIF Foundation, Division of Robotics and Computer Vision, Parque Tecnologico de Boecillo, 205 Boecillo, Valladolid, Spain, 47151, sammar@cartif.es},
  issn      = {1873-7951(Electronic),0953-5438(Print)},
  journal   = {Interacting with Computers},
  keywords  = {*Facial Expressions, *Graphical Displays, *Human Computer Interaction, *Intelligent Tutoring Systems, *Emotion Recognition, Algorithms, Avatars},
  pages     = {99--119},
  priority  = {prio3},
  publisher = {Oxford University Press},
  refid     = {2015-09096-002},
  volume    = {27},
  year      = {2015},
}

@Misc{,
  author    = {Lobmaier, Janek S. and Fischer, Martin H.},
  title     = {Facial feedback affects perceived intensity but not quality of emotional expressions.},
  doi       = {10.3390/brainsci5030357},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {Motivated by conflicting evidence in the literature, we re-assessed the role of facial feedback when detecting quantitative or qualitative changes in others’ emotional expressions. Fifty-three healthy adults observed self-paced morph sequences where the emotional facial expression either changed quantitatively (i.e., sad-to-neutral, neutral-to-sad, happy-to-neutral, neutral-to-happy) or qualitatively (i.e. from sad to happy, or from happy to sad). Observers held a pen in their own mouth to induce smiling or frowning during the detection task. When morph sequences started or ended with neutral expressions we replicated a congruency effect: Happiness was perceived longer and sooner while smiling; sadness was perceived longer and sooner while frowning. Interestingly, no such congruency effects occurred for transitions between emotional expressions. These results suggest that facial feedback is especially useful when evaluating the intensity of a facial expression, but less so when we have to recognize which emotion our counterpart is expressing. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Lobmaier, Janek S.: Institute of Psychology, University of Bern, Fabrikstrasse 8, Bern, Switzerland, 3012, janek.lobmaier@psy.unibe.ch},
  issn      = {2076-3425(Electronic)},
  journal   = {Brain Sciences},
  keywords  = {*Facial Expressions, *Feedback, *Emotion Recognition, Cognition},
  pages     = {357--368},
  priority  = {prio2},
  publisher = {Multidisciplinary Digital Publishing Institute},
  refid     = {2015-45397-004},
  volume    = {5},
  year      = {2015},
}

@Misc{,
  author    = {Belkaid, Marwen and Cuperlier, Nicolas and Gaussier, Philippe},
  title     = {Emotional metacontrol of attention: Top-down modulation of sensorimotor processes in a robotic visual search task.},
  doi       = {10.1371/journal.pone.0184960},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {Emotions play a significant role in internal regulatory processes. In this paper, we advocate four key ideas. First, novelty detection can be grounded in the sensorimotor experience and allow higher order appraisal. Second, cognitive processes, such as those involved in self-assessment, influence emotional states by eliciting affects like boredom and frustration. Third, emotional processes such as those triggered by self-assessment influence attentional processes. Last, close emotion-cognition interactions implement an efficient feedback loop for the purpose of top-down behavior regulation. The latter is what we call 'Emotional Metacontrol'. We introduce a model based on artificial neural networks. This architecture is used to control a robotic system in a visual search task. The emotional metacontrol intervenes to bias the robot visual attention during active object recognition. Through a behavioral and statistical analysis, we show that this mechanism increases the robot performance and fosters the exploratory behavior to avoid deadlocks. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Belkaid, Marwen: marwen.belkaid@ensea.fr},
  issn      = {1932-6203(Electronic)},
  journal   = {PLoS ONE},
  keywords  = {*Attention, *Emotions, *Object Recognition, *Visual Search, *Artificial Neural Networks, Perceptual Motor Processes, Robotics},
  priority  = {prio2},
  publisher = {Public Library of Science},
  refid     = {2018-17743-001},
  volume    = {12},
  year      = {2017},
}

@Misc{,
  author    = {Lamping, Donna L. and Spring, Bonnie and Gelenberg, Alan J.},
  title     = {Effects of two antidepressants on memory performance in depressed outpatients: A double-blind study.},
  doi       = {10.1007/BF00427455},
  status    = {Rejeitado - Escopo},
  abstract  = {Outpatients with primary depression received either amitriptyline (50-243 mg/day [n = 9, mean age 37.95 yrs]) or clovoxamine (60-238 mg/day [n = 21, mean age 31.19 yrs]) in a double-blind study of the effects of the 2 antidepressants on memory performance. During the 28-day study period, Ss were administered a signal-detection-recognition-memory task, the Benton Visual Retention Test, and the Wechsler Memory Scale on Days 4, 7, and 28. Results from the signal-detection task suggest an amitriptyline-associated memory impairment. The conventional memory measures failed to detect differential effects on memory of the 2 antidepressants. (44 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Germany},
  issn      = {1432-2072(Electronic),0033-3158(Print)},
  journal   = {Psychopharmacology},
  keywords  = {*Amitriptyline, *Antidepressant Drugs, *Depression (Emotion), *Memory, *Side Effects (Drug), Drug Therapy},
  pages     = {254--261},
  publisher = {Springer},
  refid     = {1985-26177-001},
  volume    = {84},
  year      = {1984},
}

@Misc{,
  author    = {Zeng, Xuemei and Wu, Qi and Zhang, Siwei and Liu, Zheying and Zhou, Qing and Zhang, Meishan},
  title     = {A false trail to follow: Differential effects of the facial feedback signals from the upper and lower face on the recognition of micro-expressions.},
  doi       = {10.3389/fpsyg.2018.02015},
  procedure = {Observação},
  status    = {Aceito},
  abstract  = {Micro-expressions, as fleeting facial expressions, are very important for judging people’s true emotions, thus can provide an essential behavioral clue for lie and dangerous demeanor detection. From embodied accounts of cognition, we derived a novel hypothesis that facial feedback from upper and lower facial regions has differential effects on micro-expression recognition. This hypothesis was tested and supported across three studies. Specifically, the results of Study 1 showed that people became better judges of intense micro-expressions with a duration of 450 ms when the facial feedback from upper face was enhanced via a restricting gel. Additional results of Study 2 showed that the recognition accuracy of subtle micro-expressions was significantly impaired under all duration conditions (50, 150, 333, and 450 ms) when facial feedback from lower face was enhanced. In addition, the results of Study 3 also revealed that blocking the facial feedback of lower face, significantly boosted the recognition accuracy of subtle and intense micro-expressions under all duration conditions (150 and 450 ms). Together, these results highlight the role of facial feedback in judging the subtle movements of micro-expressions. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  address   = {Wu, Qi: sandwich624@yeah.net},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Emotions, *Facial Expressions, *Feedback, Emotion Recognition},
  priority  = {prio2},
  publisher = {Frontiers Media S.A.},
  refid     = {2018-55617-001},
  volume    = {9},
  year      = {2018},
}

@Misc{,
  author    = {Korb, Sebastian and Malsert, Jennifer and Rochas, Vincent and Rihs, Tonia A. and Rieger, Sebastian W. and Schwab, Samir and Niedenthal, Paula M. and Grandjean, Didier},
  title     = {Gender differences in the neural network of facial mimicry of smiles--An rTMS study.},
  doi       = {10.1016/j.cortex.2015.06.025},
  status    = {Rejeitado - Escopo},
  abstract  = {Under theories of embodied emotion, exposure to a facial expression triggers facial mimicry. Facial feedback is then used to recognize and judge the perceived expression. However, the neural bases of facial mimicry and of the use of facial feedback remain poorly understood. Furthermore, gender differences in facial mimicry and emotion recognition suggest that different neural substrates might accompany the production of facial mimicry, and the processing of facial feedback, in men and women. Here, repetitive transcranial magnetic stimulation (rTMS) was applied to the right primary motor cortex (M1), the right primary somatosensory cortex (S1), or, in a control condition, the vertex (VTX). Facial mimicry of smiles and emotion judgments were recorded in response to video clips depicting changes from neutral or angry to happy facial expressions. While in females rTMS over M1 and S1 compared to VTX led to reduced mimicry and, in the case of M1, delayed detection of smiles, there was no effect of TMS condition for males. We conclude that in female participants M1 and S1 play a role in the mimicry and in the use of facial feedback for accurate processing of smiles. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Korb, Sebastian: SISSA, Via Bonomea 265, Trieste, Italy, 34136, skorb@sissa.it},
  issn      = {1973-8102(Electronic),0010-9452(Print)},
  journal   = {Cortex: A Journal Devoted to the Study of the Nervous System and Behavior},
  keywords  = {*Human Sex Differences, *Neural Networks, *Neurology, *Somatosensory Cortex, *Transcranial Magnetic Stimulation, Smiles},
  pages     = {101--114},
  publisher = {Elsevier Masson SAS},
  refid     = {2015-39377-011},
  volume    = {70},
  year      = {2015},
}

@Misc{,
  author    = {Grotegerd, Dominik and Stuhrmann, Anja and Kugel, Harald and Schmidt, Simone and Redlich, Ronny and Zwanzger, Peter and Rauch, Astrid Veronika and Heindel, Walter and Zwitserlood, Pienie and Arolt, Volker and Suslow, Thomas and Dannlowski, Udo},
  title     = {Amygdala excitability to subliminally presented emotional faces distinguishes unipolar and bipolar depression: An fMRI and pattern classification study.},
  doi       = {10.1002/hbm.22380},
  status    = {Rejeitado - Escopo},
  abstract  = {Bipolar disorder and Major depressive disorder are difficult to differentiate during depressive episodes, motivating research for differentiating neurobiological markers. Dysfunctional amygdala responsiveness during emotion processing has been implicated in both disorders, but the important rapid and automatic stages of emotion processing in the amygdala have so far never been investigated in bipolar patients. Methods fMRI data of 22 bipolar depressed patients (BD), 22 matched unipolar depressed patients (MDD), and 22 healthy controls (HC) were obtained during processing of subliminal sad, happy and neutral faces. Amygdala responsiveness was investigated using standard univariate analyses as well as pattern‐recognition techniques to differentiate the two clinical groups. Furthermore, medication effects on amygdala responsiveness were explored. Results All subjects were unaware of the emotional faces. Univariate analysis revealed a significant group × emotion interaction within the left amygdala. Amygdala responsiveness to sad > neutral faces was increased in MDD relative to BD. In contrast, responsiveness to happy > neutral faces showed the opposite pattern, with higher amygdala activity in BD than in MDD. Most of the activation patterns in both clinical groups differed significantly from activation patterns of HC--and therefore represent abnormalities. Furthermore, pattern classification on amygdala activation to sad > happy faces yielded almost 80% accuracy differentiating MDD and BD patients. Medication had no significant effect on these findings. Conclusions Distinct amygdala excitability during automatic stages of the processing of emotional faces may reflect differential pathophysiological processes in BD versus MDD depression, potentially representing diagnosis‐specific neural markers mostly unaffected by current psychotropic medication. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Dannlowski, Udo: Department of Psychiatry, University of Munster, Albert-Schweitzer-Campus 1 A9, Munster, Germany, 48149, dannlow@uni-muenster.de},
  issn      = {1097-0193(Electronic),1065-9471(Print)},
  journal   = {Human Brain Mapping},
  keywords  = {*Parietal Lobe, *Prefrontal Cortex, *Reasoning, *Simulation, *Medial Prefrontal Cortex, False Beliefs, Theory of Mind},
  pages     = {2995--3007},
  publisher = {John Wiley & Sons},
  refid     = {2014-17724-001},
  volume    = {35},
  year      = {2014},
}

@Misc{,
  author    = {Hamstra, Danielle A. and De Rover, Mischa and De Rijk, Roel H. and Van der Does, Willem},
  title     = {Oral contraceptives may alter the detection of emotions in facial expressions.},
  doi       = {10.1016/j.euroneuro.2014.08.015},
  status    = {Rejeitado - Escopo},
  abstract  = {A possible effect of oral contraceptives on emotion recognition was observed in the context of a clinical trial with a corticosteroid. Users of oral contraceptives detected significantly fewer facial expressions of sadness, anger and disgust than non-users. This was true for trial participants overall as well as for those randomized to placebo. Although it is uncertain whether this is an effect of oral contraceptives or a pre-existing difference, future studies on the effect of interventions should control for the effects of oral contraceptives on emotional and cognitive outcomes. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  address   = {Van der Does, Willem: Institute of Psychology, Clinical Psychology Department, Leiden University, Wassenaarseweg 52, Leiden, Netherlands, 2333 AK, vanderdoes@fsw.leidenuniv.nl},
  issn      = {1873-7862(Electronic),0924-977X(Print)},
  journal   = {European Neuropsychopharmacology},
  keywords  = {*Corticosteroids, *Emotions, *Facial Expressions, *Oral Contraceptives, Clinical Trials, Estrogens, Progesterone},
  pages     = {1855--1859},
  publisher = {Elsevier Science},
  refid     = {2014-39018-001},
  volume    = {24},
  year      = {2014},
}
