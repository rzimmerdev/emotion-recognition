BibliographyType,ISBN,Identifier,Author,Title,Journal,Volume,Number,Month,Pages,Year,Address,Note,URL,Booktitle,Chapter,Edition,Series,Editor,Publisher,ReportType,Howpublished,Institution,Organizations,School,Annote,Custom1,Custom2,Custom3,Custom4,Custom5
6,"","Kahou2015","Kahou, Samira Ebrahimi; Michalski, Vincent; Konda, Kishore; Memisevic, Roland; Pal, Christopher","Recurrent Neural Networks for Emotion Recognition in Video","",,,"nov","",2015,"","","","Proceedings of the 2015 {ACM} on International Conference on Multimodal Interaction","","","","","{ACM}","","","","","","","","","","",""
7,"","Mavadati2013","Mavadati, S. Mohammad; Mahoor, Mohammad H.; Bartlett, Kevin; Trinh, Philip; Cohn, Jeffrey F.","DISFA: A Spontaneous Facial Action Intensity Database","{IEEE} Transactions on Affective Computing",4,2,"apr","151--160",2013,"","","","","","","","","Institute of Electrical and Electronics Engineers ({IEEE})","","","","","","","","","","",""
6,"","Taigman2014","Taigman, Yaniv; Yang, Ming; Ranzato, Marc{\textquotesingle}Aurelio; Wolf, Lior","DeepFace: Closing the Gap to Human-Level Performance in Face Verification","",,,"jun","",2014,"","","","2014 {IEEE} Conference on Computer Vision and Pattern Recognition","","","","","{IEEE}","","","","","","","","","","",""
7,"","WOS:000442238900013","Sreedharan, Ninu Preetha Nirmala; Ganesan, Brammya; Raveendran, Ramya; Sarala, Praveena; Dennis, Binu; Boothalingam, Rajakumar R.","Grey Wolf optimisation-based feature selection and classification for facial emotion recognition","IET BIOMETRICS",7,5,"SEP","490-499",2018,"MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND","","","","","","","","INST ENGINEERING TECHNOLOGY-IET","","","","","","","The channels used to convey the human emotions consider actions,    behaviours, poses, facial expressions, and speech. An immense research    has been carried out to analyse the relationship between the facial    emotions and these channels. The goal of this study is to develop a    system for Facial Emotion Recognition (FER) that can analyse the    elemental facial expressions of human, such as normal, smile, sad,    surprise, anger, fear, and disgust. The recognition process of the    proposed FER system is categorised into four processes, namely    pre-processing, feature extraction, feature selection, and    classification. After preprocessing, scale invariant feature transform    -based feature extraction method is used to extract the features from    the facial point. Further, a meta-heuristic algorithm called Grey Wolf    optimisation (GWO) is used to select the optimal features. Subsequently,    GWO-based neural network (NN) is used to classify the emotions from the    selected features. Moreover, an effective performance analysis of the    proposed as well as the conventional methods such as convolutional    neural network, NN-Levenberg-Marquardt, NN-Gradient Descent,    NN-Evolutionary Algorithm, NN-firefly, and NN-Particle Swarm    Optimisation is provided by evaluating few performance measures and    thereby, the effectiveness of the proposed strategy over the    conventional methods is validated.","","face recognition; gradient methods; feature extraction; evolutionary computation; particle swarm optimisation; neural nets; optimisation; emotion recognition; Grey Wolf optimisation; feature selection; facial emotion recognition; human emotions; facial emotions; elemental facial expressions; fear; recognition process; FER system; pre-processing; scale invariant feature; feature extraction method; facial point; optimal features; NN-particle swarm optimisation","",""
7,"","WOS:000447336700033","Luis Espinosa-Aranda, Jose; Vallez, Noelia; Maria Rico-Saavedra, Jose; Parra-Patino, Javier; Bueno, Gloria; Sorci, Matteo; Moloney, David; Pena, Dexmont; Deniz, Oscar","Smart Doll: Emotion Recognition Using Embedded Deep Learning","SYMMETRY-BASEL",10,9,"SEP","",2018,"ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","","","","","","","","MDPI","","","","","","","Computer vision and deep learning are clearly demonstrating a capability    to create engaging cognitive applications and services. However, these    applications have been mostly confined to powerful Graphic Processing    Units (GPUs) or the cloud due to their demanding computational    requirements. Cloud processing has obvious bandwidth, energy consumption    and privacy issues. The Eyes of Things (EoT) is a powerful and versatile    embedded computer vision platform which allows the user to develop    artificial vision and deep learning applications that analyse images    locally. In this article, we use the deep learning capabilities of an    EoT device for a real-life facial informatics application: a doll    capable of recognizing emotions, using deep learning techniques, and    acting accordingly. The main impact and significance of the presented    application is in showing that a toy can now do advanced processing    locally, without the need of further computation in the cloud, thus    reducing latency and removing most of the ethical issues involved.    Finally, the performance of the convolutional neural network developed    for that purpose is studied and a pilot was conducted on a panel of 12    children aged between four and ten years old to test the doll.","CK+ dataset
Menciona Ekman","facial informatics; deep learning; computer vision; mobile applications; real-time and embedded systems","",""
7,"","WOS:000451598900403","Leo, Marco; Carcagni, Pierluigi; Distante, Cosimo; Spagnolo, Paolo; Mazzeo, Pier Luigi; Rosato, Anna Chiara; Petrocchi, Serena; Pellegrino, Chiara; Levante, Annalisa; De Lume, Filomena; Lecciso, Flavia","Computational Assessment of Facial Expression Production in ASD Children","SENSORS",18,11,"NOV","",2018,"ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","","","","","","","","MDPI","","","","","","","In this paper, a computational approach is proposed and put into    practice to assess the capability of children having had diagnosed    Autism Spectrum Disorders (ASD) to produce facial expressions. The    proposed approach is based on computer vision components working on    sequence of images acquired by an off-the-shelf camera in unconstrained    conditions. Action unit intensities are estimated by analyzing local    appearance and then both temporal and geometrical relationships, learned    by Convolutional Neural Networks, are exploited to regularize gathered    estimates. To cope with stereotyped movements and to highlight even    subtle voluntary movements of facial muscles, a personalized and    contextual statistical modeling of non-emotional face is formulated and    used as a reference. Experimental results demonstrate how the proposed    pipeline can improve the analysis of facial expressions produced by ASD    children. A comparison of system's outputs with the evaluations    performed by psychologists, on the same group of ASD children, makes    evident how the performed quantitative analysis of children's abilities    helps to go beyond the traditional qualitative ASD assessment/diagnosis    protocols, whose outcomes are affected by human limitations in observing    and understanding multi-cues behaviors such as facial expressions.","","quantitative facial expression analysis; geometrical and temporal regularization of facial action units; ASD diagnosis and assessment","",""
7,"","WOS:000455331200022","Ozturk, Mahiye Uluyagmur; Arman, Ayse Rodopman; Bulut, Gresa Carkaxhiu; Findik, Onur Tugce Poyraz; Yilmaz, Sultan Seval; Genc, Herdem Aslan; Yazgan, M. Yanki; Teker, Umut; Cataltepe, Zehra","Statistical Analysis and Multimodal Classification on Noisy Eye Tracker and Application Log Data of children with Autism and ADHD","INTELLIGENT AUTOMATION AND SOFT COMPUTING",24,4,"DEC","891-906",2018,"871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA","","","","","","","","TECH SCIENCE PRESS","","","","","","","Emotion recognition behavior and performance may vary between people    with major neurodevelopmental disorders such as Autism Spectrum Disorder    (ASD), Attention Deficit Hyperactivity Disorder (ADHD) and control    groups. It is crucial to identify these differences for early diagnosis    and individual treatment purposes. This study represents a methodology    by using statistical data analysis and machine learning to provide help    to psychiatrists and therapists on the diagnosis and individualized    treatment of participants with ASD and ADHD. In this paper we propose an    emotion recognition experiment environment and collect eye tracker    fixation data together with the application log data (APL). In order to    detect the diagnosis of the participant we used classification    algorithms with the Tomek links noise removing method. The highest    classification accuracy results were reported as 86.36\% for ASD vs.    Control, 81.82\% for ADHD vs. Control and 70.83\% for ASD vs. ADHD. This    study provides evidence that fixation and APL data have distinguishing    features for the diagnosis of ASD and ADHD.","","Classification of Medical Diagnosis; Emotion Recognition Ability; Eye Tracking; Noise Removal","",""
7,"","WOS:000457525200014","Khan, Rizwan Ahmed; Meyer, Alexandre; Konik, Hubert; Bouakaz, Saida","Saliency-based framework for facial expression recognition","FRONTIERS OF COMPUTER SCIENCE",13,1,"FEB","183-198",2019,"CHAOYANG DIST, 4, HUIXINDONGJIE, FUSHENG BLDG, BEIJING 100029, PEOPLES R CHINA","","","","","","","","HIGHER EDUCATION PRESS","","","","","","","This article proposes a novel framework for the recognition of six    universal facial expressions. The framework is based on three sets of    features extracted from a face image: entropy, brightness, and local    binary pattern. First, saliency maps are obtained using the    state-of-the-art saliency detection algorithm frequency-tuned salient    region detection. The idea is to use saliency maps to determine    appropriate weights or values for the extracted features (i.e.,    brightness and entropy). We have performed a visual experiment to    validate the performance of the saliency detection algorithm against the    human visual system. Eye movements of 15 subjects were recorded using an    eye-tracker in free-viewing conditions while they watched a collection    of 54 videos selected from the Cohn-Kanade facial expression database.    The results of the visual experiment demonstrated that the obtained    saliency maps are consistent with the data on human fixations. Finally,    the performance of the proposed framework is demonstrated via    satisfactory classification results achieved with the Cohn-Kanade    database, FG-NET FEED database, and Dartmouth database of children's    faces.","","facial expression recognition; classification; salient regions; entropy; brightness; local binary pattern","",""
6,"978-1-5386-6420-9","WOS:000457843602030","Marinoiu, Elisabeta; Zanfir, Mihai; Olaru, Vlad; Sminchisescu, Cristian","3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children with Autism","",,,"","2158-2167",2018,"345 E 47TH ST, NEW YORK, NY 10017 USA","31st IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, JUN 18-23, 2018","","2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)","","","IEEE Conference on Computer Vision and Pattern Recognition","","IEEE","","","","IEEE; CVF; IEEE Comp Soc","","","We introduce new, fine-grained action and emotion recognition tasks    defined on non-staged videos, recorded during robot-assisted therapy    sessions of children with autism. The tasks present several challenges:    a large dataset with long videos, a large number of highly variable    actions, children that are only partially visible, have different ages    and may show unpredictable behaviour, as well as non-standard camera    viewpoints. We investigate how state-of-the-art 3d human pose    reconstruction methods perform on the newly introduced tasks and propose    extensions to adapt them to deal with these challenges. We also analyze    multiple approaches in action and emotion recognition from 3d human pose    data, establish several baselines, and discuss results and their    implications in the context of child-robot interaction.","","","",""
7,"","WOS:000460064700015","Camacho, M. Catalina; Karim, Helmet T.; Perlman, Susan B.","Neural architecture supporting active emotion processing in children: A multivariate approach","NEUROIMAGE",188,,"MAR","171-180",2019,"525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA","","","","","","","","ACADEMIC PRESS INC ELSEVIER SCIENCE","","","","","","","Background: Adaptive emotion processing is critical for nearly all    aspects of social and emotional functioning. There are distinct    developmental trajectories associated with improved emotion processing,    with a protracted developmental course for negative or complex emotions.    The specific changes in neural circuitry that underlie this development,    however are still scarcely understood. We employed a multivariate    approach in order to elucidate distinctions in complex, naturalistic    emotion processing between childhood and adulthood.    Method: Twenty-one adults (M +/- SD age = 26.57 +/- 5.08 years) and    thirty children (age = 7.75 +/- 1.80 years) completed a free-viewing    movie task during BOLD fMRI scanning. This task was designed to assess    naturalistic processing of movie clips portraying positive, negative,    and neutral emotions. Multivariate support vector machines (SVM) were    trained to classify age groups based on neural activation during the    task.    Results: SVMs were able to successfully classify condition (positive,    negative, and neutral) across all participants with high accuracy    (61.44\%). SVMs could successfully distinguish adults and children    within each condition (ps < 0.05). Regions that informed the age group    SVMs were associated with sensory and socio-emotional processing    (inferior parietal lobule), emotion regulation (inferior frontal gyrus),    and sensory regions of the temporal and occipital lobes.    Conclusions: These results point to distributed differences in    activation between childhood and adulthood unique to each emotional    condition. In the negative condition specifically, there is evidence for    a shift in engagement from regions of sensory and socio-emotional    integration to emotion regulation regions between children and adults.    These results provide insight into circuitry contributing to maturation    of emotional processing across development.","","Emotion processing; Development; Machine learning; Naturalistic viewing; Children","",""
6,"978-1-5386-7266-2","WOS:000461314200089","Ul Haque, Md Inzamam; Valles, Damian","A Facial Expression Recognition Approach Using DCNN for Autistic Children to Identify Emotions","",,,"","546-551",2018,"345 E 47TH ST, NEW YORK, NY 10017 USA","9th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), Univ British Columbia, Vancouver, CANADA, NOV 01-03, 2018","https://ieeexplore.ieee.org/document/8614802","2018 IEEE 9TH ANNUAL INFORMATION TECHNOLOGY, ELECTRONICS AND MOBILE COMMUNICATION CONFERENCE (IEMCON)","","","","Chakrabarti, S.; Saha, H. N.","IEEE","","","","Inst Engn \& Management; IEEE Vancouver Sect; UBC; Univ Engn \& Management","","","In this paper, an initial work of a research is discussed which is to    teach young autistic children recognizing human facial expression with    the help of computer vision and image processing. This paper mostly    discusses the initial work of facial expression recognition using a deep    convolutional neural network. The Kaggle's FER2013 dataset has been used    to train and experiment with a deep convolutional neural network model.    Once a satisfactory result is achieved, the dataset is modified with    pictures of four different lighting conditions and each of these    datasets is again trained with the same model. This is necessary for the    end goal of the research which is to recognize facial expression in any    possible environment. Finally, the comparison between results with    different datasets is discussed and future work of the project is    outlined.","FER2013 dataset","Facial Expression Recognition; Autistic Children; DCNN; Loss; Accuracy","",""
7,"","WOS:000466824000006","Khan, Rizwan Ahmed; Crenn, Arthur; Meyer, Alexandre; Bouakaz, Saida","A novel database of children's spontaneous facial expressions (LIRIS-CSE)","IMAGE AND VISION COMPUTING",83-84,,"MAR-APR","61-69",2019,"RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","","","","","","","","ELSEVIER","","","","","","","Computing environment is moving towards human-centered designs instead    of computer centered designs and human's tend to communicate wealth of    information through affective states or expressions. Traditional Human    Computer Interaction (HCI) based systems ignores bulk of information    communicated through those affective states and just caters for user's    intentional input. Generally, for evaluating and benchmarking different    facial expression analysis algorithms, standardized databases are needed    to enable a meaningful comparison. In the absence of comparative tests    on such standardized databases it is difficult to find relative    strengths and weaknesses of different facial expression recognition    algorithms. In this article we present a novel video database for    Children's Spontaneous facial Expressions (LIRIS-CSE). Proposed video    database contains six basic spontaneous facial expressions shown by 12    ethnically diverse children between the ages of 6 and 12 years with mean    age of 7.3 years. To the best of our knowledge, this database is first    of its kind as it records and shows spontaneous facial expressions of    children. Previously there were few database of children expressions and    all of them show posed or exaggerated expressions which are different    from spontaneous or natural expressions. Thus, this database will be a    milestone for human behavior researchers. This database will be a    excellent resource for vision community for benchmarking and comparing    results. In this article, we have also proposed framework for automatic    expression recognition based on Convolutional Neural Network (CNN)    architecture with transfer learning approach. Proposed architecture    achieved average classification accuracy of 75\% on our proposed    database i.e. LIRIS-CSE. (C) 2019 Elsevier B.V. All rights reserved.","LIRIS-CSE dataset","Facial expressions database; Spontaneous expressions; Convolutional neural network; Expression recognition; Transfer learning","",""
6,"978-1-7281-1114-8","WOS:000470221500006","Irani, Atefeh; Moradi, Hadi; Vahid, Leila Kashani","Autism Screening Using a Video Game Based on Emotions","",,,"","40-45",2018,"345 E 47TH ST, NEW YORK, NY 10017 USA","2nd National and 1st International Digital Games Research Conference - Trends, Technologies, and Applications (DGRC), Tehran, IRAN, NOV 29-30, 2018","","2018 2ND NATIONAL AND 1ST INTERNATIONAL DIGITAL GAMES RESEARCH CONFERENCE: TRENDS, TECHNOLOGIES, AND APPLICATIONS (DGRC)","","","","","IEEE","","","","DIREC; Iran Comp \& Video Games Fdn; Univ Islam Republ Iran Broadcasting; Digital Games Res Ctr","","","Autism spectrum disorders (ASDs) is referred to as a range of mental and    physical. Usually these children deal with disorders such as social    problems, repeated patterns of behaviors and inability to understand    abstract concepts. If at the very childhood the occupational therapy    exercises are done on a person with autism, he can return to normal    life. Given the importance of early diagnosis of autism, this study    presents the structure of a serious smart game for screening and    rehabilitation of autism. This computer game is based on training of    three important skills in understanding emotions i. e. the recognition    of emotions, the emergence of emotions, and emotion regulation. The game    was performed on children with autism and normal children for designing    pattern-recognition algorithms for screening children with autism. Then    the screening precision reached 93\% using a support vector machine with    Gaussian kernel.","","autism; Serious Game; emotion expression; emotion recognition; emotion regulation; autism screening","",""
7,"","WOS:000477045000007","Goulart, Christiane; Valadao, Carlos; Delisle-Rodriguez, Denis; Funayama, Douglas; Favarato, Alvaro; Baldo, Guilherme; Binotte, Vinicius; Caldeira, Eliete; Bastos-Filho, Teodiano","Visual and Thermal Image Processing for Facial Specific Landmark Detection to Infer Emotions in a Child-Robot Interaction","SENSORS",19,13,"JUL 1","",2019,"ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","","","","","","","","MDPI","","","","","","","Child-Robot Interaction (CRI) has become increasingly addressed in    research and applications. This work proposes a system for emotion    recognition in children, recording facial images by both visual    (RGB-red, green and blue) and Infrared Thermal Imaging (IRTI) cameras.    For this purpose, the Viola-Jones algorithm is used on color images to    detect facial regions of interest (ROIs), which are transferred to the    thermal camera plane by multiplying a homography matrix obtained through    the calibration process of the camera system. As a novelty, we propose    to compute the error probability for each ROI located over thermal    images, using a reference frame manually marked by a trained expert, in    order to choose that ROI better placed according to the expert criteria.    Then, this selected ROI is used to relocate the other ROIs, increasing    the concordance with respect to the reference manual annotations.    Afterwards, other methods for feature extraction, dimensionality    reduction through Principal Component Analysis (PCA) and pattern    classification by Linear Discriminant Analysis (LDA) are applied to    infer emotions. The results show that our approach for ROI locations may    track facial landmarks with significant low errors with respect to the    traditional Viola-Jones algorithm. These ROIs have shown to be relevant    for recognition of five emotions, specifically disgust, fear, happiness,    sadness, and surprise, with our recognition system based on PCA and LDA    achieving mean accuracy (ACC) and Kappa values of 85.75\% and 81.84\%,    respectively. As a second stage, the proposed recognition system was    trained with a dataset of thermal images, collected on 28 typically    developing children, in order to infer one of five basic emotions    (disgust, fear, happiness, sadness, and surprise) during a child-robot    interaction. The results show that our system can be integrated to a    social robot to infer child emotions during a child-robot interaction.","","Viola-Jones; facial emotion recognition; facial expression recognition; facial detection; facial landmarks; infrared thermal imaging; homography matrix; socially assistive robot","",""
7,"","WOS:000483067100012","Martinez, Aleix M.","The Promises and Perils of Automated Facial Action Coding in Studying Children's Emotions","DEVELOPMENTAL PSYCHOLOGY",55,9, SI,"SEP","1965-1981",2019,"750 FIRST ST NE, WASHINGTON, DC 20002-4242 USA","","","","","","","","AMER PSYCHOLOGICAL ASSOC","","","","","","","Computer vision algorithms have made tremendous advances in recent    years. We now have algorithms that can detect and recognize objects,    faces, and even facial actions in still images and video sequences. This    is wonderful news for researchers that need to code facial articulations    in large data sets of images and videos, because this task is time    consuming and can only be completed by expert coders, making it very    expensive. The availability of computer algorithms that can    automatically code facial actions in extremely large data sets also    opens the door to studies in psychology and neuroscience that were not    previously possible, for example, to study the development of the    production of facial expressions from infancy to adulthood within and    across cultures. Unfortunately, there is a lack of methodological    understanding on how these algorithms should and should not be used, and    on how to select the most appropriate algorithm for each study. This    article aims to address this gap in the literature. Specifically, we    present several methodologies for use in hypothesis-based and    exploratory studies, explain how to select the computer algorithms that    best fit to the requirements of our experimental design, and detail how    to evaluate whether the automatic annotations provided by existing    algorithms are trustworthy.","","facial action coding; facial expression; emotion; computer vision; machine learning","",""
6,"978-3-030-23207-8; 978-3-030-23206-1","WOS:000495603500014","Farzaneh, Amir Hossein; Kim, Yanghee; Zhou, Mengxi; Qi, Xiaojun","Developing a Deep Learning-Based Affect Recognition System for Young Children","",11626,,"","73-78",2019,"GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND","20th International Conference on Artificial Intelligence in Education (AIED), Chicago, IL, JUN 25-29, 2019","","ARTIFICIAL INTELLIGENCE IN EDUCATION, AIED 2019, PT II","","","Lecture Notes in Artificial Intelligence","Isotani, S.; Millan, E.; Ogan, A.; Hastings, P.; McLaren, B.; Luckin, R.","SPRINGER INTERNATIONAL PUBLISHING AG","","","","","","","Affective interaction in tutoring environments has been of great    interest among several researchers in this community, which has spurred    the development of various systems to capture learners' emotional    states. Young children are one of the biggest learner groups in digital    learning environments, but these studies have rarely targeted them. Our    current study leverages computer vision and deep learning to analyze    young childrens' learning-related affective states. We developed an    effective recognition system to compute the probability for a child to    present neutral or positive affective state. Our results showed that the    prototype was able to achieve an average affective state prediction    accuracy of 93.05\%.","","Emotion recognition; Deep learning; Computer vision; Young children; Learner affect","",""
6,"978-3-030-19651-6; 978-3-030-19650-9","WOS:000502114100045","Imbernon Cuadrado, Luis-Eduardo; Manjarres Riesco, Angeles; de la Paz Lopez, Felix","FER in Primary School Children for Affective Robot Tutors","",11487,,"","461-471",2019,"GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND","8th International Work-Conference on the Interplay Between Natural and Artificial Computation (IWINAC), Almeria, SPAIN, JUN 03-07, 2019","","FROM BIOINSPIRED SYSTEMS AND BIOMEDICAL APPLICATIONS TO MACHINE LEARNING, PT II","","","Lecture Notes in Computer Science","Vicente, J. M. F.; AlvarezSanchez, J. R.; Lopez, F. D.; Moreo, J. T.; Adeli, H.","SPRINGER INTERNATIONAL PUBLISHING AG","","","","Spanish CYTED; Red Nacl Computac Nat \& Artificial, Programa Grupos Excelencia Fundac Seneca \& Apliquem Microones 21 s l","","","In the last few years, robotics has attracted much interest as a tool to    support education through social interaction. Since Social-Emotional    Learning (SEL) influences academic success, affective robot tutors have    a great potential within education. In this article we report on our    research in recognition of facial emotional expressions, aimed at    improving ARTIE, an integrated environment for the development of    affective robot tutors. A Full Convolutional Neural Network (FCNN) model    has been trained with the Fer2013 dataset, and then validated with    another dataset containing facial images of primary school children,    which has been compiled during computing lab sessions. Our first    prototype recognizes primary school children facial emotional    expressions with 69,15\% accuracy. As a future work we intend to further    refine the ARTIE Emotional Component with a view to integrating the main    singularities of primary school children emotional expression.","","Emotion recognition; Affective robot tutors; Facial emotional expression; Social emotional learning","",""
6,"978-1-4503-6860-5","WOS:000518657800086","Li, Sunan; Zheng, Wenming; Zong, Yuan; Lu, Cheng; Tang, Chuangao; Jiang, Xingxun; Liu, Jiateng; Xia, Wanchuang","Bi-modality Fusion for Emotion Recognition in the Wild","",,,"","589-594",2019,"1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES","21st ACM International Conference on Multimodal Interaction (ICMI), Suzhou, PEOPLES R CHINA, OCT 14-18, 2019","","ICMI'19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION","","","","","ASSOC COMPUTING MACHINERY","","","","ACM SIGCHI; Assoc Comp Machinery; Openstream; Alibaba Grp; Microsoft; Baidu; Sensetime; Tencent YouTu Lab; AISpeech","","","The emotion recognition in the wild has been a hot research topic in the    field of affective computing. Though some progresses have been achieved,    the emotion recognition in the wild is still an unsolved problem due to    the challenge of head movement, face deformation, illumination variation    etc. To deal with these unconstrained challenges, we propose a    bimodality fusion method for video based emotion recognition in the    wild. The proposed framework takes advantages of the visual information    from facial expression sequences and the speech information from audio.    The state-of-the-art CNN based object recognition models are employed to    facilitate the facial expression recognition performance. A bi-direction    long short term Memory (Bi-LSTM) is employed to capture dynamic    information of the learned features. Additionally, to take full    advantages of the facial expression information, the VGG16 network is    trained on AffectNet dataset to learn a specialized facial expression    recognition model. On the other hand, the audio based features, like low    level descriptor (LLD) and deep features obtained by spectrogram image,    are also developed to improve the emotion recognition performance. The    best experimental result shows that the overall accuracy of our    algorithm on the Test dataset of the EmotiW challenge is 62.78\%, which    outperforms the best result of EmotiW2018 and ranks 2nd at the    EmotiW2019 challenge.","","Emotion Recognition; Deep Learning; Convolutional Neural Networks","",""
7,"","WOS:000523305700001","Kalantarian, Haik; Jedoui, Khaled; Dunlap, Kaitlyn; Schwartz, Jessey; Washington, Peter; Husic, Arman; Tariq, Qandeel; Ning, Michael; Kline, Aaron; Wall, Dennis Paul","The Performance of Emotion Classifiers for Children With Parent-Reported Autism: Quantitative Feasibility Study","JMIR MENTAL HEALTH",7,4,"APR 1","",2020,"130 QUEENS QUAY East, Unit 1100, TORONTO, ON M5A 0P6, CANADA","","","","","","","","JMIR PUBLICATIONS, INC","","","","","","","Background: Autism spectrum disorder (ASD) is a developmental disorder    characterized by deficits in social communication and interaction, and    restricted and repetitive behaviors and interests. The incidence of ASD    has increased in recent years; it is now estimated that approximately 1    in 40 children in the United States are affected. Due in part to    increasing prevalence, access to treatment has become constrained. Hope    lies in mobile solutions that provide therapy through artificial    intelligence (AI) approaches, including facial and emotion detection AI    models developed by mainstream cloud providers, available directly to    consumers. However, these solutions may not be sufficiently trained for    use in pediatric populations.    Objective: Emotion classifiers available off-the-shelf to the general    public through Microsoft, Amazon, Google, and Sighthound are well-suited    to the pediatric population, and could be used for developing mobile    therapies targeting aspects of social communication and interaction,    perhaps accelerating innovation in this space. This study aimed to test    these classifiers directly with image data from children with    parent-reported ASD recruited through crowdsourcing.    Methods: We used a mobile game called Guess What? that challenges a    child to act out a series of prompts displayed on the screen of the    smartphone held on the forehead of his or her care provider. The game is    intended to be a fun and engaging way for the child and parent to    interact socially, for example, the parent attempting to guess what    emotion the child is acting out (eg, surprised, scared, or disgusted).    During a 90-second game session, as many as 50 prompts are shown while    the child acts, and the video records the actions and expressions of the    child. Due in part to the fun nature of the game, it is a viable way to    remotely engage pediatric populations, including the autism population    through crowdsourcing. We recruited 21 children with ASD to play the    game and gathered 2602 emotive frames following their game sessions.    These data were used to evaluate the accuracy and performance of four    state-of-the-art facial emotion classifiers to develop an understanding    of the feasibility of these platforms for pediatric research.    Results: All classifiers performed poorly for every evaluated emotion    except happy. None of the classifiers correctly labeled over 60.18\%    (1566/2602) of the evaluated frames. Moreover, none of the classifiers    correctly identified more than 11\% (6/51) of the angry frames and 14\%    (10/69) of the disgust frames.    Conclusions: The findings suggest that commercial emotion classifiers    may be insufficiently trained for use in digital approaches to autism    treatment and treatment tracking. Secure, privacy-preserving methods to    increase labeled training data are needed to boost the models'    performance before they can be used in AI-enabled approaches to social    therapy of the kind that is common in autism treatments.","","mobile phone; emotion; autism; digital data; mobile app; mHealth; affect; machine learning; artificial intelligence; digital health","",""
6,"978-1-7281-0899-5","WOS:000542980800097","Rani, Pooja","Emotion Detection of Autistic Children Using Image Processing","",,,"","532-535",2019,"345 E 47TH ST, NEW YORK, NY 10017 USA","5th International Conference on Image Information Processing (ICIIP), Waknaghat, INDIA, NOV 15-17, 2019","https://ieeexplore.ieee.org/abstract/document/8985706","2019 FIFTH INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP 2019)","","","","Gupta, P. K.; Gandotra, E.; Tyagi, V.; Ghrera, S. P.; Sehgal, V. K.","IEEE","","","","IEEE; IEEE Jaypee Univ Informat Technol, Student Branch; IEEE Delhi Sect; Comp Soc India; Jaypee Grp; CSIR; Jaypee Univ Informat Technol, Dept CSE \& IT","","","Facial Emotion Detection is an approach towards detecting human emotions    through facial expressions. Autism Spectrum Disorder is an advance    neurobehavioral disorder. Autistic people have repetitive, rude    behavior. They are not ready to do social communication. People with    this syndrome have problems with emotion recognition. This paper works    on detecting the emotions of autistic children from the expression of    their faces. This paper works on four emotions. These emotions are sad,    happy, neutral, and angry. To detect the emotion of autistic children is    performed with image processing and machine learning algorithms. The    features are extracted from the faces of autistic children with local    binary pattern. Machine learning algorithms are used for classification    of emotions. Machine learning classifiers used in classification process    are support vector machine and neural network.","","Machine Learning; Support Vector Machine; Neural Network; Emotion Detection; Image Processing; Local Binary Pattern","",""
7,"","WOS:000549823900015","Jarraya, Salma Kammoun; Masmoudi, Marwa; Hammami, Mohamed","Compound Emotion Recognition of Autistic Children During Meltdown Crisis Based on Deep Spatio-Temporal Analysis of Facial Geometric Features","IEEE ACCESS",8,,"","69311-69326",2020,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","An important contribution to computer vision applications has been made    by recognizing human emotion. Although it is very significant, this work    considers the security of autistic people while in meltdown crisis by    introducing a new system to warn caregivers through facial expressions    detection. A precautionary approach has been taken to deal with meltdown    crisis. Certainly, the indications of Meltdown are linked to abnormal    facial expressions related to compound emotions. Actually, researchers    thought long ago that Human Facial Expressions (HFE) are not able to    express more than the seven basics emotions. HFE have been considered by    psychologists as very complicated one, which can indicate two or even    more emotions known as compound or mixed ones. A few studies have been    done concerning Compound Emotion (CE). As well as, many difficult tasks    to detect Compound Emotion Recognition (CER). In this paper, we    empirically assess a group of deep spatio-temporal geometric features of    micro-expressions of autistic children during a meltdown crisis. To    achieve this goal, we make a comparison of the CER performance and    diverse collections of micro-expressions features to select the features    which best differentiates autistic children CE in meltdown crisis from    normal state, and the best classifier performance. We record autistic    children videos in normal and meltdown crisis using Kinect camera in    serious circumstances. The experimental evaluation shows that the deep    spatio-temporal geometric features and Recurrent Neural Network RNN with    3 hidden layer using Information Gain Feature Selection methods provide    best performance (85.8\%).","","Compounds; Emotion recognition; Autism; Cameras; Feature extraction; Face; Computer vision; Autism; deep spatio-temporal features; meltdown crisis; facial expressions; compound emotions","",""
6,"978-1-4503-6324-2","WOS:000556121100052","Bryant, De'Aira; Howard, Ayanna","A Comparative Analysis of Emotion-Detecting Al Systems with Respect to Algorithm Performance and Dataset Diversity","",,,"","377-382",2019,"1515 BROADWAY, NEW YORK, NY 10036-9998 USA","2nd AAAI/ACM Conference on AI, Ethics, and Society (AIES), Honolulu, HI, JAN 27-28, 2019","","AIES `19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY","","","","","ASSOC COMPUTING MACHINERY","","","","AAAI; Assoc Comp Machinery; ACM SIGAI; Berkeley Existential Risk Initiat; DeepMind Eth \& Soc; Google; Natl Sci Fdn; IBM Res; Facebook; Amazon; PricewaterhouseCoopers; Future Life Inst; Partnership AI","","","In recent news, organizations have been considering the use of facial    and emotion recognition for applications involving youth such as    tackling surveillance and security in schools. However, the majority of    efforts on facial emotion recognition research have focused on adults.    Children, particularly in their early years, have been shown to express    emotions quite differently than adults. Thus, before such algorithms are    deployed in environments that impact the wellbeing and circumstance of    youth, a careful examination should be made on their accuracy with    respect to appropriateness for this target demographic. In this work, we    utilize several datasets that contain facial expressions of children    linked to their emotional state to evaluate eight different commercial    emotion classification systems. We compare the ground truth labels    provided by the respective datasets to the labels given with the highest    confidence by the classification systems and assess the results in terms    of matching score (TPR), positive predictive value, and failure to    compute rate. Overall results show that the emotion recognition systems    displayed subpar performance on the datasets of children `s expressions    compared to prior work with adult datasets and initial human ratings. We    then identify limitations associated with automated recognition of    emotions in children and provide suggestions on directions with    enhancing recognition accuracy through data diversification, dataset    accountability, and algorithmic regulation.","","","",""
6,"978-1-7281-2334-9","WOS:000568448300017","Iqbal, Tahreem; Javed, Sobia Tariq","A New Perspective towards Analysis of Human Facial Expression Using Supervised Classification Algorithms","",,,"","94-99",2019,"345 E 47TH ST, NEW YORK, NY 10017 USA","8th International Conference on Information and Communication Technologies (ICICT), Inst Business Adm, Karachi, PAKISTAN, NOV 16-17, 2019","","2019 8TH INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES (ICICT 2019)","","","International Conference on Information and Communication Technologies ICICT","Mahmood, T.; Khoja, S.; Ghani, S.","IEEE","","","","","","","As cost-effective and relatively accurate models for behavior    classification, automatic facial expression analysis has the potential    to be applied to multiple disciplines. The current research deals with    real-time classification of evoked emotions in children. Reason behind    analyzing children behavior is that they possess immature cognitive    abilities compared to adults, they are more likely to be influenced by    external stimuli and they are less likely to pose and hide expression.    This research work uses a three-phase classification model to analyze    real time captured emotion varying children facial expressions. Two    classifiers K-Nearest Neighbor and SVM were used for classification. The    classification model was tested using our own real time recorded    children Facial Expression (CFE).","","Face Detection; Feature Extraction; Eigenface; KNN; SVM; Voila Jones; Facial Expression","",""
7,"","WOS:000629356500006","Yu, Guiping","Emotion Monitoring for Preschool Children Based on Face Recognition and Emotion Recognition Algorithms","COMPLEXITY",2021,,"MAR 2","",2021,"ADAM HOUSE, 3RD FL, 1 FITZROY SQ, LONDON, WIT 5HE, ENGLAND","","","","","","","","WILEY-HINDAWI","","","","","","","In this paper, we study the face recognition and emotion recognition    algorithms to monitor the emotions of preschool children. For previous    emotion recognition focusing on faces, we propose to obtain more    comprehensive information from faces, gestures, and contexts. Using the    deep learning approach, we design a more lightweight network structure    to reduce the number of parameters and save computational resources.    There are not only innovations in applications, but also algorithmic    enhancements. And face annotation is performed on the dataset, while a    hierarchical sampling method is designed to alleviate the data imbalance    phenomenon that exists in the dataset. A new feature descriptor, called    ``oriented gradient histogram from three orthogonal planes,{''} is    proposed to characterize facial appearance variations. A new efficient    geometric feature is also proposed to capture facial contour variations,    and the role of audio methods in emotion recognition is explored.    Multifeature fusion can be used to optimally combine different features.    The experimental results show that the method is very effective compared    to other recent methods in dealing with facial expression recognition    problems about videos in both laboratory-controlled environments and    outdoor environments. The method performed experiments on expression    detection in a facial expression database. The experimental results are    compared with data from previous studies and demonstrate the    effectiveness of the proposed new method.","","","",""
7,"","WOS:000647216000001","Dapogny, Arnaud; Grossard, Charline; Hun, Stephanie; Serret, Sylvie; Grynszpan, Ouriel; Dubuisson, Severine; Cohen, David; Bailly, Kevin","On Automatically Assessing Children's Facial Expressions Quality: A Study, Database, and Protocol","FRONTIERS IN COMPUTER SCIENCE",1,,"OCT 11","",2019,"AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND","","","","","","","","FRONTIERS MEDIA SA","","","","","","","While there exists a number of serious games geared toward helping    children with ASD to produce facial expressions, most of them fail to    provide a precise feedback to help children to adequately learn. In the    scope of the JEMImE project, which aims at developing such serious game    platform, we introduce throughout this paper a machine learning approach    for discriminating between facial expressions and assessing the quality    of the emotional display. In particular, we point out the limits in    generalization capacities of models trained on adult subjects. To    circumvent this issue in the design of our system, we gather a large    database depicting children's facial expressions to train and validate    the models. We describe our protocol to elicit facial expressions and    obtain quality annotations, and empirically show that our models obtain    high accuracies in both classification and quality assessment of    children's facial expressions. Furthermore, we provide some insight on    what the models learn and which features are the most useful to    discriminate between the various facial expressions classes and    qualities. This new model trained on the dedicated dataset has been    integrated into a proof of concept of the serious game.","Criação de dataset próprio - Dataset
Proposta de criação de datasets","facial expression recognition; expression quality; random forests; emotion; children; dataset","",""
6,"978-1-7281-3885-5","WOS:000652198600074","Ul Haque, Md Inzamam; Valles, Damian","Facial Expression Recognition Using DCNN and Development of an iOS App for Children with ASD to Enhance Communication Abilities","",,,"","476-482",2019,"345 E 47TH ST, NEW YORK, NY 10017 USA","IEEE 10th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference (UEMCON), Columbia Univ, New York, NY, OCT 10-12, 2019","","2019 IEEE 10TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS \& MOBILE COMMUNICATION CONFERENCE (UEMCON)","","","","Chakrabarti, S.; Saha, H. N.","IEEE","","","","IEEE; IEEE New York Sect; IEEE R1 Reg; IEEE USA; Inst Engn \& Management; Univ Engn \& Management","","","In this paper, continued work of a research project is discussed which    achieved the end goal of the project - to build a mobile device    application that can teach children with Autism Spectrum Disorder (ASD)    to recognize human facial expressions utilizing computer vision and    image processing. Universally, there are seven facial expressions    categories: angry, disgust, happy, sad, fear, surprise, and neutral. To    recognize all these facial expressions and to predict the current mood    of a person is a difficult task for a child. A child with ASD, this    problem presents itself in a more sophisticated manner due to the nature    of the disorder. The main goal of this research was to develop a deep    Convolutional Neural Network (DCNN) for facial expression recognition,    which can help young children with ASD to recognize facial expressions,    using mobile devices. The Kaggle's FER2013 and Karolinska Directed    Emotional Faces (KDEF) dataset have been used to train and test with the    DCNN model, which can classify facial expressions from different    viewpoints and in different lighting contrasts. An 86.44\% accuracy was    achieved with good generalizability for the DCNN model. The results show    an improvement of the DCNN accuracy in dealing with lighting contrast    changes, and the implementation of image processing before performing    the facial expression classification. As a byproduct of this research    project, an app suitable for the iOS platform was developed for running    both the DCNN model and image processing algorithm. The app can be used    by speechlanguage pathologies, teacher, care-takers, and parents as a    technological tool when working with children with ASD.","","Facial Expression Recognition; Autism; Deep Learning; Convolutional Neural Network; iOS App","",""
7,"","WOS:000697827200029","Jaison, Asha; Deepa, C.","A Review on Facial Emotion Recognition and Classification Analysis with Deep Learning","BIOSCIENCE BIOTECHNOLOGY RESEARCH COMMUNICATIONS",14,5, SI,"","154-161",2021,"C-52 HOUSING BOARD COLONY, KOHE FIZA, BHOPAL, MADHYA PRADESH 462 001, INDIA","","","","","","","","SOC SCIENCE \& NATURE","","","","","","","Automatic face expression recognition is an exigent research subject and    a challenge in computer vision. It is an interdisciplinary domain    standing at the crossing of behavioural science, psychology, neurology,    and artificial intelligence. Human-robot interaction is getting more    significant with the automation of every field, like treating autistic    patients, child therapy, babysitting, etc. In all the cases robots need    to understand the present state of mind for better decision making. It    is difficult for machine learning techniques to recognize the    expressions of people since there will be significant changes in the way    of their expressions. The emotions expressed through the human face have    its importance in making arguments and decisions on different subjects.    Machine Learning with Computer Vision and Deep Learning can be used to    recognize facial expressions from the preloaded or real time images with    human faces. DNN (Deep Neural Networking) is one among the hottest areas    of research and is found to be very effective in classification of    images with a high degree of accuracy. In the proposed work, the popular    dataset CK+ is analysed for comparison. The dataset FER 2013 and    home-brewed data sets are used in the work for calculating the accuracy    of the model created. The results are obtained in such a way that DCNN    approach is very efficient in facial emotion recognition. Experiments    and study show that the dataset, FER 2013 is a high-quality dataset with    equal efficiency as the other two popular datasets. This paper aims to    ameliorate the accuracy of classification of facial emotion.","","FACIAL EMOTION RECOGNITION (FER); DEEP CONVOLUTIONAL NEURAL NET WORK (DCNN); OPEN CV (OPEN-SOURCE COMPUTER VISION LIBRARY); VGG 16","",""
7,"","WOS:000700975300001","Washington, Peter; Kalantarian, Haik; Kent, Jack; Husic, Arman; Kline, Aaron; Leblanc, Emilie; Hou, Cathy; Mutlu, Cezmi; Dunlap, Kaitlyn; Penev, Yordan; Stockham, Nate; Chrisman, Brianna; Paskov, Kelley; Jung, Jae-Yoon; Voss, Catalin; Haber, Nick; Wall, Dennis P.","Training Affective Computer Vision Models by Crowdsourcing Soft-Target Labels","COGNITIVE COMPUTATION",13,5,"SEP","1363-1373",2021,"ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES","","","","","","","","SPRINGER","","","","","","","Emotion detection classifiers traditionally predict discrete emotions.    However, emotion expressions are often subjective, thus requiring a    method to handle compound and ambiguous labels. We explore the    feasibility of using crowdsourcing to acquire reliable soft-target    labels and evaluate an emotion detection classifier trained with these    labels. We hypothesize that training with labels that are representative    of the diversity of human interpretation of an image will result in    predictions that are similarly representative on a disjoint test set. We    also hypothesize that crowdsourcing can generate distributions which    mirror those generated in a lab setting. We center our study on the    Child Affective Facial Expression (CAFE) dataset, a gold standard    collection of images depicting pediatric facial expressions along with    100 human labels per image. To test the feasibility of crowdsourcing to    generate these labels, we used Microworkers to acquire labels for 207    CAFE images. We evaluate both unfiltered workers and workers selected    through a short crowd filtration process. We then train two versions of    a ResNet-152 neural network on soft-target CAFE labels using the    original 100 annotations provided with the dataset: (1) a classifier    trained with traditional one-hot encoded labels and (2) a classifier    trained with vector labels representing the distribution of CAFE    annotator responses. We compare the resulting softmax output    distributions of the two classifiers with a 2-sample independent t-test    of L1 distances between the classifier's output probability distribution    and the distribution of human labels. While agreement with CAFE is weak    for unfiltered crowd workers, the filtered crowd agree with the CAFE    labels 100\% of the time for happy, neutral, sad, and ``fear +    surprise{''} and 88.8\% for ``anger + disgust.{''} While the F1-score    for a one-hot encoded classifier is much higher (94.33\% vs. 78.68\%)    with respect to the ground truth CAFE labels, the output probability    vector of the crowd-trained classifier more closely resembles the    distribution of human labels (t = 3.2827, p = 0.0014). For many    applications of affective computing, reporting an emotion probability    distribution that accounts for the subjectivity of human interpretation    can be more useful than an absolute label. Crowdsourcing, including a    sufficient filtering mechanism for selecting reliable crowd workers, is    a feasible solution for acquiring soft-target labels.","","","",""
7,"","WOS:000711695600001","Singh, Tripti; Mohadikar, Mohan; Gite, Shilpa; Patil, Shruti; Pradhan, Biswajeet; Alamri, Abdullah","Attention Span Prediction Using Head-Pose Estimation With Deep Neural Networks","IEEE ACCESS",9,,"","142632-142643",2021,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","Automated human pose estimation is evolving as an exciting research area    in human activity detection. It includes sophisticated applications such    as malpractice detection in the examination, distracted driving, gesture    detection, etc., and requires robust and reliable pose estimation    techniques. These applications help to map the attention of the user    with head pose estimation (HPE) metrics supported by emotion and gaze    analysis. This paper solves the problem of attention score estimation    with HPE. The proposed method ensures ease of implementation while    addressing head pose estimation using 68 facial features. Further, to    attain reliability and precision, head pose estimation has been    implemented as a regression task. The coordinate pair angle method    (CPAM) with deep neural network (DNN) regression and elastic net    regression is carried out. The use of DNN ensures precision on low    lighting, distorted or occluded images. CPAM methodology leverages    facial landmark detection and angular difference to estimate head pose.    Experimentation results showed that the proposed model could handle    large datasets, real-time data processing, significant pose variations,    partial occlusions, and diverse facial expressions with a mean absolute    error (MAE) of 3 degrees and less. The proposed system was evaluated on    three standard databases: the 300W across large poses (300W-LP) dataset,    annotated facial landmarks in the wild (AFLW2000) dataset, and the    national institute of mental health child emotional faces picture set    (NIMH-ChEFS) dataset. The results achieved are on par with recent    state-of-the-art methodologies such as anisotropic angle distribution    learning (AADL), joint head pose estimation and face alignment algorithm    (JFA), rotation axis focused attention network (RAFA-Net), and propose    an MAE ranging up to 6 degrees. The paper could achieve remarkable    results for attention span prediction using head pose estimation and for    many possible future applications.","","Head; Pose estimation; Faces; Three-dimensional displays; Magnetic heads; Convolutional neural networks; Training; Head pose estimation; CPAM; convolutional neural network; regression; attention span prediction","",""
7,"","WOS:000711808500036","Zhang, Tao; Liu, Minjie; Yuan, Tian; Al-Nabhan, Najla","Emotion-Aware and Intelligent Internet of Medical Things Toward Emotion Recognition During COVID-19 Pandemic","IEEE INTERNET OF THINGS JOURNAL",8,21,"NOV 1","16002-16013",2021,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","The Internet of Medical Things (IoMT) is a brand new technology of    combining medical devices and other wireless devices to access to the    healthcare management systems. This article has sought the possibilities    of aiding the current Corona Virus Disease 2019 (COVID-19) pandemic by    implementing machine learning algorithms while offering emotional    treatment suggestion to the doctors and patients. The cognitive model    with respect to IoMT is best suited to this pandemic as every person is    to be connected and monitored through a cognitive network. However, this    COVID-19 pandemic still remain some challenges about emotional    solicitude for infants and young children, elderly, and mentally ill    persons during pandemic. Confronting these challenges, this article    proposes an emotion-aware and intelligent IoMT system, which contains    information sharing, information supervision, patients tracking, data    gathering and analysis, healthcare, etc. Intelligent IoMT devices are    connected to collect multimodal data of patients in a surveillance    environments. The latest data and inputs from official websites and    reports are tested for further investigation and analysis of the emotion    analysis. The proposed novel IoMT platform enables remote health    monitoring and decision-making about the emotion, therefore greatly    contribute convenient and continuous emotion-aware healthcare services    during COVID-19 pandemic. Experimental results on some emotion data    indicate that the proposed framework achieves significant advantage when    compared with the some mainstream models. The proposed cognition-based    dynamic technology is an effective solution way for accommodating a big    number of devices and this COVID-19 pandemic application. The    controversy and future development trend are also discussed.","","Emotion recognition; COVID-19; Medical services; Internet of Things; Pandemics; Wireless communication; Convolution; Cognitive model; Corona Virus Disease 2019 (COVID-19); emotion-aware; healthcare management systems; internet of medical things (IoMT)","",""
7,"","WOS:000717219500001","Pereira, Monica; Meng, Hongying; Hone, Kate","Prediction of Communication Effectiveness During Media Skills Training Using Commercial Automatic Non-verbal Recognition Systems","FRONTIERS IN PSYCHOLOGY",12,,"SEP 29","",2021,"AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND","","","","","","","","FRONTIERS MEDIA SA","","","","","","","It is well recognised that social signals play an important role in    communication effectiveness. Observation of videos to understand    non-verbal behaviour is time-consuming and limits the potential to    incorporate detailed and accurate feedback of this behaviour in    practical applications such as communication skills training or    performance evaluation. The aim of the current research is twofold: (1)    to investigate whether off-the-shelf emotion recognition technology can    detect social signals in media interviews and (2) to identify which    combinations of social signals are most promising for evaluating    trainees' performance in a media interview. To investigate this,    non-verbal signals were automatically recognised from practice on-camera    media interviews conducted within a media training setting with a sample    size of 34. Automated non-verbal signal detection consists of multimodal    features including facial expression, hand gestures, vocal behaviour and    `honest' signals. The on-camera interviews were categorised into    effective and poor communication exemplars based on communication skills    ratings provided by trainers and neutral observers which served as a    ground truth. A correlation-based feature selection method was used to    select signals associated with performance. To assess the accuracy of    the selected features, a number of machine learning classification    techniques were used. Naive Bayes analysis produced the best results    with an F-measure of 0.76 and prediction accuracy of 78\%. Results    revealed that a combination of body movements, hand movements and facial    expression are relevant for establishing communication effectiveness in    the context of media interviews. The results of the current study have    implications for the automatic evaluation of media interviews with a    number of potential application areas including enhancing communication    training including current media skills training.","","social signals detection; commercial technologies; communication skills; training; non-verbal signals; media interviews; multimodal fusion","",""
7,"","WOS:000756540200001","Hong, Jiuk; Lee, Chaehyeon; Jung, Heechul","Late Fusion-Based Video Transformer for Facial Micro-Expression Recognition","APPLIED SCIENCES-BASEL",12,3,"FEB","",2022,"ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","","","","","","","","MDPI","","","","","","","In this article, we propose a novel model for facial micro-expression    (FME) recognition. The proposed model basically comprises a transformer,    which is recently used for computer vision and has never been used for    FME recognition. A transformer requires a huge amount of data compared    to a convolution neural network. Then, we use motion features, such as    optical flow and late fusion to complement the lack of FME dataset. The    proposed method was verified and evaluated using the SMIC and CASME II    datasets. Our approach achieved state-of-the-art (SOTA) performance of    0.7447 and 73.17\% in SMIC in terms of unweighted F1 score (UF1) and    accuracy (Acc.), respectively, which are 0.31 and 1.8\% higher than    previous SOTA. Furthermore, UF1 of 0.7106 and Acc. of 70.68\% were shown    in the CASME II experiment, which are comparable with SOTA.","SMIC and CASME II datasets","deep learning; image processing; facial micro-expression; emotion recognition; vision transformer","",""
6,"978-1-7281-1179-7","WOS:000760910503083","Arabian, H.; Wagner-Hartl, V.; Chase, J. Geoffrey; Moeller, K.","Facial Emotion Recognition Focused on Descriptive Region Segmentation","",,,"","3415-3418",2021,"345 E 47TH ST, NEW YORK, NY 10017 USA","43rd Annual International Conference of the IEEE-Engineering-in-Medicine-and-Biology-Society (IEEE EMBC), ELECTR NETWORK, NOV 01-05, 2021","","2021 43RD ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE \& BIOLOGY SOCIETY (EMBC)","","","IEEE Engineering in Medicine and Biology Society Conference Proceedings","","IEEE","","","","IEEE Engn Med \& Biol Soc; IEEE; Elsevier; Inst Engn \& Technol","","","Facial emotion recognition (FER) is useful in many different    applications and could offer significant benefit as part of feedback    systems to train children with Autism Spectrum Disorder (ASD) who    struggle to recognize facial expressions and emotions. This project    explores the potential of real time FER based on the use of local    regions of interest combined with a machine learning approach. Histogram    of Oriented Gradients (HOG) was implemented for feature extraction,    along with 3 different classifiers, 2 based on k-Nearest Neighbor and 1    using Support Vector Machine (SVM) classification. Model performance was    compared using accuracy of randomly selected validation sets after    training on random training sets of the Oulu-CASIA database. Image    classes were distributed evenly, and accuracies of up to 98.44\% were    observed with small variation depending on data distributions. The    region selection methodology provided a compromise between accuracy and    number of extracted features, and validated the hypothesis a focus on    smaller informative regions performs just as well as the entire image.","","Autism Spectrum Disorder (ASD); Facial Emotion Recognition (FER); Feature Extraction (FE); Machine Learning; Oulu-CASIA","",""
6,"978-3-030-96040-7; 978-3-030-96039-1","WOS:000772182600055","Kumar, Akhilesh; Kumar, Awadhesh","Analysis of Machine Learning Algorithms for Facial Expression Recognition","",1534,,"","730-750",2022,"GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND","1st International Conference on Advanced Network Technologies and Intelligent Computing (ANTIC), ELECTR NETWORK, DEC 17-18, 2021","","ADVANCED NETWORK TECHNOLOGIES AND INTELLIGENT COMPUTING, ANTIC 2021","","","Communications in Computer and Information Science","Woungang, I.; Dhurandher, S. K.; Pattanaik, K. K.; Verma, A.; Verma, P.","SPRINGER INTERNATIONAL PUBLISHING AG","","","","Banaras Hindu Univ, Inst Sci, Dept Comp Sci","","","People can identify emotion from facial expressions easily, but it is    more difficult to do it with a computer. It is now feasible to identify    feelings from images because of recent advances in computational    intelligence. Emotional responses are those mental states of thoughts    that develop without conscious effort and are naturally associated with    facial muscles, resulting in different facial expressions such as happy,    sad, angry, contempt, fear, surprise etc. Emotions play an important    role in nonverbal cues that represent a person's interior thoughts.    Intimate robots are expanding in every domain, whether it is completing    requirements of elderly people, addressing psychiatric patients, child    rehabilitation, or even childcare, as the human-robot interface is    grabbing on every day with the increased demand for automation in every    industry. We evaluate and test machine learning algorithms on the FER    2013 data set to recognize human emotion from facial expressions, with    some of them achieving the highest accuracy and the others failing to    detect emotions. Many researchers have used various machine learning    methods to identify human emotions during the last few years. In this    research article, we analyze eight frequently used machine learning    techniques on the FER 2013 dataset to determine which method performs    best at categorizing human facial expression. After analyzing the    results, it is found that the accuracy of some of the algorithms is    quite satisfactory, with 37\% for Logistic Regression, 33\% for    K-neighbors classifier, 100\% for Decision Tree Classifier, 78\% for    Random Forests, 57\% for Ada-Boost, 100\% for Gaussian NB, 33\% for LDA    (Linear Discriminant Analysis), and 99\% for QDA (Quadratic Discriminant    Analysis). Furthermore, the experimental results show that the Decision    Tree and Gaussian NB Classifier can correctly identify all of the    emotions in the FER 2013 dataset with 100\% classification accuracy,    while Quadratic Discriminant Analysis can do so with 99\% accuracy.","","Facial expression; Emotion; Machine learning; KNN; QDA; Decision tree; Gaussian NB; Random Forest","",""
6,"978-1-6654-2443-1","WOS:000783834000109","Pulido-Castro, Sergio; Palacios-Quecan, Nubia; Ballen-Cardenas, Michelle P.; Cancino-Suarez, Sandra; Rizo-Arevalo, Alejandra; Lopez Lopez, Juan M.","Ensemble of Machine Learning Models for an Improved Facial Emotion Recognition","",,,"","512-516",2021,"345 E 47TH ST, NEW YORK, NY 10017 USA","IEEE URUCON Conference (IEEE URUCON), Montevideo, URUGUAY, NOV 24-26, 2021","","2021 IEEE URUCON","","","","","IEEE","","","","IEEE; IEEE Consejo Cono Sur; IEEE Power \& Energy Soc; IEEE Comp Soc; IEEE Commun Soc; IEEE Broadcast Technol Soc; IEEE Instrumentat \& Measurement Soc; IEEE Technol \& Engn Management Soc; IEEE Engn Med \& Biol Soc; IEEE Circuits \& Syst Soc; IEEE Control Syst Soc; IEEE Educ Soc; IEEE Signal Proc Soc; IEEE Solid State Circuits Soc","","","The creation of algorithms that predict emotional recognition is a    subject that has been of particular interest by researchers around the    world for the last few years, as many computer vision-based systems make    use of this information to get an approximation of the emotional state    of an individual. This study aims to develop a real-time emotional    recognition algorithm based on the facial expression. Our main    contributions are the following: This algorithm was tested in a    computational tool designed to stimulate the imitation and recognition    of emotions of children with Autism Spectrum Disorder based on their    facial expressions. By designing an ensemble of machine learning models    which separates emotions into different sets, we are able to improve the    recognition accuracy. Additionally, the selection of relevant features    greatly reduces the execution time of the algorithm, making it feasible    for real-time recognition. Testing of different label combinations is    yet to be performed in order to further improve the recognition    accuracy.","KDEF dataset","Computer vision; Feature relevance; Emotion recognition; Facial detection; Machine learning","",""
7,"","WOS:000848611200001","Hu, Bo","Analysis of Art Therapy for Children with Autism by Using the Implemented Artificial Intelligence System","INTERNATIONAL JOURNAL OF HUMANOID ROBOTICS",19,03,"JUN","",2022,"5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE","","","","","","","","WORLD SCIENTIFIC PUBL CO PTE LTD","","","","","","","Autism is a disease that manifests as social communication disorders and    repetitive sensory movements. The current incidence of autism continues    to rise globally, and the number of children with autism is also    increasing. The treatment of autistic children in China is mainly    intervention, but experienced autism instructors are lacking. Here,    machine learning and artificial intelligence (AI) algorithms are adopted    as the technical foundation to build a system that measures the    emotional performance of autistic students in the classroom based on    actual application scenarios. By analyzing the classroom learning data    of autistic children, the students' emotions can be effectively judged    to help teachers evaluate and track their classroom performance and    reduce teachers' burden. Results demonstrate that when the matrix    composed of frame sequences and key point coordinates is used as the    input, the spatio-temporal graph convolutional network is determined as    the principal model of action recognition, with an accuracy of 90\%, and    the participation score can be obtained by calculating the action    response time. In the experimental process of facial expression    recognition, the random forest's classification accuracy of the feature    point sequence based on images can reach 99\%. Therefore, the random    forest is determined as the principal classifier for facial expression    recognition. After the relationship between expression intensity,    pleasure, and expression category is analyzed, the scoring method is    designed. The experiment also discovers that painting can be a    rehabilitation therapy for children with autism. The above results can    provide a theoretical foundation for the treatment of autistic children.","","Children; autism; artificial intelligence; art therapy","",""
7,"","WOS:000466223600017","Zhang, Zixing; Han, Jing; Coutinho, Eduardo; Schuller, Bjorn","Dynamic Difficulty Awareness Training for Continuous Emotion Prediction","IEEE TRANSACTIONS ON MULTIMEDIA",21,5,"MAY","1289-1301",2019,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","Time-continuous emotion prediction has become an increasingly compelling    task in machine learning. Considerable efforts have been made to advance    the performance of these systems. Nonetheless, the main focus has been    the development of more sophisticated models and the incorporation of    different expressive modalities (e.g., speech, face, and physiology). In    this paper, motivated by the benefit of difficulty awareness in a human    learning procedure, we propose a novel machine learning framework,    namely, dynamic difficulty awareness training (DDAT), which sheds fresh    light on the research-directly exploiting the difficulties in learning    to boost the machine learning process. The DDAT framework consists of    two stages: information retrieval and information exploitation. In the    first stage, we make use of the reconstruction error of input features    or the annotation uncertainty to estimate the difficulty of learning    specific information. The obtained difficulty level is then used in    tandem with original features to update the model input in a second    learning stage with the expectation that the model can learn to focus on    high difficulty regions of the learning process. We perform extensive    experiments on a benchmark database REmote COLlaborative and affective    to evaluate the effectiveness of the proposed framework. The    experimental results show that our approach outperforms related    baselines as well as other well-established time-continuous emotion    prediction systems, which suggests that dynamically integrating the    difficulty information for neural networks can help enhance the learning    process.","","Emotion prediction; difficulty awareness learning; dynamic learning","",""
7,"","WOS:000475910800001","Barrett, Lisa Feldman; Adolphs, Ralph; Marsella, Stacy; Martinez, Aleix M.; Pollak, Seth D.","Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements","PSYCHOLOGICAL SCIENCE IN THE PUBLIC INTEREST",20,1,"JUL","1-68",2019,"1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND","","","","","","","","SAGE PUBLICATIONS LTD","","","","","","","It is commonly assumed that a person's emotional state can be readily    inferred from his or her facial movements, typically called emotional    expressions or facial expressions. This assumption influences legal    judgments, policy decisions, national security protocols, and    educational practices; guides the diagnosis and treatment of psychiatric    illness, as well as the development of commercial applications; and    pervades everyday social interactions as well as research in other    scientific fields such as artificial intelligence, neuroscience, and    computer vision. In this article, we survey examples of this widespread    assumption, which we refer to as the common view, and we then examine    the scientific evidence that tests this view, focusing on the six most    popular emotion categories used by consumers of emotion research: anger,    disgust, fear, happiness, sadness, and surprise. The available    scientific evidence suggests that people do sometimes smile when happy,    frown when sad, scowl when angry, and so on, as proposed by the common    view, more than what would be expected by chance. Yet how people    communicate anger, disgust, fear, happiness, sadness, and surprise    varies substantially across cultures, situations, and even across people    within a single situation. Furthermore, similar configurations of facial    movements variably express instances of more than one emotion category.    In fact, a given configuration of facial movements, such as a scowl,    often communicates something other than an emotional state. Scientists    agree that facial movements convey a range of information and are    important for social communication, emotional or otherwise. But our    review suggests an urgent need for research that examines how people    actually move their faces to express emotions and other social    information in the variety of contexts that make up everyday life, as    well as careful study of the mechanisms by which people perceive    instances of emotion in one another. We make specific research    recommendations that will yield a more valid picture of how people move    their faces to express emotions and how they infer emotional meaning    from facial movements in situations of everyday life. This research is    crucial to provide consumers of emotion research with the translational    information they require.","","emotion perception; emotional expression; emotion recognition","",""
7,"","WOS:000807364500004","Li, Xiaohong","Expression Recognition of Classroom Children's Game Video Based on Improved Convolutional Neural Network","SCIENTIFIC PROGRAMMING",2022,,"APR 8","",2022,"ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND","","","","","","","","HINDAWI LTD","","","","","","","Humans express emotions in many ways, such as gestures, limbs, and    expressions. Among them, facial expressions are the most intuitive way    to express human inner emotional activities in human-to-human    communication. With the rapid development of computer vision, facial    expression recognition is an important research topic in the field of    computer vision. It plays a key role in nonverbal communication and can    be applied to human-computer interaction, social robotics, video games,    and other fields. Traditional expression recognition algorithms require    complex manual feature extraction, which takes a long time, and the    accuracy of expression recognition in complex scenes is not high.    However, with the development of deep learning, especially the    convolutional neural network, facial expression recognition technology    has also developed rapidly, and the recognition accuracy has been    greatly improved. This paper studies the facial expression recognition    method of classroom children's game video based on convolutional neural    network and proposes a convolutional neural network with deeper layers.    The full connection is modified to 4 layers of convolution, 4 layers of    pooling, and 2 layers of full connection. Firstly, the facial expression    image is preprocessed by, for example, key point location, face    cropping, and image normalization; then, the convolutional layer is used    to extract the low-dimensional and high-dimensional feature information    of the face image; and the pooling layer is used to extract the face    image. The feature information is dimensionally reduced. Finally, the    softmax classifier is used to classify and recognize the expressions of    the training sample images. In order to improve the accuracy of    expression recognition, a self-made set of labeled pictures was added to    the expression training set. Simulation and comparison experiments show    that the improved model has higher accuracy and smoother loss curve,    which verifies the effectiveness of the improved network.","","","",""
7,"","WOS:000424804400016","Gibbard, Clare R.; Ren, Juejing; Skuse, David H.; Clayden, Jonathan D.; Clark, Chris A.","Structural connectivity of the amygdala in young adults with autism spectrum disorder","HUMAN BRAIN MAPPING",39,3,"MAR","1270-1282",2018,"111 RIVER ST, HOBOKEN 07030-5774, NJ USA","","","","","","","","WILEY","","","","","","","Autism spectrum disorder (ASD) is characterized by impairments in social    cognition, a function associated with the amygdala. Subdivisions of the    amygdala have been identified which show specificity of structure,    connectivity, and function. Little is known about amygdala connectivity    in ASD. The aim of this study was to investigate the microstructural    properties of amygdalacortical connections and their association with    ASD behaviours, and whether connectivity of specific amygdala subregions    is associated with particular ASD traits. The brains of 51    high-functioning young adults (25 with ASD; 26 controls) were scanned    using MRI. Amygdala volume was measured, and amygdalacortical    connectivity estimated using probabilistic tractography. An iterative    winner takes all' algorithm was used to parcellate the amygdala based on    its primary cortical connections. Measures of amygdala connectivity were    correlated with clinical scores. In comparison with controls, amygdala    volume was greater in ASD (F(1,94) = 4.19; p = .04). In white matter    (WM) tracts connecting the right amygdala to the right cortex, ASD    subjects showed increased mean diffusivity (t = 2.35; p = .05), which    correlated with the severity of emotion recognition deficits (rho =    -0.53; p = .01). Following amygdala parcellation, in ASD subjects    reduced fractional anisotropy in WM connecting the left amygdala to the    temporal cortex was associated with with greater attention switching    impairment (rho = -0.61; p = .02). This study demonstrates that both    amygdala volume and the microstructure of connections between the    amygdala and the cortex are altered in ASD. Findings indicate that the    microstructure of right amygdala WM tracts are associated with overall    ASD severity, but that investigation of amygdala subregions can identify    more specific associations.","","amygdaloid nuclear complex; autism spectrum disorders; diffusion tensor imaging; diffusion tractography","",""
7,"","WOS:000428690000006","Daniels, Jena; Haber, Nick; Voss, Catalin; Schwartz, Jessey; Tamura, Serena; Fazel, Azar; Kline, Aaron; Washington, Peter; Phillips, Jennifer; Winograd, Terry; Feinstein, Carl; Wall, Dennis P.","Feasibility Testing of a Wearable Behavioral Aid for Social Learning in Children with Autism","APPLIED CLINICAL INFORMATICS",9,1,"JAN","129-140",2018,"RUDIGERSTR 14, D-70469 STUTTGART, GERMANY","","","","","","","","GEORG THIEME VERLAG KG","","","","","","","Background Recent advances in computer vision and wearable technology    have created an opportunity to introduce mobile therapy systems for    autism spectrum disorders (ASD) that can respond to the increasing    demand for therapeutic interventions; however, feasibility questions    must be answered first.    Objective We studied the feasibility of a prototype therapeutic tool for    children with ASD using Google Glass, examining whether children with    ASD would wear such a device, if providing the emotion classification    will improve emotion recognition, and how emotion recognition differs    between ASD participants and neurotypical controls (NC).    Methods We ran a controlled laboratory experiment with 43 children: 23    with ASD and 20 NC. Children identified static facial images on a    computer screen with one of 7 emotions in 3 successive batches: the    first with no information about emotion provided to the child, the    second with the correct classification from the Glass labeling the    emotion, and the third again without emotion information. We then    trained a logistic regression classifier on the emotion confusion    matrices generated by the two information-free batches to predict ASD    versus NC.    Results All 43 children were comfortable wearing the Glass. ASD and NC    participants who completed the computer task with Glass providing    audible emotion labeling (n = 33) showed increased accuracies in emotion    labeling, and the logistic regression classifier achieved an accuracy of    72.7\%. Further analysis suggests that the ability to recognize    surprise, fear, and neutrality may distinguish ASD cases from NC.    Conclusion This feasibility study supports the utility of a wearable    device for social affective learning in ASD children and demonstrates    subtle differences in how ASD and NC children perform on an emotion    recognition task.","","autism spectrum disorder; pilot projects; wearable device and body area network; artificial intelligence; emotion recognition; pediatrics; people with disabilities or special needs","",""
7,"","WOS:000431374500001","Movahedi, Faezeh; Coyle, James L.; Sejdic, Ervin","Deep Belief Networks for Electroencephalography: A Review of Recent Contributions and Future Outlooks","IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS",22,3,"MAY","642-652",2018,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","Deep learning, a relatively new branch of machine learning, has been    investigated for use in a variety of biomedical applications. Deep    learning algorithms have been used to analyze different physiological    signals and gain a better understanding of human physiology for    automated diagnosis of abnormal conditions. In this paper, we provide an    overview of deep learning approaches with a focus on deep belief    networks in electroencephalography applications. We investigate the    state-of-the-art algorithms for deep belief networks and then cover the    application of these algorithms and their performances in    electroencephalographic applications. We covered various applications of    electroencephalography in medicine, including emotion recognition, sleep    stage classification, and seizure detection, in order to understand how    deep learning algorithms could be modified to better suit the tasks    desired. This review is intended to provide researchers with a broad    overview of the currently existing deep belief network methodology for    electroencephalography signals, as well as to highlight potential    challenges for future research.","","Classification; deep learning; electroencephalography; machine learning","",""
7,"","WOS:000439557000051","Bi, Kun; Chattun, Mohammad Ridwan; Liu, Xiaoxue; Wang, Qiang; Tian, Shui; Zhang, Siqi; Lu, Qing; Yao, Zhijian","Abnormal early dynamic individual patterns of functional networks in low gamma band for depression recognition","JOURNAL OF AFFECTIVE DISORDERS",238,,"OCT 1","366-374",2018,"RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","","","","","","","","ELSEVIER","","","","","","","Background: The functional networks are associated with emotional    processing in depression. The mapping of dynamic spatio-temporal brain    networks is used to explore individual performance during early negative    emotional processing. However, the dysfunctions of functional networks    in low gamma band and their discriminative potentialities during early    period of emotional face processing remain to be explored.    Methods: Functional brain networks were constructed from the MEG    recordings of 54 depressed patients and 54 controls in low gamma band    (30-48 Hz). Dynamic connectivity regression (DCR) algorithm analyzed the    individual change points of time series in response to emotional stimuli    and constructed individualized spatiotemporal patterns. The nodal    characteristics of patterns were calculated and fed into support vector    machine (SVM). Performance of the classification algorithm in low gamma    band was validated by dynamic topological characteristics of individual    patterns in comparison to alpha and beta band.    Results: The best discrimination accuracy of individual spatio-temporal    patterns was 91.01\% in low gamma band. Individual temporal patterns had    better results compared to group-averaged temporal patterns in all    bands. The most important discriminative networks included affective    network (AN) and fronto-parietal network (FPN) in low gamma band.    Limitations: The sample size is relatively small. High gamma band was    not considered.    Conclusions: The abnormal dynamic functional networks in low gamma band    during early emotion processing enabled depression recognition. The    individual information processing is crucial in the discovery of    abnormal spatio-temporal patterns in depression during early negative    emotional processing. Individual spatio-temporal patterns may reflect    the real dynamic function of subjects while group-averaged data may    neglect some individual information.","","Dynamic connectivity regression; MEG; Depression; Individual dynamic patterns; Gamma band","",""
7,"","WOS:000442778600006","Ye, Liang; Wang, Peng; Wang, Le; Ferdinando, Hany; Seppanen, Tapio; Alasaarela, Esko","A Combined Motion-Audio School Bullying Detection Algorithm","INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE",32,12,"DEC","",2018,"5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE","","","","","","","","WORLD SCIENTIFIC PUBL CO PTE LTD","","","","","","","School bullying is a common social problem, which affects children both    mentally and physically, making the prevention of bullying a timeless    topic all over the world. This paper proposes a method for detecting    bullying in school based on activity recognition and speech emotion    recognition. In this method, motion and voice data are gathered by    movement sensors and a microphone, followed by extraction of a set of    motion and audio features to distinguish bullying incidents from daily    life events. Among extracted motion features are both time-domain and    frequency-domain features, while audio features are computed with    classical MFCCs. Feature selection is implemented using the wrapper    approach. At the next stage, these motion and audio features are merged    to form combined feature vectors for classification, and LDA is used for    further dimension reduction. A BPNN is trained to recognize bullying    activities and distinguish them from normal daily life activities. The    authors also propose an action transition detection method to reduce    computational complexity for practical use. Thus, the bullying detection    algorithm will only run, when an action transition event has been    detected. Simulation results show that the combined motion-audio feature    vector outperforms separate motion features and acoustic features,    achieving an accuracy of 82.4\% and a precision of 92.2\%. Moreover,    with the action transition method, the computation cost can be reduced    by half.","","Activity recognition; speech emotion recognition; movement sensors; school bullying; pattern recognition","",""
7,"","WOS:000444193800001","Daniels, Jena; Schwartz, Jessey N.; Voss, Catalin; Haber, Nick; Fazel, Azar; Kline, Aaron; Washington, Peter; Feinstein, Carl; Winograd, Terry; Wall, Dennis P.","Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism","NPJ DIGITAL MEDICINE",1,,"AUG 2","",2018,"HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY","","","","","","","","NATURE RESEARCH","","","","","","","Although standard behavioral interventions for autism spectrum disorder    (ASD) are effective therapies for social deficits, they face criticism    for being time-intensive and overdependent on specialists. Earlier    starting age of therapy is a strong predictor of later success, but    waitlists for therapies can be 18 months long. To address these    complications, we developed Superpower Glass, a    machine-learning-assisted software system that runs on Google Glass and    an Android smartphone, designed for use during social interactions. This    pilot exploratory study examines our prototype tool's potential for    social-affective learning for children with autism. We sent our tool    home with 14 families and assessed changes from intake to conclusion    through the Social Responsiveness Scale (SRS-2), a facial affect    recognition task (EGG), and qualitative parent reports. A    repeated-measures one-way ANOVA demonstrated a decrease in SRS-2 total    scores by an average 7.14 points (F(1,13) = 33.20, p = <. 001, higher    scores indicate higher ASD severity). EGG scores also increased by an    average 9.55 correct responses (F(1,10) = 11.89, p = <. 01). Parents    reported increased eye contact and greater social acuity. This    feasibility study supports using mobile technologies for potential    therapeutic purposes.","","","",""
7,"","WOS:000445816800001","Mencattini, Arianna; Mosciano, Francesco; Comes, Maria Colomba; Di Gregorio, Tania; Raguso, Grazia; Daprati, Elena; Ringeval, Fabien; Schuller, Bjorn; Di Natale, Corrado; Martinelli, Eugenio","An emotional modulation model as signature for the identification of children developmental disorders","SCIENTIFIC REPORTS",8,,"SEP 27","",2018,"MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND","","","","","","","","NATURE PUBLISHING GROUP","","","","","","","In recent years, applications like Apple's Sid or Microsoft's Cortana    have created the illusion that one can actually ``chat{''} with a    machine. However, a perfectly natural human-machine interaction is far    from real as none of these tools can empathize. This issue has raised an    increasing interest in speech emotion recognition systems, as the    possibility to detect the emotional state of the speaker. This    possibility seems relevant to a broad number of domains, ranging from    man-machine interfaces to those of diagnostics. With this in mind, in    the present work, we explored the possibility of applying a precision    approach to the development of a statistical learning algorithm aimed at    classifying samples of speech produced by children with developmental    disorders(DD) and typically developing(TD) children. Under the    assumption that acoustic features of vocal production could not be    efficiently used as a direct marker of DD, we propose to apply the    Emotional Modulation function(EMF) concept, rather than running analyses    on acoustic features per se to identify the different classes. The novel    paradigm was applied to the French Child Pathological \& Emotional    Speech Database obtaining a final accuracy of 0.79, with maximum    performance reached in recognizing language impairment (0.92) and autism    disorder (0.82).","","","",""
7,"","WOS:000448549000002","Han, Tian; Zhang, Jincheng; Zhang, Zhu; Sun, Guobing; Ye, Liang; Ferdinando, Hany; Alasaarela, Esko; Seppanen, Tapio; Yu, Xiaoyang; Yang, Shuchang","Emotion recognition and school violence detection from children speech","EURASIP JOURNAL ON WIRELESS COMMUNICATIONS AND NETWORKING",,,"OCT 4","",2018,"CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND","","","","","","","","SPRINGEROPEN","","","","","","","School violence is a serious problem all over the world, and violence    detection is significant to protect juveniles. School violence can be    detected from the biological signals of victims, and emotion recognition    is an important way to detect violence events. In this research, a    violence simulation experiment was designed and performed for school    violence detection system. Emotional voice from the experiment was    extracted and analyzed. Consecutive elimination process (CEP) algorithm    was proposed for emotion recognition in this paper. After parameters    optimization, SVM was chosen as the classifier and the algorithm was    validated by Berlin database which is an emotional speech database of    adults, and the mean accuracy for seven emotions was 79.05\%. The    emotional speech database of children extracted in violence simulation    was also classified by SVM classifier with proposed CEP algorithm, and    the mean accuracy was 66.13\%. The results showed that high    classification performance could be achieved with the CEP algorithm. The    classification result was also compared with database of adults, and the    results indicated that children and adults voice should be treated    differently in speech emotion recognition researches. The accuracy of    children database is lower than adult database; the accuracy of violence    detection will be improved by other signals in the system.","","Emotion recognition; Children speech; Violence simulation","",""
7,"","WOS:000449007900002","Simoes, Marco; Monteiro, Raquel; Andrade, Joao; Mouga, Susana; Franca, Felipe; Oliveira, Guiomar; Carvalho, Paulo; Castelo-Branco, Miguel","A Novel Biomarker of Compensatory Recruitment of Face Emotional Imagery Networks in Autism Spectrum Disorder","FRONTIERS IN NEUROSCIENCE",12,,"NOV 1","",2018,"AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND","","","","","","","","FRONTIERS MEDIA SA","","","","","","","Imagery of facial expressions in Autism Spectrum Disorder (ASD) is    likely impaired but has been very difficult to capture at a    neurophysiological level. We developed an approach that allowed to    directly link observation of emotional expressions and imagery in ASD,    and to derive biomarkers that are able to classify abnormal imagery in    ASD. To provide a handle between perception and action imagery cycles it    is important to use visual stimuli exploring the dynamical nature of    emotion representation. We conducted a case-control study providing a    link between both visualization and mental imagery of dynamic facial    expressions and investigated source responses to pure face-expression    contrasts. We were able to replicate the same highly group    discriminative neural signatures during action observation (dynamical    face expressions) and imagery, in the precuneus. Larger activation in    regions involved in imagery for the ASD group suggests that this effect    is compensatory. We conducted a machine learning procedure to    automatically identify these group differences, based on the EEG    activity during mental imagery of facial expressions. We compared two    classifiers and achieved an accuracy of 81\% using 15 features (both    linear and non-linear) of the signal from theta, high-beta and gamma    bands extracted from right-parietal locations (matching the precuneus    region), further confirming the findings regarding standard statistical    analysis. This robust classification of signals resulting from imagery    of dynamical expressions in ASD is surprising because it far and    significantly exceeds the good classification already achieved with    observation of neutral face expressions (74\%). This novel neural    correlate of emotional imagery in autism could potentially serve as a    clinical interventional target for studies designed to improve facial    expression recognition, or at least as an intervention biomarker.","","emotional facial expression; mental imagery; EEG biomarker; machine learning; autism spectrum disorder; dynamic expressions","",""
7,"","WOS:000450540800010","Bolinger, Elaina; Born, Jan; Zinke, Katharina","Sleep divergently affects cognitive and automatic emotional response in children","NEUROPSYCHOLOGIA",117,,"AUG","84-91",2018,"THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","","","","","","","","PERGAMON-ELSEVIER SCIENCE LTD","","","","","","","Sleep enhances memory for emotional experiences, but its influence on    the emotional response associated with memories is elusive. Here, we    compared the influence of nocturnal sleep on memory for negative and    neutral pictures and the associated emotional response in 8-11-year-old    children, i.e., an age group with heightened levels of emotional    memory-related sleep features. During all sessions, emotional responses    as measured by subjective ratings, the late positive potential of the    EEG (LPP) and heart rate deceleration (HRD) were recorded. Sleep    enhanced picture memory. Compared to dynamics across wakefulness, sleep    decreased the emotional response in ratings and the LPP, while    increasing the emotional response in HRD. We conclude that sleep    consolidates immediate emotional meaning by enhancing more automatic    emotional responses while concurrently promoting top-down control of    emotional responses, perhaps through strengthening respective    neocortical representations.","","Development; Children; Sleep; Emotion; EEG; Memory","",""
7,"","WOS:000452599700001","Rusli, Nazreen; Sidek, Shahrul Na'im; Yusof, Hazlina Md; Ishak, Nor Izzati","Mean of Correlation Methodfor Optimization of Affective States Detection in Children","IEEE ACCESS",6,,"","68487-68497",2018,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","At the moment, most of the studies on classification of affective states    for children focus on visual observations and physiological cues, where    all data collection for measuring physiological signals are    contact-based and invasive. With the requirement of having the measuring    device attached to the body approach, distraction of the subject    normally masks the true affective states of the subject due to    discomfort. In this paper, a non-invasive, contactless, and less    distraction method is proposed to measure the physiological cues of the    subjects using their thermal imprints from frontal face imaging. A    thermal image camera is used to identify basic affective states, where    it is a contactless and seamless device with ability to read the    radiated thermal imprint of the subjects' facial skin temperature. This    paper proposes an effective algorithm of texture analysis based on novel    technique using Gray Level Co-occurrence Matrix approach to be applied    so as to identify blood-flow region. The cues from the first order    statistics are computed in the identified blood flow region and    concatenated along with second order statistics cues, in order to    construct feature vectors to administer the vital and distinguishable    characteristic pattern between affective states in thermal images.    Result from the fine k-NN classifier obtained promises the efficacy of    the proposed approach to be applied in our future work in human-robot    interaction for autistic children learning and training.","","Affective states; facial skin temperature; thermal imaging","",""
7,"","WOS:000455813300049","McGinnis, Ryan S.; McGinnis, Ellen W.; Hruschak, Jessica; Lopez-Duran, Nestor L.; Fitzgerald, Kate; Rosenblum, Katherine L.; Muzik, Maria","Rapid detection of internalizing diagnosis in young children enabled by wearable sensors and machine learning","PLOS ONE",14,1,"JAN 16","",2019,"1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA","","","","","","","","PUBLIC LIBRARY SCIENCE","","","","","","","There is a critical need for fast, inexpensive, objective, and accurate    screening tools for childhood psychopathology. Perhaps most compelling    is in the case of internalizing disorders, like anxiety and depression,    where unobservable symptoms cause children to go unassessed-suffering in    silence because they never exhibiting the disruptive behaviors that    would lead to a referral for diagnostic assessment. If left untreated    these disorders are associated with long-term negative outcomes    including substance abuse and increased risk for suicide. This paper    presents a new approach for identifying children with internalizing    disorders using an instrumented 90-second mood induction task.    Participant motion during the task is monitored using a commercially    available wearable sensor. We show that machine learning can be used to    differentiate children with an internalizing diagnosis from controls    with 81\% accuracy (67\% sensitivity, 88\% specificity). We provide a    detailed description of the modeling methodology used to arrive at these    results and explore further the predictive ability of each temporal    phase of the mood induction task. Kinematical measures most    discriminative of internalizing diagnosis are analyzed in detail,    showing affected children exhibit significantly more avoidance of    ambiguous threat. Performance of the proposed approach is compared to    clinical thresholds on parent-reported child symptoms which    differentiate children with an internalizing diagnosis from controls    with slightly lower accuracy (.68-. 75 vs. .81), slightly higher    specificity (.88-1.00 vs. .88), and lower sensitivity (.00-. 42 vs. .67)    than the proposed, instrumented method. These results point toward the    future use of this approach for screening children for internalizing    disorders so that interventions can be deployed when they have the    highest chance for long-term success.","","","",""
6,"978-1-5386-6420-9","WOS:000457843608078","Vicol, Paul; Tapaswi, Makarand; Castrejon, Lluis; Fidler, Sanja","MovieGraphs: Towards Understanding Human-Centric Situations from Videos","",,,"","8581-8590",2018,"345 E 47TH ST, NEW YORK, NY 10017 USA","31st IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, JUN 18-23, 2018","","2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)","","","IEEE Conference on Computer Vision and Pattern Recognition","","IEEE","","","","IEEE; CVF; IEEE Comp Soc","","","There is growing interest in artificial intelligence to build socially    intelligent robots. This requires machines to have the ability to    ``read{''} people's emotions, motivations, and other factors that affect    behavior. Towards this goal, we introduce a novel dataset called    MovieGraphs which provides detailed, graph-based annotations of social    situations depicted in movie clips. Each graph consists of several types    of nodes, to capture who is present in the clip, their emotional and    physical attributes, their relationships (i.e., parent/child), and the    interactions between them. Most interactions are associated with topics    that provide additional details, and reasons that give motivations for    actions. In addition, most interactions and many attributes are grounded    in the video with time stamps. We provide a thorough analysis of our    dataset, showing interesting common-sense correlations between different    social aspects of scenes, as well as across scenes over time. We propose    a method for querying videos and text with graphs, and show that: 1) our    graphs contain rich and sufficient information to summarize and localize    each scene; and 2) subgraphs allow us to describe situations at an    abstract level and retrieve multiple semantically relevant situations.    We also propose methods for interaction understanding via ordering, and    reason understanding. MovieGraphs is the first benchmark to focus on    inferred properties of human-centric situations, and opens up an    exciting avenue towards socially-intelligent AI agents.","","","",""
7,"","WOS:000458071800001","Song, Tengfei; Zheng, Wenming; Lu, Cheng; Zong, Yuan; Zhang, Xilei; Cui, Zhen","MPED: A Multi-Model Physiological Emotion Database for Discrete Emotion Recongnition","IEEE ACCESS",7,,"","12177-12191",2019,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","To explore human emotions, in this paper, we design and build a    multi-modal physiological emotion database, which collects four modal    physiological signals, i.e., electroencephalogram (EEG), galvanic skin    response, respiration, and electrocardiogram (ECG). To alleviate the    influence of culture dependent elicitation materials and evoke desired    human emotions, we specifically collect an emotion elicitation material    database selected from more than 1500 video clips. By the considerable    amount of strict man-made labeling, we elaborately choose 28 videos as    standardized elicitation samples, which are assessed by psychological    methods. The physiological signals of participants were synchronously    recorded when they watched these standardized video clips that described    six discrete emotions and neutral emotion. With three types of    classification protocols, different feature extraction methods and    classifiers (support vector machine and k-NearestNeighbor) were used to    recognize the physiological responses of different emotions, which    presented the baseline results. Simultaneously, we present a novel    attention-long short-term memory (A-LSTM), which strengthens the    effectiveness of useful sequences to extract more discriminative    features. In addition, correlations between the EEG signals and the    participants' ratings are investigated. The database has been made    publicly available to encourage other researchers to use it to evaluate    their own emotion estimation methods.","","Discrete emotion recognition; physiological signals; EEG; affective computing; machine learning; video-induced emotion; LSTM","",""
6,"978-1-5386-4110-1","WOS:000458534800090","Althobaiti, Turke; Katsigiannis, Stamos; West, Daune; Bronte-Stewart, Malcolm; Ramzan, Naeem","Affect Detection for Human-Horse Interaction","",,,"","",2018,"345 E 47TH ST, NEW YORK, NY 10017 USA","21st Saudi-Computer-Society National Computer Conference (NCC), Riyadh, SAUDI ARABIA, APR 25-26, 2018","","2018 21ST SAUDI COMPUTER SOCIETY NATIONAL COMPUTER CONFERENCE (NCC)","","","","","IEEE","","","","Saudi Comp Soc; IEEE Saudi Sect; IEEE","","","In this work, we aim to study the potential use of affect recognition    techniques for examining the interaction between humans and horses using    qualitative and quantitative methods. To this end, we propose a    multi-modal portable system for physiological signal acquisition such as    the electrocardiogram (ECG), electromyogram (EMG), and    electroencephalogram (EEG). The proposed system is used to acquire    signals while users are interacting with horses. The captured signals    will then be used in order to quantitatively evaluate human and equine    interaction by mapping the signals to the emotional state of the    subjects using machine learning techniques. In this preliminary study,    ECG based features were utilised in order to create a supervised    classification model that can identify emotions elicited during    human-horse interaction. Experimental results provide evidence about the    efficiency of the proposed approach in distinguishing between negative    and positive emotions, reaching a classification accuracy of 74.21\%.","","Emotion recognition; physiological signals; human/horse interaction; EEG; ECG; EMG","",""
6,"978-1-5386-2471-5","WOS:000458669700022","Rusli, Nazreen; Yusof, Hazlina Md; Sidek, Shahrul Naim; Ishak, Nor Izzati","GLCM Correlation Approach for Blood Vessel Identification in Thermal Image","",,,"","112-116",2018,"345 E 47TH ST, NEW YORK, NY 10017 USA","IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), Kuching, MALAYSIA, DEC 03-06, 2018","https://ieeexplore.ieee.org/document/8626697","2018 IEEE-EMBS CONFERENCE ON BIOMEDICAL ENGINEERING AND SCIENCES (IECBES)","","","IEEE EMBS Conference on Biomedical Engineering and Sciences","","IEEE","","","","SARAWAK Convent Bur; Physiolog Measurement","","","The maturity of detection in emotions via thermal camera is evolving    recently since it is able to detect the hot parts of human face    composition replicating the area of blood vessels. The notion of    non-invasive tools for data gatherings via a thermal camera has also    been vigorously highlighted. We hypothesize that, the impact of    cutaneous temperature changes due to blood flows in the blood vessels    could be correlated to specific emotion state for healthy as well as    autistic children. The autistic children are less able to present    emotion through facial expression. In this work, healthy children were    assigned as subjects prior to the development of the algorithm for    thermal imaging analysis to form a reference model. Facial thermal    distribution was analyzed and a technique using Correlation in Gray    Level Co-occurrence Matrices (GLCM) was proposed to determine the blood    vessels' region. A k-Nearest Neighbor (k-NN) classifier shows a    promising result for the proposed method and suggests that these    analyses are momentous for distinguishing between five basic emotions    and it could be used as non-verbal mediums to help on autistic children.","","Autistic; Emotion; GLCM; Thermal Imaging; Texture Analysis","",""
6,"978-1-5386-6378-3","WOS:000462636300077","Allouch, Merav; Azaria, Amos; Azoulay, Rina; Ben-Izchak, Ester; Zwilling, Moti; Zachor, Ditza A.","Automatic Detection of Insulting Sentences in Conversation","",,,"","",2018,"345 E 47TH ST, NEW YORK, NY 10017 USA","IEEE International Conference on the Science of Electrical Engineering in Israel (ICSEE), Eilat, ISRAEL, DEC 12-14, 2018","","2018 IEEE INTERNATIONAL CONFERENCE ON THE SCIENCE OF ELECTRICAL ENGINEERING IN ISRAEL (ICSEE)","","","","","IEEE","","","","IEEE","","","An overall goal of our work is to use machine-learning based solutions    to assist children with communication difficulties in their    communication task. In this paper, we concentrate on the problem of    recognizing insulting sentences the child says, or insulting sentences    that are told to him. An automated agent that is able to recognize such    sentences can alert the child in real time situations, and can suggest    how to respond to the resulting social situation. We composed a dataset    of 1241 non-insulting and 1255 insulting sentences. We trained different    machine learning methods on 90\% randomly chosen sentences from the    dataset and tested it on the remaining. We used the following machine    learning methods: Multi-Layer Neural Network, SVM, Naive Bayes, Decision    Tree, and Tree Bagger for the task. We found that the best predictors of    the insulting sentences, were the SVM method, with 80\% recall and over    75\% precision, and the Multi-Layer Neural Network and the Tree Bagger,    with precision and recall exceeding 75\%, We also found that adding    additional data to the learning process, such as 9500 labeled sentences    from twitter, or adding the word ``positive{''} and the word    ``negative{''} to sentences including positive or negative words,    respectively, slightly improves the results in most of the cases. Our    results provide the cornerstones for an automated system that would    enable on-line assistance and consultation for children with    communication disabilities, and also for other persons with    communication problems, in a way that will enable them to function    better in society through this assistance.","","Autism Spectrum Disorder; Machine Learning; Text Emotion Recognition","",""
7,"","WOS:000463786600008","Li, Yang; Zheng, Wenming; Cui, Zhen; Zong, Yuan; Ge, Sheng","EEG Emotion Recognition Based on Graph Regularized Sparse Linear Regression","NEURAL PROCESSING LETTERS",49,2,"APR","555-571",2019,"VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS","","","","","","","","SPRINGER","","","","","","","In this paper, a novel regression model, called graph regularized sparse    linear regression (GRSLR), is proposed to deal with EEG emotion    recognition problem. GRSLR extends the conventional linear regression    method by imposing a graph regularization and a sparse regularization on    the transform matrix of linear regression, such that it is able to    simultaneously cope with sparse transform matrix learning while preserve    the intrinsic manifold of the data samples. To detailed discuss the EEG    emotion recognition, we collect a set of 14 subjects EEG emotion data    and provide the experiment results on different features. To evaluate    the proposed GRSLR model, we conduct experiments on the SEED database    and RCLS database. The experimental results show that the proposed    algorithm GRSLR is superior to the classic baselines. The RCLS database    is made publicly available and other researchers could use it to test    their own emotion recognition method.","","EEG; Emotion recognition; Sparse linear regression","",""
6,"978-1-5108-7221-9","WOS:000465363900052","Yijia, Xu; Allan, Hasegawa Johnson Mark; Nancy, McElwain L.","Infant emotional outbursts detection in infant-parent spoken interactions","",,,"","242-246",2018,"C/O EMMANUELLE FOXONET, 4 RUE DES FAUVETTES, LIEU DIT LOUS TOURILS, BAIXAS, F-66390, FRANCE","19th Annual Conference of the International-Speech-Communication-Association (INTERSPEECH 2018), Hyderabad, INDIA, AUG 02-SEP 06, 2018","","19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING MARKETS IN MULTILINGUAL SOCIETIES","","","Interspeech","","ISCA-INT SPEECH COMMUNICATION ASSOC","","","","Int Speech Commun Assoc","","","Detection of infant emotional outbursts, such as crying, in large    corpora of recorded infant speech, is essential to the study of dyadic    social process, by which infants learn to identify and regulate their    own emotions. Such large corpora now exist with the advent of LENA    speech monitoring systems, but are not labeled for emotional outbursts.    This paper reports on our efforts to manually code child utterances as    being of type ``laugh{''}, ``cry{''}, ``fuss{''}, ``babble{''} and    ``hiccup{''}, and to develop algorithms capable of performing the same    task automatically. Human labelers achieve much higher rates of    inter-coder agreement for some of these categories than for others.    Linear discriminant analysis (LDA) achieves better accuracy on tokens    that have been coded by two human labelers than on tokens that have been    coded by only one labeler, but the difference is not as much as we    expected, suggesting that the acoustic and contextual features being    used by human labelers are not yet available to the LDA. Convolutional    neural network and hidden markov model achieve better accuracy than LDA,    but worse F-score, because they over-weight the prior. Discounting the    transition probability does not solve the problem.","","infant vocalizations; infant emotional outbursts; convolutional neural network; linear discriminant analysis; hidden markov model","",""
7,"","WOS:000473119900013","Capriola-Hall, Nicole N.; Wieckowski, Andrea Trubanova; Swain, Deanna; Tech, Virginia; Aly, Sherin; Youssef, Amira; Abbott, A. Lynn; White, Susan W.","Group Differences in Facial Emotion Expression in Autism: Evidence for the Utility of Machine Classification","BEHAVIOR THERAPY",50,4,"JUL","828-838",2019,"525 B STREET, STE 1900, SAN DIEGO, CA 92101-4495 USA","","","","","","","","ELSEVIER INC","","","","","","","Effective social communication relies, in part, on accurate nonverbal    expression of emotion. To evaluate the nature of facial emotion    expression (FEE) deficits in children with autism spectrum disorder    (ASD), we compared 20 youths with ASD to a sample of typically    developing (TD) youth (n = 20) using a machine-based classifier of FEE.    Results indicate group differences in FEE for overall accuracy across    emotions. In particular, a significant group difference in accuracy of    FEE was observed when participants were prompted by a video of a human    expressing an emotion, F(2, 36) = 4.99, p = .032, eta(2) = .12.    Specifically, youth with ASD made significantly more errors in FEE    relative to TD youth. Findings support continued refinement of    machine-based approaches to assess and potentially remediate FEE    impairment in youth with ASD.","","autism spectrum disorder; facial emotion expression; machine learning","",""
7,"","WOS:000473769800001","Dawood, Amina; Turner, Scott; Perepa, Prithvi","Natural-Spontaneous Affective-Cognitive Dataset for Adult Students With and Without Asperger Syndrome","IEEE ACCESS",7,,"","77990-77999",2019,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","Any viable algorithm to infer affective states of individuals with    autism requires natural and reliable data in real time and in an    uncontrolled environment. For this purpose, this study provides a new    natural-spontaneous affective-cognitive dataset based on facial    expressions, eye gaze, and head movements for adult students with and    without Asperger syndrome (AS). The data gathering and collecting in a    computer-based learning environment is one of the significant areas,    which has attracted researchers' attention in affective computing    applications. Due to the important impact of emotions on students    learning outcome and their performance, the dataset included a range of    affective-cognitive states which goes beyond basic emotions. This study    reports the methodology that was used in data collection and annotation.    Description and comparison of other available datasets were summarized,    and also the study presents the results that were concluded in more    details. In addition, some challenges were inherent to this study.","","Emotional dataset; autism; Asperger syndrome; spontaneous; natural; facial expressions; affective computing; affective-cognitive states","",""
7,"","WOS:000473770200001","Althobaiti, Turke; Katsigiannis, Stamos; West, Daune; Ramzan, Naeem","Examining Human-Horse Interaction by Means of Affect Recognition via Physiological Signals","IEEE ACCESS",7,,"","77857-77867",2019,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","For some time, equine-assisted therapy (EAT), i.e., the use of    horse-related activities for therapeutic reasons, has been recognised as    a useful approach in the treatment of many mental health issues such as    post-traumatic stress disorder (PTSD), depression, and anxiety. However,    despite the interest in EAT, few scientific studies have focused on    understanding the complex emotional response that horses seem to elicit    in human riders and handlers. In this work, the potential use of affect    recognition techniques based on physiological signals is examined for    the task of assessing the interaction between humans and horses in terms    of the emotional response of the humans to this interaction.    Electroencephalography (EEG), electrocardiography (ECG), and    electromyography (EMG) signals were captured from humans interacting    with horses, and machine learning techniques were applied in order to    predict the self-reported emotional states of the human subjects in    terms of valence and arousal. Supervised classification experiments    demonstrated the potential of this approach for affect recognition    during human-horse interaction, reaching an F1-score of 78.27\% for    valence and 65.49\% for arousal.","","Affective computing; ECG; EEG; EMG; emotion recognition; equine assisted therapy (EAT); human-horse interaction; physiological signals","",""
7,"","WOS:000474504900002","Samad, Manar D.; Diawara, Norou; Bobzien, Jonna L.; Taylor, Cora M.; Harrington, John W.; Iftekharuddin, Khan M.","A pilot study to identify autism related traits in spontaneous facial actions using computer vision","RESEARCH IN AUTISM SPECTRUM DISORDERS",65,,"SEP","14-24",2019,"THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND","","","","","","","","ELSEVIER SCI LTD","","","","","","","Background: Individuals with autism spectrum disorders (ASD) may be    differentiated from typically developing controls (TDC) based on    phenotypic features in spontaneous facial expressions. Computer vision    technology can automatically track subtle facial actions to gain    quantitative insights into ASD related behavioral abnormalities.    Method: This study proposes a novel psychovisual human-study to elicit    spontaneous facial expressions in response to a variety of social and    emotional contexts. We introduce a markerless facial motion capture and    computer vision methods to track spontaneous and subtle activations of    facial muscles. The facial muscle activations are encoded into ten    representative facial action units (FAU) to gain quantitative, granular,    and contextual insights into the psychophysical development of the    participating individuals. Statistical tests are performed to identify    differential traits in individuals with ASD after comparing those in a    cohort of age-matched TDC individuals.    Results: The proposed framework has revealed significant difference (p <    0.001) in the activation of ten FAU and contrasting activations of FAU    between the group with ASD and the TDC group. Unlike the TDC group, the    group with ASD has shown unusual prevalence of mouth frown (FAU 15) and    low correlations in temporal activations of several FAU pairs: 6-12,    10-12, and 10-20. The interpretation of different FAU activations    suggests quantitative evidence of expression bluntness, lack of    expression mimicry, incongruent reaction to negative emotions in the    group with ASD.    Conclusion: Our generalized framework may be used to quantify    psychophysical traits in individuals with ASD and replicate in similar    studies that require quantitative measurements of behavioral responses.","","ASD; Behavioral marker; Differential traits; Facial action units; Computer vision; Spontaneous expressions","",""
7,"","WOS:000474919900007","Haines, Nathaniel; Bell, Ziv; Crowell, Sheila; Hahn, Hunter; Kamara, Dana; McDonough-Caplan, Heather; Shader, Tiffany; Beauchaine, Theodore P.","Using automated computer vision and machine learning to code facial expressions of affect and arousal: Implications for emotion dysregulation research","DEVELOPMENT AND PSYCHOPATHOLOGY",31,3, SI,"AUG","871-886",2019,"32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA","","","","","","","","CAMBRIDGE UNIV PRESS","","","","","","","As early as infancy, caregivers' facial expressions shape children's    behaviors, help them regulate their emotions, and encourage or dissuade    their interpersonal agency. In childhood and adolescence, proficiencies    in producing and decoding facial expressions promote social competence,    whereas deficiencies characterize several forms of psychopathology. To    date, however, studying facial expressions has been hampered by the    labor-intensive, time-consuming nature of human coding. We describe a    partial solution: automated facial expression coding (AFEC), which    combines computer vision and machine learning to code facial expressions    in real time. Although AFEC cannot capture the full complexity of human    emotion, it codes positive affect, negative affect, and arousal-core    Research Domain Criteria constructs-as accurately as humans, and it    characterizes emotion dysregulation with greater specificity than other    objective measures such as autonomic responding. We provide an example    in which we use AFEC to evaluate emotion dynamics in mother-daughter    dyads engaged in conflict. Among other findings, AFEC (a) shows    convergent validity with a validated human coding scheme, (b)    distinguishes among risk groups, and (c) detects developmental increases    in positive dyadic affect correspondence as teen daughters age. Although    more research is needed to realize the full potential of AFEC, findings    demonstrate its current utility in research on emotion dysregulation.","","arousal; emotion dysregulation; facial expression; negative valence system; positive valence system","",""
7,"","WOS:000475304200070","Gao, Zhilin; Cui, Xingran; Wan, Wang; Gu, Zhongze","Recognition of Emotional States Using Multiscale Information Analysis of High Frequency EEG Oscillations","ENTROPY",21,6,"JUN","",2019,"ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","","","","","","","","MDPI","","","","","","","Exploring the manifestation of emotion in electroencephalogram (EEG)    signals is helpful for improving the accuracy of emotion recognition.    This paper introduced the novel features based on the multiscale    information analysis (MIA) of EEG signals for distinguishing emotional    states in four dimensions based on Russell's circumplex model. The    algorithms were applied to extract features on the DEAP database, which    included multiscale EEG complexity index in the time domain, and    ensemble empirical mode decomposition enhanced energy and fuzzy entropy    in the frequency domain. The support vector machine and cross validation    method were applied to assess classification accuracy. The    classification performance of MIA methods (accuracy = 62.01\%, precision    = 62.03\%, recall/sensitivity = 60.51\%, and specificity = 82.80\%) was    much higher than classical methods (accuracy = 43.98\%, precision =    43.81\%, recall/sensitivity = 41.86\%, and specificity = 70.50\%), which    extracted features contain similar energy based on a discrete wavelet    transform, fractal dimension, and sample entropy. In this study, we    found that emotion recognition is more associated with high frequency    oscillations (51-100Hz) of EEG signals rather than low frequency    oscillations (0.3-49Hz), and the significance of the frontal and    temporal regions are higher than other regions. Such information has    predictive power and may provide more insights into analyzing the    multiscale information of high frequency oscillations in EEG signals.","","emotion recognition; EEG; multiscale information analysis; multiscale sample entropy; ensemble empirical mode decomposition; fuzzy entropy; support vector machine","",""
6,"978-3-030-01449-0; 978-3-030-01448-3","WOS:000476892400027","De Feyter, Floris; Van Beeck, Kristof; Goedeme, Toon","Automatically Selecting the Best Pictures for an Individualized Child Photo Album","",11182,,"","321-332",2018,"GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND","19th International Conference on Advanced Concepts for Intelligent Vision Systems (ACIVS), Poitiers, FRANCE, SEP 24-27, 2018","","ADVANCED CONCEPTS FOR INTELLIGENT VISION SYSTEMS, ACIVS 2018","","","Lecture Notes in Computer Science","BlancTalon, J.; Helbert, D.; Philips, W.; Popescu, D.; Scheunders, P.","SPRINGER INTERNATIONAL PUBLISHING AG","","","","Univ Antwerp","","","In this paper we investigate the best way to automatically compose a    photo album for an individual child from a large collection of    photographs taken during a school year. For this, we efficiently combine    state-of-the-art identification algorithms to select relevant photos,    with an aesthetics estimation algorithm to only keep the best images.    For the identification task, we achieved 86\% precision for 86\% recall    on a real-life dataset containing lots of specific challenges of this    application. Indeed, playing children appear in non-standard poses and    facial expressions, can be dressed up or have their faces painted etc.    In a top-1 sense, our system was able to correctly identify 89.2\% of    the faces in close-up. Apart from facial recognition, we discuss and    evaluate extending the identification system with person    re-identification. To select out the best-looking photos from the    identified child photos to fill the album with, we propose an automatic    assessment technique that takes into account the aesthetic photo quality    as well as the emotions in the photos. Our experiments show that this    measure correlates well with a manually labeled general appreciation    score.","","Face recognition; Person re-identification; Aesthetics analysis; Emotion classification; Child identification","",""
7,"","WOS:000477906900017","Wei, Pengcheng; Zhao, Yu","A novel speech emotion recognition algorithm based on wavelet kernel sparse classifier in stacked deep auto-encoder model","PERSONAL AND UBIQUITOUS COMPUTING",23,3-4, SI,"JUL","521-529",2019,"236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND","","","","","","","","SPRINGER LONDON LTD","","","","","","","Since the contextual information has an important impact on the    speaker's emotional state, how to use emotion-related context    information to conduct feature learning is a key problem. The existing    speech emotion recognition algorithms achieve the relatively high    recognition rate; these algorithms are not very good application to the    real-life speech emotion recognition systems. Therefore, in order to    address the abovementioned issues, a novel speech emotion recognition    algorithm based on improved stacked kernel sparse deep model is proposed    in this paper, which is based on auto-encoder, denoising auto-encoder,    and sparse auto-encoder to improve the Chinese speech emotion    recognition. The first layer of the structure uses a denoising    auto-encoder to learn a hidden feature with a larger dimension than the    dimension of the input features, and the second layer employs a sparse    auto-encoder to learn sparse features. Finally, a wavelet-kernel sparse    SVM classifier is applied to classify the features. The proposed    algorithm is evaluated on the testing dataset, which contains the speech    emotion data of spontaneous, non-prototypical, and long-term. The    experimental results show that the proposed algorithm outperforms the    existing state-of-the-art algorithms in speech emotion recognition.","","Contextual information; Emotion recognition; Auto-encoder; Kernel sparse; Sub-utterance-level; Support vector machine; Hidden feature; Deep learning","",""
7,"","WOS:000480354200004","Filntisis, Panagiotis Paraskevas; Efthymiou, Niki; Koutras, Petros; Potamianos, Gerasimos; Maragos, Petros","Fusing Body Posture With Facial Expressions for Joint Recognition of Affect in Child-Robot Interaction","IEEE ROBOTICS AND AUTOMATION LETTERS",4,4,"OCT","4011-4018",2019,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","In this letter, we address the problem of multi-cue affect recognition    in challenging scenarios such as child-robot interaction. Toward this    goal we propose a method for automatic recognition of affect that    leverages body expressions alongside facial ones, as opposed to    traditional methods that typically focus only on the latter. Our    deep-learning based method uses hierarchical multi-label annotations and    multi-stage losses, can be trained both jointly and separately, and    offers us computational models for both individual modalities, as well    as for the whole body emotion. We evaluate our method on a challenging    child-robot interaction database of emotional expressions collected by    us, as well as on the GEneva multimodal emotion portrayal public    database of acted emotions by adults, and show that the proposed method    achieves significantly better results than facial-only expression    baselines.","","Gesture; posture and facial expressions; computer vision for other robotic applications; social human-robot interaction; deep learning in robotics and automation","",""
6,"978-1-7281-0397-6","WOS:000483076402203","Aslam, Abdul Rehman; Bin Altaf, Muhammad Awais","An 8 Channel Patient Specific Neuromorphic Processor for the early screening of Autistic Children through Emotion Detection","",,,"","",2019,"345 E 47TH ST, NEW YORK, NY 10017 USA","IEEE International Symposium on Circuits and Systems (IEEE ISCAS), Sapporo, JAPAN, MAY 26-29, 2019","","2019 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)","","","IEEE International Symposium on Circuits and Systems","","IEEE","","","","IEEE; IEEE Circuits \& Syst Soc; Hokkaido Univ, Global Inst Collaborat Res \& Educ, Big Data \& Cybersecur; Springer Nature; River Publishers; Sci Council Japan; IEEE Circuits \& Syst Soc Japan Joint Chapter; IEEE Circuits \& Syst Soc Fukuoka Chapter; IEEE Circuits \& Syst Soc Kansai Chapter; IEEE Circuits \& Syst Soc Shikoku Chapter","","","Autism Spectrum Disorder (ASD) is a neurodevelopment disorder that    affects children's development and can lead to handicap life if remain    untreated. Scalp Electroencephalography (EEG) data can be used as a    biomarker to characterize the human emotions on the valence-arousal    scale. This work presents a machine learning patient-specific emotion    detection (PSED) classification processor based on an eight-channel EEG    signal. The proposed PSED classification processor integrates a    hardware-efficient feature extraction engine and patient-specific    support vector machine (SVM) classifier to discriminate the emotions in    real-time. To utilize minimal hardware resources a hardware realizable    feature set comprising of power spectral density (PSD), an absolute    difference of interhemispheric power asymmetry (IHPD), and the scaled    interhemispheric power asymmetry ratio (SIHPR) of eight electrode pairs    are evaluated. To avoid high overhead of area and power consumption for    an integer divider for SIHPR; simple LUT based divider is proposed that    calculates the approximated value of SIHPR with a minimal overhead of 64    Bytes. The classification is performed using a Linear SVM and resulted    in an accuracy of 63\% and 60\% for valence and arousal, respectively,    based on the database for emotion analysis using physiological signals    (DEAP). The PSED processor is synthesized using a 65nm CMOS technology    with an overall energy efficiency of 10uJ/classification.","","arousal; autism spectrum disorder (ASD); electroencephalogram (EEG); emotion; a look-up table (LUT); support vector machine (SVM); valence","",""
7,"","WOS:000486598700001","Vaidya, Chandan J.; You, Xiaozhen; Mostofsky, Stewart; Pereira, Francisco; Berl, Madison M.; Kenworthy, Lauren","Data-driven identification of subtypes of executive function across typical development, attention deficit hyperactivity disorder, and autism spectrum disorders","JOURNAL OF CHILD PSYCHOLOGY AND PSYCHIATRY",61,1,"JAN","51-61",2020,"111 RIVER ST, HOBOKEN 07030-5774, NJ USA","","","","","","","","WILEY","","","","","","","Background Impairment of executive function (EF), the goal-directed    regulation of thoughts, actions, and emotions, drives negative outcomes    and is common across neurodevelopmental disorders including attention    deficit hyperactivity disorder (ADHD) and autism spectrum disorder    (ASD). A primary challenge to its amelioration is heterogeneity in    symptom expression within and across disorders. Parsing this    heterogeneity is necessary to attain diagnostic precision, a goal of the    NIMH Research Domain Criteria Initiative. We aimed to identify    transdiagnostic subtypes of EF that span the normal to impaired spectrum    and establish their predictive and neurobiological validity. Methods    Community detection was applied to clinical parent-report measures in    8-14-year-old children with and without ADHD and ASD from two    independent cohorts (discovery N = 320; replication N = 692) to identify    subgroups with distinct behavioral profiles. Support vector machine    (SVM) classification was used to predict subgroup membership of unseen    cases. Preliminary neurobiological validation was obtained with existing    functional magnetic resonance imaging (fMRI) data on a subsample (N =    84) by testing hypotheses about sensitivity of EF subgroups versus DSM    categories. Results We observed three transdiagnostic EF subtypes    characterized by behavioral profiles that were defined by relative    weakness in: (a) flexibility and emotion regulation; (b) inhibition; and    (c) working memory, organization, and planning. The same tripartite    structure was also present in the typically developing children. SVM    trained on the discovery sample and tested on the replication sample    classified subgroup membership with 77.0\% accuracy. Split-half SVM    classification on the combined sample (N = 1,012) yielded 88.9\%    accuracy (this SVM is available for public use). As hypothesized,    frontal-parietal engagement was better distinguished by EF subtype than    DSM diagnosis and the subgroup characterized with inflexibility failed    to modulate right IPL activation in response to increased executive    demands. Conclusions The observed transdiagnostic subtypes refine    current diagnostic nosology and augment clinical decision-making for    personalizing treatment of executive dysfunction in children.","","Attention deficit hyperactivity disorder; autism spectrum disorders; functional MRI (fMRI); individual differences; machine learning","",""
7,"","WOS:000488323400008","Kalantarian, Haik; Jedoui, Khaled; Washington, Peter; Tariq, Qandeel; Dunlap, Kaiti; Schwartz, Jessey; Wall, Dennis P.","Labeling images with facial emotion and the potential for pediatric healthcare","ARTIFICIAL INTELLIGENCE IN MEDICINE",98,,"JUL","77-86",2019,"RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","","","","","","","","ELSEVIER","","","","","","","Autism spectrum disorder (ASD) is a neurodevelopmental disorder    characterized by repetitive behaviors, narrow interests, and deficits in    social interaction and communication ability. An increasing emphasis is    being placed on the development of innovative digital and mobile systems    for their potential in therapeutic applications outside of clinical    environments. Due to recent advances in the field of computer vision,    various emotion classifiers have been developed, which have potential to    play a significant role in mobile screening and therapy for    developmental delays that impair emotion recognition and expression.    However, these classifiers are trained on datasets of predominantly    neurotypical adults and can sometimes fail to generalize to children    with autism. The need to improve existing classifiers and develop new    systems that overcome these limitations necessitates novel methods to    crowdsource labeled emotion data from children. In this paper, we    present a mobile charades-style game, Guess What?, from which we derive    egocentric video with a high density of varied emotion from a 90-second    game session. We then present a framework for semi-automatic labeled    frame extraction from these videos using meta information from the game    session coupled with classification confidence scores. Results show that    94\%, 81\%, 92\%, and 56\% of frames were automatically labeled    correctly for categories disgust, neutral, surprise, and scared    respectively, though performance for angry and happy did not improve    significantly from the baseline.","Proposta de criação de datasets","Mobile games; Computer vision; Autism; Emotion; Emotion classification","",""
6,"978-1-5386-7980-7","WOS:000494315600125","Askari, Farzaneh; Feng, Haunghao; Sweeny, Timothy; Mahoor, Mohammad H.","A Pilot Study on Facial Expression Recognition Ability of Autistic Children Using Ryan, A Rear-Projected Humanoid Robot","",,,"","790-795",2018,"345 E 47TH ST, NEW YORK, NY 10017 USA","27th IEEE International Symposium on Robot and Human Interactive Communication (IEEE RO-MAN), Nanjing, PEOPLES R CHINA, AUG 27-31, 2018","","2018 27TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (IEEE RO-MAN 2018)","","","IEEE RO-MAN","Cabibihan, J. J.; Mastrogiovanni, F.; Pandey, A. K.; Rossi, S.; Staffa, M.","IEEE","","","","IEEE; IEEE Robot \& Automat Soc; Robot Soc Japan; Korea Robot Soc; Nanjing Forestry Univ","","","Rear-projected robots use computer graphics technology to create facial    animations and project them on a mask to show the robot's facial cues    and expressions. These types of robots are becoming commercially    available, though more research is required to understand how they can    be effectively used as a socially assistive robotic agent. This paper    presents the results of a pilot study on comparing the facial expression    recognition abilities of children with Autism Spectrum Disorder (ASD)    with typically developing (TD) children using a rear-projected humanoid    robot called Ryan. Six children with ASD and six TD children    participated in this research, where Ryan showed them six basic    expressions (i.e. anger, disgust, fear, happiness, sadness, and    surprise) with different intensity levels. Participants were asked to    identify the expressions portrayed by Ryan. The results of our study    show that there is not any general impairment in expression recognition    ability of the ASD group comparing to the TD control group; however,    both groups showed deficiencies in identifying disgust and fear.    Increasing the intensity of Ryan's facial expressions significantly    improved the expression recognition accuracy. Both groups were    successful to recognize the expressions demonstrated by Ryan with high    average accuracy.","","","",""
7,"","WOS:000501385900001","Khullar, Vikas; Bala, Manju; Singh, Harjit Pal","Interactive video-player to improve social smile in individuals with autism spectrum disorder","ADVANCES IN AUTISM",6,2,"DEC 4","109-119",2019,"HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND","","","","","","","","EMERALD GROUP PUBLISHING LTD","","","","","","","Purpose The purpose of this paper is to propose and develop a live    interaction-based video player system named LIV4Smile for the    improvement of the social smile in individuals with autism spectrum    disorder (ASD). Design/methodology/approach The proposed LIV4Smile    intervention was a video player that operated by detecting smile using a    convolutional neural network (CNN)-based algorithm. To maintain a live    interaction, a CNN-based smile detector was configured and used in this    system. The statistical test was also conducted to validate the    performance of the system. Findings The significant improvement was    observed in smile responses of individuals with ASD with the utilization    of the proposed LIV4Smile system in a real-time environment.    Originality/value The main aim of this study was to address the    inclusive practices for children with autism. The proposed CNN    algorithm-based LIV4Smile intervention resulted in high accuracy in    facial smile detection.","","Autism spectrum disorder; Convolutional neural network; Behavioral improvement; Facial emotion; Social smile; Video player","",""
6,"978-1-5386-7693-6","WOS:000503454900009","Alshamsi, Humaid; VetonKepuska; Alshamsi, Hazza; Meng, Hongying","Automated Speech Emotion Recognition on Smart Phones","",,,"","44-50",2018,"345 E 47TH ST, NEW YORK, NY 10017 USA","9th Annual IEEE Ubiquitous Computing, Electronics and Mobile Communication Conference (UEMCON), New York, NY, NOV 08-10, 2018","","2018 9TH IEEE ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS \& MOBILE COMMUNICATION CONFERENCE (UEMCON)","","","","Chakrabarti, S.; Saha, H. N.","IEEE","","","","IEEE; Columbia Univ; Int Engn \& Management; IEEE Reg 1; IEEE USA; IEEE New York Sect; Univ Engn \& Management","","","The emergence of Speech Emotion Recognition (SER) as a focal point of    research into speech processing reflects its significance in the field    of Human-Computer Interaction (HCI). It is core required functionality    for a variety of applications and a high degree of accuracy is critical;    activities as diverse as evaluating levels of emotion in children in    care and measuring customer satisfaction. The extent of demand for    accurate SER is reflected in the significant number of papers that have    been published and studies performed. An innovative approach to speech    recognition is presented in this paper centered on a cloud model    alongside the conventional system for measuring emotion in speech. There    are multiple stages involved in detecting and identifying emotions in    speech from audio clips. The initial pre-processing stage detects the    speech in the audio file and applies noise reduction. Next, the system    uses Mel-frequency cepstral coefficient (MFCC) algorithms to extract    features.This process results in the creation of testing and training    datasets populated with the emotions: Neutral, Happiness, Sadness, Fear,    Surprise, Disgust and Anger. The classification stage utilizes Support    Vector Machine (SVM) classifiers to identify the emotion. An additional    step implements a Confusion Matrix (CM) method to assess how these    classifiers performed. Testing was executed against RAVDESS and SAVEE    databases, where the detection rate achieved against the RAVDESS    database was 95.3\%.","","Speech Processing; Mel-frequency cepstral coefficient; Speech Emotion Recognition; Mobile Computing","",""
7,"","WOS:000513275100008","Jaliaawala, Muhammad Shoaib; Khan, Rizwan Ahmed","Can autism be catered with artificial intelligence-assisted intervention technology? A comprehensive survey","ARTIFICIAL INTELLIGENCE REVIEW",53,2,"FEB","1039-1069",2020,"VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS","","","","","","","","SPRINGER","","","","","","","This article presents an extensive literature review of technology based    intervention methodologies for individuals facing autism spectrum    disorder (ASD). Reviewed methodologies include: contemporary computer    aided systems, computer vision assisted technologies and virtual reality    (VR) or artificial intelligence (AI)-assisted interventions. The    research over the past decade has provided enough demonstrations that    individuals with ASD have a strong interest in technology based    interventions, which are useful in both, clinical settings as well as at    home and classrooms. Despite showing great promise, research in    developing an advanced technology based intervention that is clinically    quantitative for ASD is minimal. Moreover, the clinicians are generally    not convinced about the potential of the technology based interventions    due to non-empirical nature of published results. A major reason behind    this lack of acceptability is that a vast majority of studies on    distinct intervention methodologies do not follow any specific standard    or research design. We conclude from our findings that there remains a    gap between the research community of computer science, psychology and    neuroscience to develop an AI assisted intervention technology for    individuals suffering from ASD. Following the development of a    standardized AI based intervention technology, a database needs to be    developed, to devise effective AI algorithms.","","Computer aided systems (CAS); Computer vision assisted technologies (CVAT); Autism spectrum disorder (ASD); Facial expression recognition; Artificial intelligence; Virtual reality","",""
7,"","WOS:000518894200002","Drimalla, Hanna; Scheffer, Tobias; Landwehr, Niels; Baskow, Irina; Roepke, Stefan; Behnia, Behnoush; Dziobek, Isabel","Towards the automatic detection of social biomarkers in autism spectrum disorder: introducing the simulated interaction task (SIT)","NPJ DIGITAL MEDICINE",3,1,"FEB 28","",2020,"HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY","","","","","","","","NATURE PORTFOLIO","","","","","","","Social interaction deficits are evident in many psychiatric conditions    and specifically in autism spectrum disorder (ASD), but hard to assess    objectively. We present a digital tool to automatically quantify    biomarkers of social interaction deficits: the simulated interaction    task (SIT), which entails a standardized 7-min simulated dialog via    video and the automated analysis of facial expressions, gaze behavior,    and voice characteristics. In a study with 37 adults with ASD without    intellectual disability and 43 healthy controls, we show the potential    of the tool as a diagnostic instrument and for better description of    ASD-associated social phenotypes. Using machine-learning tools, we    detected individuals with ASD with an accuracy of 73\%, sensitivity of    67\%, and specificity of 79\%, based on their facial expressions and    vocal characteristics alone. Especially reduced social smiling and    facial mimicry as well as a higher voice fundamental frequency and    harmony-to-noise-ratio were characteristic for individuals with ASD. The    time-effective and cost-effective computer-based analysis outperformed a    majority vote and performed equal to clinical expert ratings.","","","",""
7,"","WOS:000520552800001","Law, Effie Lai-Chong; Soleimani, Samaneh; Watkins, Dawn; Barwick, Joanna","Automatic voice emotion recognition of child-parent conversations in natural settings","BEHAVIOUR \& INFORMATION TECHNOLOGY",40,11,"AUG 18","1072-1089",2021,"2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND","","","","","","","","TAYLOR \& FRANCIS LTD","","","","","","","While voice communication of emotion has been researched for decades,    the accuracy of automatic voice emotion recognition (AVER) is yet to    improve. In particular, the intergenerational communication has been    under-researched, as indicated by the lack of an emotion corpus on    child-parent conversations. In this paper, we presented our work of    applying Support-Vector Machines (SVMs), established machine learning    models, to analyze 20 pairs of child-parent dialogues on everyday life    scenarios. Among many issues facing the emerging work of AVER, we    explored two critical ones: the methodological issue of optimising its    performance against computational costs, and the conceptual issue on the    state of emotionally neutral. We used the minimalistic/extended acoustic    feature set extracted with OpenSMILE and a small/large set of annotated    utterances for building models, and analyzed the prevalence of the class    neutral. Results indicated that the bigger the combined sets, the better    the training outcomes. Regardless, the classification models yielded    modest average recall when applied to the child-parent data, indicating    their low generalizability. Implications for improving AVER and its    potential uses are drawn.","","Vocal emotion; child-parent conversation; recognition accuracy; emotion corpora; emotion neutrality; IEMOCAP","",""
7,"","WOS:000520872700001","Shrawankar, Urmila; Shireen, Azra","Suggesting teaching methods by analyzing the behavior of children with special needs","BIO-ALGORITHMS AND MED-SYSTEMS",16,1,"MAR","",2020,"GENTHINER STRASSE 13, D-10785 BERLIN, GERMANY","","","","","","","","WALTER DE GRUYTER GMBH","","","","","","","The behavioral pattern of children with special needs depends on their    emotional and developmental disability. Any abnormal and incorrect    pattern of behavior which is below the level of development as the    expected norm can be considered as ``the challenging behavior.{''} For    supporting children with behavioral problems, many interventions and    strategies alone, or in combination, are used. Behavioral and    developmental problems, if not treated well, in childhood, may cause a    problem and have negative long-term and short-term effects on a child's    personal life, education, family, and professional life. Detailed    knowledge about the child's behavior is important to define the problem.    Hence, the software contains the questionnaire pattern, which is divided    into categories like parents, teachers, doctors, and friends. Child    behavior checklist is used to define the questionnaire. Images and video    analysis are used to detect the current emotion in the child. By    analyzing the behavioral pattern and current emotion, the teaching    method will be suggested.","","behavioral disorder; behavioral recognizing; data analytics; data preprocessing; decision tree algorithm; Microsoft emotion/face API; mood detection; teaching interventions; video frames detector; video processing","",""
6,"978-1-7281-1859-8","WOS:000524664400038","Zheng, Chunjun; Jia, Ning; Sun, Wei","The Extraction Method of Emotional Feature Based on Children's Spoken Speech","",,,"","165-168",2019,"345 E 47TH ST, NEW YORK, NY 10017 USA","11th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC), Zhejiang Univ, Hangzhou, PEOPLES R CHINA, AUG 24-25, 2019","","2019 11TH INTERNATIONAL CONFERENCE ON INTELLIGENT HUMAN-MACHINE SYSTEMS AND CYBERNETICS (IHMSC 2019), VOL 1","","","International Conference on Intelligent Human-Machine Systems and Cybernetics","","IEEE","","","","IEEE Comp Soc; Univ Bristol; Japan Adv Inst Sci \& Technol; IEEE CIS Nanjing Chapter; Beihang Univ; IEEE","","","Most modern people ignore the importance of reading aloud. However, for    children aged 5-12, reading aloud is not only an essential skill in the    learning process, but also an effective means of cultivating sentiment.    Because there is a nonlinear relationship between the characteristics of    the spoken speech signal and the evaluation criteria, the emotional    features suitable for children's reading evaluation are extracted from    the audio signal, which is very important for the recognition of    children's reading emotions. However, automatically recognizing emotions    from speech is a challenging task, and its recognition depends on the    validity of the speech emotion features and the accuracy of the model.    In this research, we start with traditional Low Level Descriptors (LLD)    to learn emotion-related features automatically which were found in    speech, using High Level Statistics Functions (HSF), and emotion-related    Short time frame level acoustic features can be learned. These features    are appropriately aggregated into a compact feature representation in    conjunction with a spectrogram to form a set of features that    effectively characterize the emotion signal. The proposed solution is    evaluated on the children's emotional reading speech library and shows    more accurate predictions than existing emotion recognition algorithms.","","Feature extraction; spectral map; low-level descriptor; emotion recognition","",""
7,"","WOS:000526271800001","Puli, Akshay; Kushki, Azadeh","Toward Automatic Anxiety Detection in Autism: A Real-Time Algorithm for Detecting Physiological Arousal in the Presence of Motion","IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING",67,3,"MAR","646-657",2020,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","Objective: Anxiety is a significant clinical concern in autism spectrum    disorder (ASD) due to its negative impact on physical and psychological    health. Treatment of anxiety in ASD remains a challenge due to    difficulties with self-awareness and communication of anxiety symptoms.    To reduce these barriers to treatment, physiological markers of    autonomic arousal, collected through wearable sensors, have been    proposed as real-time, objective, and language-free measures of anxiety.    A critical limitation of the existing anxiety detection systems is that    physiological arousal is not specific to anxiety and can occur with    other user states such as physical activity. This can result in false    positives, which can hinder the operation of these systems in real-world    situations. The objective of this paper was to address this challenge by    proposing an approach for real-time detection and mitigation of physical    activity effects. Methods: A novel multiple model Kalman-like filter is    proposed to integrate heart rate and accelerometry signals. The filter    tracks user heart rate under different motion assumptions and chooses    the appropriate model for anxiety detection based on user motion    conditions. Results: Evaluation of the algorithm using data from a    sample of children with ASD shows a significant reduction in false    positives compared to the state-of-the-art, and an overall arousal    detection accuracy of 93\%. Conclusion: The proposed method is able to    reduce false detections due to user motion and effectively detect    arousal states during movement periods. Significance: The results add to    the growing evidence supporting the feasibility of wearable technologies    for anxiety detection and management in naturalistic settings.","","Heart rate; Kalman filters; Physiology; Biomedical measurement; Mathematical model; Microsoft Windows; Real-time systems; Multimodal Kalman filter; anxiety detection; autism; ASD","",""
7,"","WOS:000527294800005","Bassell, Julia; Srivastava, Siddharth; Prohl, Anna K.; Scherrer, Benoit; Kapur, Kush; Filip-Dhima, Rajna; Berry-Kravis, Elizabeth; Soorya, Latha; Thurm, Audrey; Powell, Craig M.; Bernstein, Jonathan A.; Buxbaum, Joseph D.; Kolevzon, Alexander; Warfield, Simon K.; Sahin, Mustafa; Consortium, Dev Synaptopathies","Diffusion Tensor Imaging Abnormalities in the Uncinate Fasciculus and Inferior Longitudinal Fasciculus in Phelan-McDermid Syndrome","PEDIATRIC NEUROLOGY",106,,"MAY","24-31",2020,"STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA","","","","","","","","ELSEVIER SCIENCE INC","","","","","","","Background: This cohort study utilized diffusion tensor imaging    tractography to compare the uncinate fasciculus and inferior    longitudinal fasciculus in children with Phelan-McDermid syndrome with    age-matched controls and investigated trends between autism spectrum    diagnosis and the integrity of the uncinate fasciculus and inferior    longitudinal fasciculus white matter tracts.    Methods: This research was conducted under a longitudinal study that    aims to map the genotype, phenotype, and natural history of    Phelan-McDermid syndrome and identify biomarkers using neuroimaging    (ClinicalTrial NCT02461420). Patients were aged three to 21 years and    underwent longitudinal neuropsychologic assessment over 24 months. MRI    processing and analyses were completed using previously validated image    analysis software distributed as the Computational Radiology Kit    (http://crl.med.harvard.edu/). Whole-brain connectivity was generated    for each subject using a stochastic streamline tractography algorithm,    and automatically defined regions of interest were used to map the    uncinate fasciculus and inferior longitudinal fasciculus.    Results: There were 10 participants (50\% male; mean age 11.17 years)    with Phelan-McDermid syndrome (n = 8 with autism). Age-matched controls,    enrolled in a separate longitudinal study (NIH R01 NS079788), underwent    the same neuroimaging protocol. There was a statistically significant    decrease in the uncinate fasciculus fractional anisotropy measure and a    statistically significant increase in uncinate fasciculus mean    diffusivity measure, in the patient group versus controls in both right    and left tracts (P <= 0.024).    Conclusion: Because the uncinate fasciculus plays a critical role in    social and emotional interaction, this tract may underlie some deficits    seen in the Phelan-McDermid syndrome population. These findings need to    be replicated in a larger cohort. (C) 2020 Elsevier Inc. All rights    reserved.","","22q13.3 deletion; SHANK3; DTI; Autism","",""
7,"","WOS:000527464200001","Nag, Anish; Haber, Nick; Voss, Catalin; Tamura, Serena; Daniels, Jena; Ma, Jeffrey; Chiang, Bryan; Ramachandran, Shasta; Schwartz, Jessey; Winograd, Terry; Feinstein, Carl; Wall, Dennis P.","Toward Continuous Social Phenotyping: Analyzing Gaze Patterns in an Emotion Recognition Task for Children With Autism Through Wearable Smart Glasses","JOURNAL OF MEDICAL INTERNET RESEARCH",22,4,"APR 22","",2020,"130 QUEENS QUAY E, STE 1102, TORONTO, ON M5A 0P6, CANADA","","","","","","","","JMIR PUBLICATIONS, INC","","","","","","","Background: Several studies have shown that facial attention differs in    children with autism. Measuring eye gaze and emotion recognition in    children with autism is challenging, as standard clinical assessments    must be delivered in clinical settings by a trained clinician. Wearable    technologies may be able to bring eye gaze and emotion recognition into    natural social interactions and settings.    Objective: This study aimed to test: (1) the feasibility of tracking    gaze using wearable smart glasses during a facial expression recognition    task and (2) the ability of these gaze-tracking data, together with    facial expression recognition responses, to distinguish children with    autism from neurotypical controls (NCs).    Methods: We compared the eye gaze and emotion recognition patterns of 16    children with autism spectrum disorder (ASD) and 17 children without ASD    via wearable smart glasses fitted with a custom eye tracker. Children    identified static facial expressions of images presented on a computer    screen along with nonsocial distractors while wearing Google Glass and    the eye tracker. Faces were presented in three trials, during one of    which children received feedback in the form of the correct    classification. We employed hybrid human-labeling and computer    vision-enabled methods for pupil tracking and world-gaze translation    calibration. We analyzed the impact of gaze and emotion recognition    features in a prediction task aiming to distinguish children with ASD    from NC participants.    Results: Gaze and emotion recognition patterns enabled the training of a    classifier that distinguished ASD and NC groups. However, it was unable    to significantly outperform other classifiers that used only age and    gender features, suggesting that further work is necessary to    disentangle these effects.    Conclusions: Although wearable smart glasses show promise in identifying    subtle differences in gaze tracking and emotion recognition patterns in    children with and without ASD, the present form factor and data do not    allow for these differences to be reliably exploited by machine learning    systems. Resolving these challenges will be an important step toward    continuous tracking of the ASD phenotype.","","autism spectrum disorder; translational medicine; eye tracking; wearable technologies; artificial intelligence; machine learning; precision health; digital therapy","",""
7,"","WOS:000530252300003","Bagirathan, Anandhi; Selvaraj, Jerritta; Gurusamy, Anusuya; Das, Himangshu","Recognition of positive and negative valence states in children with autism spectrum disorder (ASD) using discrete wavelet transform (DWT) analysis of electrocardiogram signals (ECG)","JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING",12,1, SI,"JAN","405-416",2021,"TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY","","","","","","","","SPRINGER HEIDELBERG","","","","","","","Children with autism spectrum disorder (ASD) are deficit in    communication, social skills, empathy, emotional responsiveness and have    significant behavioral pattern. They have difficulty in understanding    other feelings and their own emotions. This leads to the sudden    emotional outburst and aggressive behavior in these children. Parents,    caretakers and doctors find it very difficult to prevent such extreme    behaviors. Learning the positive and negative valence leads in    determining the early indications before the onset of emotional    outbursts in children with ASD. The present study measures the psycho    physiological electrocardiogram (ECG) signal from the typically    developed (TD) children and children with ASD in the age group of 5-11    years. Personalized protocol was developed for every child with ASD to    induce positive and negative valence and ECG data was collected using    wearable Shimmer ECG device. The heart rate variability (HRV) and the    QRS amplitude were derived from ECG signal using Pan-Tompkins algorithm    and eleven features were extracted using DWT (db2, db4 and db8) mother    wavelet. The significant features of ECG, HRV and QRS amplitude were    classified using the K nearest neighbor (KNN), support vector machine    (SVM) and ensemble classifier. Ensemble and KNN classifier achieved    maximum accuracy of 81\% and 76.2\% for children with ASD and Ensemble    and SVM classifiers obtained maximum accuracy of 87.4\% and 83.8\% for    TD children using HRV data.","","Autism spectrum disorder (ASD); Heart rate variability (HRV); Pan-Tompkins algorithm; K nearest neighbor (KNN)","",""
6,"978-1-7281-1985-4","WOS:000530893804122","Rahman, Jessica Sharmin; Gedeon, Tom; Caldwell, Sabrina; Jones, Richard; Hossain, Md Zakir; Zhu, Xuanying","Melodious Micro-frissons: Detecting Music Genres From Skin Response","",,,"","",2019,"345 E 47TH ST, NEW YORK, NY 10017 USA","International Joint Conference on Neural Networks (IJCNN), Budapest, HUNGARY, JUL 14-19, 2019","","2019 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)","","","IEEE International Joint Conference on Neural Networks (IJCNN)","","IEEE","","","","","","","The relationship between music and human physiological signals has been    a topic of interest among researchers for many years. Understanding this    relationship can not only lead to more enhanced music therapy methods,    but it may also help in finding a cure to mental disorders and epileptic    seizures that are triggered by certain music. In this paper, we    investigate the effects of 3 different genres of music in participants'    Electrodermal Activity (EDA). Signals were recorded from 24 participants    while they listened to 12 music stimuli. Various feature selection    methods were applied to a number of features which were extracted from    the signals. A simple neural network using Genetic Algorithm (GA)    feature selection can reach as high as 96.8\% accuracy in classifying 3    different music genres. Classification based on participants' subjective    rating of emotion reaches 98.3\% accuracy with the Statistical    Dependency (SD) / Minimal Redundancy Maximum Relevance (MRMR) feature    selection technique. This shows that human emotion has a strong    correlation with different types of music. In the future this system can    be used to distinguish music based on their positive of negative effect    on human mental health.","","Music Therapy; Physiological Signals; Electrodermal Activity; Classification","",""
6,"978-1-7281-0824-7","WOS:000534480500016","Shahin, Mostafa; Ahmed, Beena; Smith V, Daniel; Duenser, Andreas; Epps, Julien","AUTOMATIC SCREENING OF CHILDREN WITH SPEECH SOUND DISORDERS USING PARALINGUISTIC FEATURES","",,,"","",2019,"345 E 47TH ST, NEW YORK, NY 10017 USA","IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP), Pittsburgh, PA, OCT 13-16, 2019","","2019 IEEE 29TH INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING (MLSP)","","","IEEE International Workshop on Machine Learning for Signal Processing","","IEEE","","","","IEEE","","","Subjective screening of children with speech disorders is costly, time    consuming and infeasible due to the limited availability of Speech and    Language Pathologists (SLPs). Therefore, there is an increasing interest    in automatic speech analysis of children with speech disorders as it can    offer a practical alternative to human assessment. Paralinguistic    features are a set of low-level descriptors commonly used in speech    emotion recognition. However, they have not yet been examined with    childhood speech sound disorders such as, apraxia-of-speech and    phonological and articulation disorders. In this paper, we investigated    the effectiveness of paralinguistic features in discriminating between    typically developing children and those who suffer from different types    of speech sound disorders. Two types of standard paralinguistic features    were explored, the Geneva Minimalistic Acoustic Parameter Set (GeMAPS)    and its extended version, (cGcMAPS) feature sets. We applied feature    selection to find the most discriminant set of features and employed    binary classification using a support vector machine (SVM) to    discriminate between the two groups. The method was tested on a    recently-released public speech corpus collected from typically    developing children and children with various types of speech sound    disorders. The system achieved segment-level and subject-level    unweighted average recall (UAR) of around 78\% and 87\% respectively.","","speech sound disorders; speech therapy; paralinguistic features","",""
7,"","WOS:000544034800009","Kalantarian, Haik; Jedoui, Khaled; Washington, Peter; Wall, Dennis P.","A Mobile Game for Automatic Emotion-Labeling of Images","IEEE TRANSACTIONS ON GAMES",12,2,"JUN","213-218",2020,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","In this short paper, we describe challenges in the development of a    mobile charades-style game for delivery of social training to children    with autism spectrum disorder (ASD). Providing real-time feedback and    adapting game difficulty in response to the child's performance    necessitates the integration of emotion classifiers into the system. Due    to the limited performance of existing emotion recognition platforms for    children with ASD, we propose a novel technique to automatically extract    emotion-labeled frames from video acquired from game sessions, which we    hypothesize can be used to train new emotion classifiers to overcome    these limitations. Our technique, which uses probability scores from    three different classifiers and meta information from game sessions,    correctly identified 83\% of frames compared to a baseline of 51.6\%    from the best emotion classification API evaluated in this paper.","","Games; Autism; Emotion recognition; Medical treatment; Training; Databases; Pediatrics; Autism; crowdsourcing; emotion; mobile; domain adaptation; machine learning; deep learning","",""
6,"978-1-4503-6673-1","WOS:000546032800016","Rouhi, Amirreza; Spitale, Micol; Catania, Fabio; Cosentino, Giulia; Gelsomini, Mirko; Garzotto, Franca","Emotify: Emotional Game for Children with Autism Spectrum Disorder based-on Machine Learning","",,,"","31-32",2019,"1515 BROADWAY, NEW YORK, NY 10036-9998 USA","24th International Conference on Intelligent User Interfaces (IUI), Marina Del Rey, CA, MAR 17-20, 2019","","PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES: COMPANION (IUI 2019)","","","","","ASSOC COMPUTING MACHINERY","","","","Assoc Comp Machinery","","","Children with Autism Spectrum Disorder (ASD) often face the challenge of    detecting and expressing emotions. I.e., it's hard for them to recognize    happiness, sadness and anger in other people and to express their own    feelings. This difficulty produces severe impairments in communication    and social functioning. The paper proposes a spoken educational game,    exploiting Machine Learning techniques, to help children with ASD to    understand how to correctly identify and express emotions. The game    focuses on four emotional states (happiness, sadness, anger and    neutrality) and is divided in two levels with increasingly difficulty:    the first step is to learn how to recognize and express feelings and in    the second phase emotional skills by the user are examined and    evaluated. The application integrates a multilingual emotion recognizer    from the pitch of the voice.","","ASD children; machine learning game; classification; random forest classifier; children's speech; speech recognition","",""
7,"","WOS:000548230800001","Zampella, Casey J.; Bennetto, Loisa; Herrington, John D.","Computer Vision Analysis of Reduced Interpersonal Affect Coordination in Youth With Autism Spectrum Disorder","AUTISM RESEARCH",13,12,"DEC","2133-2142",2020,"111 RIVER ST, HOBOKEN 07030-5774, NJ USA","","","","","","","","WILEY","","","","","","","Atypical social-emotional reciprocity is a core feature of autism    spectrum disorder (ASD) but can be difficult to operationalize. One    measurable manifestation of reciprocity may be interpersonal    coordination, the tendency to align the form and timing of one's    behaviors (including facial affect) with others. Interpersonal affect    coordination facilitates sharing and understanding of emotional cues,    and there is evidence that it is reduced in ASD. However, most research    has not measured this process in true social contexts, due in part to a    lack of tools for measuring dynamic facial expressions over the course    of an interaction. Automated facial analysis via computer vision    provides an efficient, granular, objective method for measuring    naturally occurring facial affect and coordination. Youth with ASD and    matched typically developing youth participated in cooperative    conversations with their mothers and unfamiliar adults.    Time-synchronized videos were analyzed with an open-source computer    vision toolkit for automated facial analysis, for the presence and    intensity of facial movements associated with positive affect. Both    youth and adult conversation partners exhibited less positive affect    during conversations when the youth partner had ASD. Youth with ASD also    engaged in less affect coordination over the course of conversations.    When considered dimensionally across youth with and without ASD, affect    coordination significantly predicted scores on rating scales of    autism-related social atypicality, adaptive social skills, and empathy.    Findings suggest that affect coordination is an important interpersonal    process with implications for broader social-emotional functioning. This    preliminary study introduces a promising novel method for quantifying    moment-to-moment facial expression and emotional reciprocity during    natural interactions. Lay Summary This study introduces a novel,    automated method for measuring social-emotional reciprocity during    natural conversations, which may improve assessment of this core autism    diagnostic behavior. We used computerized methods to measure facial    affect and the degree of affect coordination between conversation    partners. Youth with autism displayed reduced affect coordination, and    reduced affect coordination predicted lower scores on measures of    broader social-emotional skills.","","affect; emotion; social-emotional reciprocity; computer vision; interpersonal coordination; facial expression; synchrony","",""
6,"978-1-5386-1311-5","WOS:000557295306110","Jiang, Ming; Francis, Sunday M.; Srishyla, Diksha; Conelea, Christine; Zhao, Qi; Jacob, Suma","Classifying Individuals with ASD Through Facial Emotion Recognition and Eye-Tracking","",,,"","6063-6068",2019,"345 E 47TH ST, NEW YORK, NY 10017 USA","41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Berlin, GERMANY, JUL 23-27, 2019","","2019 41ST ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY (EMBC)","","","IEEE Engineering in Medicine and Biology Society Conference Proceedings","","IEEE","","","","","","","Individuals with Autism Spectrum Disorder (ASD) have been shown to have    atypical scanning patterns during face and emotion perception. While    previous studies characterized ASD using eye-tracking data, this study    examined whether the use of eye movements combined with task performance    in facial emotion recognition could be helpful to identify individuals    with ASD. We tested 23 subjects with ASD and 35 controls using a Dynamic    Affect Recognition Evaluation (DARE) task that requires an individual to    recognize one of six emotions (i.e., anger, disgust, fear, happiness,    sadness, and surprise) while observing a slowly transitioning face    video. We observed differences in response time and eye movements, but    not in the recognition accuracy. Based on these observations, we    proposed a machine learning method to distinguish between individuals    with ASD and typically developing (TD) controls. The proposed method    classifies eye fixations based on a comprehensive set of features that    integrate task performance, gaze information, and face features    extracted using a deep neural network. It achieved an 86\%    classification accuracy that is comparable with the standardized    diagnostic scales, with advantages of efficiency and objectiveness.    Feature visualization and interpretations were further carried out to    reveal distinguishing features between the two subject groups and to    understand the social and attentional deficits in ASD.","","","",""
7,"","WOS:000560656800001","Song, Peng; Zheng, Wenming","Feature Selection Based Transfer Subspace Learning for Speech Emotion Recognition","IEEE TRANSACTIONS ON AFFECTIVE COMPUTING",11,3,"JUL-SEP","373-382",2020,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","Cross-corpus speech emotion recognition has recently received    considerable attention due to the widespread existence of various    emotional speech. It takes one corpus as the training data aiming to    recognize emotions of another corpus, and generally involves two basic    problems, i.e., feature matching and feature selection. Many previous    works study these two problems independently, or just focus on solving    the first problem. In this paper, we propose a novel algorithm, called    feature selection based transfer subspace learning (FSTSL), to address    these two problems. To deal with the first problem, a latent common    subspace is learnt by reducing the difference of different corpora and    preserving the important properties. Meanwhile, we adopt the l(2,1)-norm    on the projection matrix to deal with the second problem. Besides, to    guarantee the subspace to be robust and discriminative, the geometric    information of data is exploited simultaneously in the proposed FSTSL    framework. Empirical experiments on cross-corpus speech emotion    recognition tasks demonstrate that our proposed method can achieve    encouraging results in comparison with state-of-the-art algorithms.","","Feature selection; transfer learning; subspace learning; speech emotion recognition","",""
7,"","WOS:000562099400019","Aslam, Abdul Rehman; Bin Altaf, Muhammad Awais","An On-Chip Processor for Chronic Neurological Disorders Assistance Using Negative Affectivity Classification","IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS",14,4,"AUG","838-851",2020,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","IEEE Biomedical Circuits and Systems Conference (BioCAS), Nara, JAPAN, OCT 17-19, 2019","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","IEEE; IEEE Circuits \& Syst Soc; IEEE Engn Med \& Biol Soc; IEEE Solid State Circuits Soc; Tateisi Sci \& Technol Fdn; Nanolux; Nidek Co Ltd; Hisol; Keysight Technologies; Horiba Adv Techno; Maxwell Biosystems; Sysmex; Kyocera; Shimadzu; Santen; Omron; ThorLabs; Hamamatsu; Mdpi, Micromachines","","","Chronic neurological disorders (CND's) are lifelong diseases and cannot    be eradicated, but their severe effects can be alleviated by early    preemptive measures. CND's, such as Alzheimer's, Autism Spectrum    Disorder (ASD), and Amyotrophic Lateral Sclerosis (ALS), are the chronic    ailment of the central nervous system that causes the degradation of    emotional and cognitive abilities. Long term continuous monitoring with    neuro-feedback of human emotions for patients with CND's is crucial in    mitigating its harmful effect. This paper presents hardware efficient    and dedicated human emotion classification processor for CND's. Scalp    EEG is used for the emotion's classification using the valence and    arousal scales. A linear support vector machine classifier is used with    power spectral density, logarithmic interhemispheric power spectral    ratio, and the interhemispheric power spectral difference of eight EEG    channel locations suitable for a wearable non-invasive classification    system. A look-up-table based logarithmic division unit (LDU) is to    represent the division features in machine learning (ML) applications.    The implemented LDU minimizes the cost of integer division by 34\% for    ML applications. The implemented emotion's classification processor    achieved an accuracy of 72.96\% and 73.14\%, respectively, for the    valence and arousal classification on multiple publicly available    datasets. The 2 x 3mm(2) processor is fabricated using a 0.18 mu m 1P6M    CMOS process with power and energy utilization of 2.04 mW and 16 mu    J/classification, respectively, for 8-channel operation.","","Continuous health monitoring; classification processor; electroencephalogram (EEG); emotion detection; machine learning; neurological disorder; support vector machine","",""
7,"","WOS:000562697400005","Jarraya, Salma Kammoun; Masmoudi, Marwa; Hammami, Mohamed","A comparative study of Autistic Children Emotion recognition based on patio-Temporal and Deep analysis of facial expressions features during a Meltdown Crisis","MULTIMEDIA TOOLS AND APPLICATIONS",80,1,"JAN","83-125",2021,"VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS","","","","","","","","SPRINGER","","","","","","","The recognition of human emotion is a significant contribution to many    computer vision appli-cations. Despite its importance, this work is the    first one towards an automatic Autistic Children emotion recognition    system to ensure their security during meltdown crisis. The current    solutions to handle a meltdown crisis are based on a preventive    approach. Indeed, Meltdown symptoms are determined by abnormal facial    expressions related to compound emotions. To provide for this    correspondence, we experimentally evaluate, in this paper, hand-crafted    Geometric Spatio-Temporal and Deep features of realistic autistic    children facial expressions. Towards this end, we compared the Compound    Emotion Recognition (CER) performance for different combinations of    these features, and we determined the features that best distinguish a    Compound Emotion (CE) of autistic children during a meltdown crisis from    the normal state. We used ``Meltdown crisis{''}(1)dataset to conduct our    experiments on realistic Meltdown / Normal scenarios of autistic    children. In this evaluation, we show that the gathered features can    lead to very encouraging performances through the use ofRandom    Forestclassifier (91.27\%) with hand-crafted features. Moreover,    classifiers trained on deep features fromInceptionResnetV2show higher    performance (97.5\%) with supervised learning techniques.","","Autism; Meltdown crisis; Facial expressions; Compound emotions; Spatio-temporal Features; CNN","",""
6,"978-3-030-29736-7; 978-3-030-29735-0","WOS:000569373500037","de Morais, Felipe; Kautzmann, Tiago R.; Bittencourt, Ig I.; Jaques, Patricia A.","EmAP-ML: A Protocol of Emotions and Behaviors Annotation for Machine Learning Labels","",11722,,"","495-509",2019,"GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND","14th European Conference on Technology Enhanced Learning (EC-TEL), Delft Univ Technol, Leiden Delft Erasmus Ctr Educ \& Learning, Delft, NETHERLANDS, SEP 16-19, 2019","","TRANSFORMING LEARNING WITH MEANINGFUL TECHNOLOGIES, EC-TEL 2019","","","Lecture Notes in Computer Science","Scheffel, M.; Broisin, J.; PammerSchindler, V.; Ioannou, A.; Schneider, J.","SPRINGER INTERNATIONAL PUBLISHING AG","","","","Int Assoc Mobile Learning","","","The detection of students' emotions in computer-based learning    environments is a complex task. Although emotions can be detected from    sensors, a less intrusive method is to train supervised machine learning    algorithms for the emotions prediction based on the log of students'    actions on the system. For these algorithms to work as expected, they    need to be trained with a large amount of reliable ground truth labels.    Generally, labels are generated by students themselves or by coders    monitoring students, watching videos from the students, or reviewing    logs of students' actions. Younger learners (i.e., children) are unable    to label their emotions properly. Still, it is difficult for a coder to    identify students' emotions only from their face since the emotional    facial expression is generally subtle in a learning setting. This    article describes EmAP-ML (Emotions Annotation Protocol for Machine    Learning), a protocol for coders to annotate students' learning emotions    and behaviors based on video records, which contains facial expressions,    ambient audio, and computer screen. The screen and ambient audio records    allow coders to infer students' appraisal (an evaluation that elicits an    emotion) to identify emotions even when the facial expression is subtle.    This protocol was evaluated by two coders who annotated videos obtained    from 55 students while using a tutoring system, having achieved an    agreement coefficient of 0.62, measured through Cohen's Kappa    statistics.","","Annotation Protocol; Learning emotions and behaviors; Affective computing; Machine learning; Educational Data Mining","",""
7,"","WOS:000576776700017","Ghosh, Lidia; Saha, Sriparna; Konar, Amit","Bi-directional Long Short-Term Memory model to analyze psychological effects on gamers","APPLIED SOFT COMPUTING",95,,"OCT","",2020,"RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","","","","","","","","ELSEVIER","","","","","","","With the increasing popularity of android gaming applications on smart    phones, detection of emotional states of hard-core gamers become the    interest of study among psychologists. Although there exist a few    interesting research works on the impact of video games over the child    and adult group, most of them are only able to throw light on    psychological aspects associated with the said cognitive task. The    real-time detection of emotional states of the player while playing    video game is still an unexplored area of research. The present work    feels the void by proposing a novel scheme of detecting the emotional    changes of human subject from their electroencephalographic (EEG) signal    acquired during their engagement in playing video games. The problem is    formulated in the settings of pattern classification, which involves    four main steps: Data collection, pre-processing and artifact removal,    feature extraction and classification.    The novelty of the work lies in extracting the emotional content with a    high recognition rate from the acquired EEG response using a deep    learning algorithm. The primary contribution of the paper lies in    efficient usage of a novel phase-sensitive Common Spatial Pattern    algorithm for feature extraction and design of an attention-based    Bi-directional Long Short-Term Memory (Bi-LSTM) network for classifying    the emotional states of a video-game player into five classes:    happiness, sadness, surprise, anger and neutral. Moreover, the scarcity    of labeled data in EEG-based brain-computer interfacing (BCI) tasks is a    serious issue while understanding the performance capabilities of the    data-driven deep-learning models. Therefore, the present work also makes    an attempt to handle the scarcity in the dimension of the extracted    feature using a novel feature augmentation algorithm before feeding the    feature-vector to the proposed Bi-LSTM network. Experiments undertaken    yield productive and conclusive results that validate the efficacy of    the proposed framework with the accuracy rate of 88.71\%. (C) 2020    Elsevier B.V. All rights reserved.","","Emotion; Games; Deep learning; Long Short-Term Memory model; Electroencephalography","",""
7,"","WOS:000577077800002","de Belen, Ryan Anthony J.; Bednarz, Tomasz; Sowmya, Arcot; Del Favero, Dennis","Computer vision in autism spectrum disorder research: a systematic review of published studies from 2009 to 2019","TRANSLATIONAL PSYCHIATRY",10,1,"SEP 30","",2020,"CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND","","","","","","","","SPRINGERNATURE","","","","","","","The current state of computer vision methods applied to autism spectrum    disorder (ASD) research has not been well established. Increasing    evidence suggests that computer vision techniques have a strong impact    on autism research. The primary objective of this systematic review is    to examine how computer vision analysis has been useful in ASD    diagnosis, therapy and autism research in general. A systematic review    of publications indexed on PubMed, IEEE Xplore and ACM Digital Library    was conducted from 2009 to 2019. Search terms included {[}'autis{*}' AND    ('computer vision' OR `behavio{*} imaging' OR `behavio{*} analysis' OR    `affective computing')]. Results are reported according to PRISMA    statement. A total of 94 studies are included in the analysis. Eligible    papers are categorised based on the potential biological/behavioural    markers quantified in each study. Then, different computer vision    approaches that were employed in the included papers are described.    Different publicly available datasets are also reviewed in order to    rapidly familiarise researchers with datasets applicable to their field    and to accelerate both new behavioural and technological work on autism    research. Finally, future research directions are outlined. The findings    in this review suggest that computer vision analysis is useful for the    quantification of behavioural/biological markers which can further lead    to a more objective analysis in autism research.","","","",""
7,"","WOS:000579186700025","Han, Jing; Zhang, Zixing; Pantic, Maja; Schuller, Bjoern","Internet of emotional people: Towards continual affective computing cross cultures via audiovisual signals","FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE",114,,"JAN","294-306",2021,"RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","","","","","","","","ELSEVIER","","","","","","","Despite considerable advances achieved in affective computing over the    past decade, the related learning paradigms still remain in an isolated    fashion, i.e., models are often designed and developed task-dependently.    Nevertheless, with the inherently heterogeneous and dynamic property of    multiple tasks, we inevitably face several challenges, such as the    implementation feasibility, when dealing with the growing number of new    tasks of interest. For this reason, in this study, we endeavour to shed    some fresh light on shifting the conventional isolated affective    computing into a lifelong learning paradigm, namely continual affective    computing. As the first tentative work in audio and video domains, we    explore the lifelong learning algorithm of elastic weight consolidation    for this benchmark work, in an application of well-established    audiovisual emotion recognition in a cross-culture scenario, i.e.,    French and German emotion recognition. To evaluate the feasibility and    effectiveness of the introduced lifelong learning, we perform extensive    experiments across the RECOLA and SEWA databases. The empirical results    show that the implemented lifelong learning approach remarkably    outperforms other baselines in most cases, and is even competitive to    the joint training process in some cases, indicating its capability when    handling the sequential learning process with multiple tasks. (C) 2020    Elsevier B.V. All rights reserved.","","Affective computing; Continual learning; Elastic weight consolidation; Emotional intelligence","",""
6,"978-0-7354-1946-9","WOS:000598539700016","Romanova, I.; Alaverdyan, R.","Automated Robotic System For Autism Treatment","",2195,,"","",2019,"2 HUNTINGTON QUADRANGLE, STE 1NO1, MELVILLE, NY 11747-4501 USA","International Scientific and Practical Conference on Modeling in Education, Bauman Moscow State Tech Univ, Moscow, RUSSIA, JUN 19-21, 2019","","INTERNATIONAL SCIENTIFIC AND PRACTICAL CONFERENCE MODELING IN EDUCATION 2019","","","AIP Conference Proceedings","Tsvetkov, Y. B.; Romanova, I. K.","AMER INST PHYSICS","","","","","","","This article is about possible implementation of automated robotic    system for children's autism treatment. There are many well established    approaches to autism treatment, however all of them are done manually by    therapists and parents. In order to reduce manual work, we try to    partially automate ABA method -, the one which has practically proved to    be useful and scientifically well researched, - applied behaviour    analysis based treatment. Simply put ABA, among others, includes    time-consuming and laborious procedures which can be transferred into    computational tasks. That allows us to apply computational algorithms    from data mining and computer vision domain. Therefore, we propose that    automation and further data intellectual analysis can increase    productivity and effectiveness of applied method. In this work we first    look at socio-economic and social aspects of issue, analyse frequently    used methods and then based on specifics of autism and applied treatment    we suggest basic workflow for automation of ABA method and data    intellectual analysis. Workflow includes video and audio data    acquisition and preprocessing for further emotion and behaviour    recognition in correlation to external events. Similarly, suggested    system allows monitoring and possible alteration of child's performance    throughout the course of treatment. System implementation is heavily    dependent on data mining and computer vision technologies, such as    neural networks, clustering algorithms, video segmentation, feature    extraction etc. Desired performance of overall system and its units is    made possible by the function of feedback, on hardware and soft are    level.","","","",""
6,"978-1-7281-6075-7","WOS:000598571700144","Kivrak, Hasan; Uluer, Pinar; Kose, Hatice; Gumuslu, Elif; Barkana, Duygun Erol; Cakmak, Furkan; Yavuz, Sirma","Physiological Data-Based Evaluation of a Social Robot Navigation System","",,,"","994-999",2020,"345 E 47TH ST, NEW YORK, NY 10017 USA","29th IEEE International Conference on Robot and Human Interactive Communication (IEEE RO-MAN), ELECTR NETWORK, AUG 31-SEP 04, 2020","","2020 29TH IEEE INTERNATIONAL CONFERENCE ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN)","","","IEEE RO-MAN","","IEEE","","","","IEEE; IEEE Robot \& Automat Soc; Robot Soc Japan; Korean Robot Soc; Furhat Robot; EurAi; Assoc Italiana Lingusitica Computazionale; Assoc Italiana Scienze Voce; Springer; Robotics","","","The aim of this work is to create a social navigation system for an    affective robot that acts as an assistant in the audiology department of    hospitals for children with hearing impairments. Compared to traditional    navigation systems, this system differentiates between objects and human    beings and optimizes several parameters to keep at a social distance    during motion when faced with humans not to interfere with their    personal zones. For this purpose, social robot motion planning    algorithms are employed to generate human-friendly paths that maintain    humans' safety and comfort during the robot's navigation. This paper    evaluates this system compared to traditional navigation, based on the    surveys and physiological data of the adult participants in a    preliminary study before using the system with children. Although the    self-report questionnaires do not show any significant difference    between navigation profiles of the robot, analysis of the physiological    data may be interpreted that, the participants felt comfortable and less    threatened in social navigation case.","","social navigation; personal zone; HRI; emotion recognition; deeplearning; physiological data","",""
7,"","WOS:000605354700001","Li, Russell; Liu, Zhandong","Stress detection using deep neural networks","BMC MEDICAL INFORMATICS AND DECISION MAKING",20,11, SI,"DEC 30","",2020,"CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND","Annual International Conference on Intelligent Biology and Medicine (ICIBM) - Scalable Techniques and Algorithms for Computational Genomics, Univ Philadelphia, Int Assoc Intelligent Biol \& Med, ELECTR NETWORK, AUG 09-10, 2020","","","","","","","BMC","","","","Temple Univ","","","BackgroundOver 70\% of Americans regularly experience stress. Chronic    stress results in cancer, cardiovascular disease, depression, and    diabetes, and thus is deeply detrimental to physiological health and    psychological wellbeing. Developing robust methods for the rapid and    accurate detection of human stress is of paramount importance.    MethodsPrior research has shown that analyzing physiological signals is    a reliable predictor of stress. Such signals are collected from sensors    that are attached to the human body. Researchers have attempted to    detect stress by using traditional machine learning methods to analyze    physiological signals. Results, ranging between 50 and 90\% accuracy,    have been mixed. A limitation of traditional machine learning algorithms    is the requirement for hand-crafted features. Accuracy decreases if    features are misidentified. To address this deficiency, we developed two    deep neural networks: a 1-dimensional (1D) convolutional neural network    and a multilayer perceptron neural network. Deep neural networks do not    require hand-crafted features but instead extract features from raw data    through the layers of the neural networks. The deep neural networks    analyzed physiological data collected from chest-worn and wrist-worn    sensors to perform two tasks. We tailored each neural network to analyze    data from either the chest-worn (1D convolutional neural network) or    wrist-worn (multilayer perceptron neural network) sensors. The first    task was binary classification for stress detection, in which the    networks differentiated between stressed and non-stressed states. The    second task was 3-class classification for emotion classification, in    which the networks differentiated between baseline, stressed, and amused    states. The networks were trained and tested on publicly available data    collected in previous studies.ResultsThe deep convolutional neural    network achieved 99.80\% and 99.55\% accuracy rates for binary and    3-class classification, respectively. The deep multilayer perceptron    neural network achieved 99.65\% and 98.38\% accuracy rates for binary    and 3-class classification, respectively. The networks' performance    exhibited a significant improvement over past methods that analyzed    physiological signals for both binary stress detection and 3-class    emotion classification.ConclusionsWe demonstrated the potential of deep    neural networks for developing robust, continuous, and noninvasive    methods for stress detection and emotion classification, with the end    goal of improving the quality of life.","","Convolutional neural network; Emotion classification; Multilayer perceptron; Stress detection","",""
7,"","WOS:000608175000013","Camada, Marcos Y. O.; Cerqueira, Jes J. F.; Lima, Antonio M. N.","Computational model for identifying stereotyped behaviors and determining the activation level of pseudo-autistic","APPLIED SOFT COMPUTING",99,,"FEB","",2021,"RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","","","","","","","","ELSEVIER","","","","","","","Affective state recognition of an individual is based on the emotional    cues, such as the activation level. Body expression is a modal able to    convey emotions and can be used for autism diagnosis through the    presence of stereotyped behaviors (SBs). These behaviors are atypical    and repetitive movements of the body, which can be related to a low    mental health condition. The development of systems able to both    recognize SBs and inferring activation level can automatically aid some    therapeutic approaches. In this paper, a computational model of low    intrusiveness is proposed to infer activation levels from recognized    SBs, Machine Learning Algorithms (MLAs) are for identifying the SBs and    for determining the related activation levels. A metric performance is    also proposed to evaluate the performance of MLAs considering the time    for classification of the SBs, accuracy, and precision. For classifying    the SBs, the Hidden Markov Models and Multilayer Perceptron presented    the best performance than Support Vector Machine and Convolutional    Neural Network. The Adaptive Neuro-Fuzzy technique based on the Fuzzy    C-Means algorithm allowed one to determine and differentiate the    activation levels of the stereotyped behaviors considered in the present    study. The experiments were performed with non-autistic participants,    here referred to as pseudo-autistic. (C) 2020 Elsevier B.V. All rights    reserved.","","Machine learning algorithm; Performance analysis; Affective state; Stereotyped behavior; Autism","",""
7,"","WOS:000609419700007","Ghosh, Lidia; Saha, Sriparna; Konar, Amit","Decoding emotional changes of android-garners using a fused Type-2 fuzzy deep neural network","COMPUTERS IN HUMAN BEHAVIOR",116,,"MAR","",2021,"THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","","","","","","","","PERGAMON-ELSEVIER SCIENCE LTD","","","","","","","With the fastest growing popularity of gaming applications on android    phone, analyzing emotion changes of steadfast android-gamers have become    a study of utmost interest among most of the psychologists. Recently,    some android games are producing negative impacts to the gamers; even in    the worst cases the effect is becoming life-threatening too. Most of the    existing research works are based on psychological view-point of    exploring the impact (positive/negative) of playing android games for    the child and adult age-group. However, the online recognition of    emotional state changes of the android-gamers while playing video games    may be relatively unexplored. To fill this void, the present study    proposes a novel method of identifying the emotional state changes of    android-gamers by decoding their brain signals and facial images    simultaneously during playing video games. Besides above, the second    novelty of the paper lies in designing a multimodal fusion method    between brain signals and facial images for the said application. To    address this challenge, the paper proposes a fused type-2 fuzzy deep    neural network (FT2FDNN) which integrates the brain signal processing    approach by a general type-2 fuzzy reasoning algorithm with the flavor    of the image/video processing approach using a deep convolutional neural    network. FT2FDNN uses multiple modalities to extract the similar    information (here, emotional changes) simultaneously from the type-2    fuzzy and deep neural representations. The proposed fused type-2 fuzzy    deep learning paradigm demonstrates promising results in classifying the    emotional changes of gamers with high classification accuracy. Thus the    proposed work explores a new era for future researchers.","","BCI; Electroencephalography; Deep learning; Type-2 fuzzy set; Emotion recognition; Android games","",""
7,"","WOS:000618556100004","Fang, Luo; Liming, Jiang; Xuetao, Tian; Mengge, Xiao; Yanzhen, Ma; Sheng, Zhang","Shyness prediction and language style model construction of elementary school students","ACTA PSYCHOLOGICA SINICA",53,2,"FEB","155-169",2021,"16 DONGHUANGCHENGGEN NORTH ST, BEIJING 100717, PEOPLES R CHINA","","","","","","","","SCIENCE PRESS","","","","","","","The present study aimed to explore a new method of measuring shyness    based on 1306 elementary school students' online writing texts. A    supervised learning method was used to map students' labels (tagged by    their results of scale) with their text features (extracted from online    writing texts based on a psychological dictionary) to build a machine    learning model. Key feature sets for different dimensions of shyness    were built and a machine learning model was constructed based on the    selected feature to achieve automatic prediction.    The labels were obtained through ``National School Children Shyness    Scale{''} completed online by elementary students. The scale includes    three dimensions of shyness: shy behavior, shy cognition and shy    emotion. Students with Z-scores of each dimension over 1 were labeled as    shy and others were labeled as normal. Students' online writing texts    were collected from ``TeachGrid{''} (https://www.jiaokee.com/), an    online learning platform wherein students writing texts.    The dictionary applied in the present study was Textmind, a widely used    Chinese psychological dictionary developed based on Linguistic Inquiry    and Word Count (LIWC). The dictionary was compiled mainly based on the    corpus of adults. To ensure the validity of extracted features, we    modified the original dictionary by expanding the categories and    vocabulary with the real writing text of elementary students. The    revised dictionary contained 118 categories.    Features were extracted based on the revised dictionary. Chi-square    algorithm was applied to identify the features that can distinguish    between shy and normal groups to the greatest extent. Three sets of key    features confirmed a significant lexical difference between shy and    normal individuals. Among the selected features, some were shared by    multiple dimensions reflecting the universal textual expression of shy    individuals (e.g., The average number of words per sentence and the    frequency of social words of shy individuals were less than that of    normal counterparts.), and there were certain features reflected the    unique characteristics of certain dimension (Perception words predicted    shy behavior reflecting that high shy behavior individuals frequently    felt being watched).    Based on the selected features, Python 3.6.2 was used to construct the    six prediction modes: Decision Tree, Random Forest, Support Vector    Machine, Logistic Stitch Regression, K-Nearest Neighbor and Multilayer    Perceptron. Overall, random forests have achieved the best results in    the present study. The F1 score was 0.582, 0.552 and 0.545 for behavior    cognition and emotion showing the feasibility of automatically    predicting shyness characteristics of elementary school students based    on textual language. The implication of word embedding, and deep    learning models would improve the final prediction.","","shyness; online writing; psychological dictionary; text mining; language style model","",""
7,"","WOS:000620958700001","Feng, Yulong; Xiao, Wei; Wu, Teng; Zhang, Jianwei; Xiang, Jing; Guo, Hong","A New Recognition Method for the Auditory Evoked Magnetic Fields","COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE",2021,,"FEB 11","",2021,"ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND","","","","","","","","HINDAWI LTD","","","","","","","Magnetoencephalography (MEG) is a persuasive tool to study the human    brain in physiology and psychology. It can be employed to obtain the    inference of change between the external environment and the internal    psychology, which requires us to recognize different single trial    event-related magnetic fields (ERFs) originated from different    functional areas of the brain. Current recognition methods for the    single trial data are mainly used for event-related potentials (ERPs) in    the electroencephalography (EEG). Although the MEG shares the same    signal sources with the EEG, much less interference from the other brain    tissues may give the MEG an edge in recognition of the ERFs. In this    work, we propose a new recognition method for the single trial auditory    evoked magnetic fields (AEFs) through enhancing the signal. We find that    the signal strength of the single trial AEFs is concentrated in the    primary auditory cortex of the temporal lobe, which can be clearly    displayed in the 2D images. These 2D images are then recognized by an    artificial neural network (ANN) with 100\% accuracy, which realizes the    automatic recognition for the single trial AEFs. The method not only may    be combined with the source estimation algorithm to improve its accuracy    but also paves the way for the implementation of the brain-computer    interface (BCI) with the MEG.","","","",""
6,"978-1-7281-1990-8","WOS:000621592201050","Jiang, Ming; Francis, Sunday M.; Tseng, Angela; Srishyla, Diksha; DuBois, Megan; Beard, Katie; Conelea, Christine; Zhao, Qi; Jacob, Suma","Predicting Core Characteristics of ASD Through Facial Emotion Recognition and Eye Tracking in Youth","",,,"","871-875",2020,"345 E 47TH ST, NEW YORK, NY 10017 USA","42nd Annual International Conference of the IEEE-Engineering-in-Medicine-and-Biology-Society (EMBC), Montreal, CANADA, JUL 20-24, 2020","","42ND ANNUAL INTERNATIONAL CONFERENCES OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY: ENABLING INNOVATIVE TECHNOLOGIES FOR GLOBAL HEALTHCARE EMBC'20","","","IEEE Engineering in Medicine and Biology Society Conference Proceedings","","IEEE","","","","IEEE Engn Med \& Biol Soc","","","Autism Spectrum Disorder (ASD) is a heterogeneous neurodevelopmental    disorder (NDD) with a high rate of comorbidity. The implementation of    eye-tracking methodologies has informed behavioral and    neurophysiological patterns of visual processing across ASD and comorbid    NDDs. In this study, we propose a machine learning method to predict    measures of two core ASD characteristics: impaired social interactions    and communication, and restricted, repetitive, and stereotyped behaviors    and interests. Our method extracts behavioral features from task    performance and eye-tracking data collected during a facial emotion    recognition paradigm. We achieved high regression accuracy using a    Random Forest regressor trained to predict scores on the SRS-2 and RBS-R    assessments; this approach may serve as a classifier for ASD diagnosis.","","","",""
7,"","WOS:000629509400015","Xie, Wanze; Leppanen, Jukka M.; Kane-Grade, Finola E.; Nelson, Charles A.","Converging neural and behavioral evidence for a rapid, generalized response to threat-related facial expressions in 3-year-old children","NEUROIMAGE",229,,"APR 1","",2021,"525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA","","","","","","","","ACADEMIC PRESS INC ELSEVIER SCIENCE","","","","","","","Electrophysiological studies on adults suggest that humans are efficient    at detecting threat from facial information and tend to grant these    signals a priority in access to attention, awareness, and action. The    developmental origins of this bias are poorly understood, partly because    few studies have examined the emergence of a generalized neural and    behavioral response to distinct categories of threat in early childhood.    We used event-related potential (ERP) and eye-tracking measures to    examine children's early visual responses and overt attentional biases    towards multiple exemplars of angry and fearful vs. other (e.g., happy    and neutral) faces. A large group of children was assessed    longitudinally in infancy (5, 7, or 12 months) and at 3 years of age.    The final ERP dataset included 148 infants and 132 3-year-old children;    and the final eye-tracking dataset included 272 infants and 334    3-year-olds. We demonstrate that 1) neural and behavioral responses to    facial expressions converge on an enhanced response to fearful and angry    faces at 3 years of age, with no differentiation between or bias towards    one or the other of these expressions, and 2) a support vector machine    learning model using data on the early-stage neural responses to threat    reliably predicts the duration of overt attentional dwell time for    threat-related faces at 3 years. However, we found little within-subject    correlation between threat-bias attention in infancy and at 3 years of    age. These results provide unique evidence for the early development of    a rapid, unified response to two distinct categories of facial    expressions with different physical characteristics, but shared    threat-related meaning.","","","",""
6,"978-1-7281-5313-1","WOS:000631808500006","Foo, Lee Sze; Yap, Wun-She; Hum, Yan Chai; Kadim, Zulaikha; Hon, Hock Woon; Tee, Yee Kai","Real-Time Baby Crying Detection in the Noisy Everyday Environment","",,,"","26-31",2020,"345 E 47TH ST, NEW YORK, NY 10017 USA","11th IEEE Control and System Graduate Research Colloquium (ICSGRC), Shah Alam, MALAYSIA, AUG 08, 2020","","2020 11TH IEEE CONTROL AND SYSTEM GRADUATE RESEARCH COLLOQUIUM (ICSGRC)","","","","","IEEE","","","","Univ Teknologi Mara; IEEE; IEEE Malaysia Sect Control Syst Soc Chapter; Univ Teknologi Mara, Fac Elect Engn; Univ Teknologi Mara, ASPRG","","","Baby crying detection is an important component in child monitoring,    diagnostics, as well as emotion detection systems. This study proposed a    real-time baby crying detection algorithm that monitors the noisy    environment for baby crying on a second-by-second basis. The algorithm    detected baby crying through five acoustic features - average frequency,    pitch frequency, short-time energy (STE) acceleration, zero-crossing    rate (ZCR), and Mel-Frequency cepstral coefficients (MFCCs). The    thresholds for each feature in classifying an audio segment as    ``crying{''} were set by extracting and examining the distribution of    the features of noise-free crying and non-crying samples collected from    an audio database freely available on the Internet. Later, the algorithm    was tested using noisy crying and non-crying samples downloaded from    YouTube, where an accuracy of 89.20\% was obtained for the offline    testing. In order to test the robustness and performance of the designed    algorithm, online testings were also conducted using three customly    composed noisy samples containing both crying and non-crying segments.    The online accuracy obtained was 80.77\%, lower compared to the offline    testing which was mainly caused by the extra noise introduced by the    experimental settings. With more advanced equipment, it should be    possible to increase the online testing to be closer to the offline    testing accuracy, paving the way to use the designed algorithm for    reliable real-time second-by-second baby crying detection.","","baby crying; real-time; live; crying detection","",""
7,"","WOS:000637495200001","Drimalla, Hanna; Baskow, Irina; Behnia, Behnoush; Roepke, Stefan; Dziobek, Isabel","Imitation and recognition of facial emotions in autism: a computer vision approach","MOLECULAR AUTISM",12,1,"APR 6","",2021,"CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND","","","","","","","","BMC","","","","","","","Background Imitation of facial expressions plays an important role in    social functioning. However, little is known about the quality of facial    imitation in individuals with autism and its relationship with defining    difficulties in emotion recognition. Methods We investigated imitation    and recognition of facial expressions in 37 individuals with autism    spectrum conditions and 43 neurotypical controls. Using a novel    computer-based face analysis, we measured instructed imitation of facial    emotional expressions and related it to emotion recognition abilities.    Results Individuals with autism imitated facial expressions if    instructed to do so, but their imitation was both slower and less    precise than that of neurotypical individuals. In both groups, a more    precise imitation scaled positively with participants' accuracy of    emotion recognition. Limitations Given the study's focus on adults with    autism without intellectual impairment, it is unclear whether the    results generalize to children with autism or individuals with    intellectual disability. Further, the new automated facial analysis,    despite being less intrusive than electromyography, might be less    sensitive. Conclusions Group differences in emotion recognition,    imitation and their interrelationships highlight potential for treatment    of social interaction problems in individuals with autism.","","Autism; Imitation; Facial expression; Emotion recognition; Automated analysis","",""
7,"","WOS:000640391700057","Washington, Peter; Tariq, Qandeel; Leblanc, Emilie; Chrisman, Brianna; Dunlap, Kaitlyn; Kline, Aaron; Kalantarian, Haik; Penev, Yordan; Paskov, Kelley; Voss, Catalin; Stockham, Nathaniel; Varma, Maya; Husic, Arman; Kent, Jack; Haber, Nick; Winograd, Terry; Wall, Dennis P.","Crowdsourced privacy-preserved feature tagging of short home videos for machine learning ASD detection","SCIENTIFIC REPORTS",11,1,"APR 7","",2021,"HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY","","","","","","","","NATURE PORTFOLIO","","","","","","","Standard medical diagnosis of mental health conditions requires licensed    experts who are increasingly outnumbered by those at risk, limiting    reach. We test the hypothesis that a trustworthy crowd of non-experts    can efficiently annotate behavioral features needed for accurate machine    learning detection of the common childhood developmental disorder Autism    Spectrum Disorder (ASD) for children under 8 years old. We implement a    novel process for identifying and certifying a trustworthy distributed    workforce for video feature extraction, selecting a workforce of 102    workers from a pool of 1,107. Two previously validated ASD logistic    regression classifiers, evaluated against parent-reported diagnoses,    were used to assess the accuracy of the trusted crowd's ratings of    unstructured home videos. A representative balanced sample (N=50 videos)    of videos were evaluated with and without face box and pitch shift    privacy alterations, with AUROC and AUPRC scores>0.98. With both    privacy-preserving modifications, sensitivity is preserved (96.0\%)    while maintaining specificity (80.0\%) and accuracy (88.0\%) at levels    comparable to prior classification methods without alterations. We find    that machine learning classification from features extracted by a    certified nonexpert crowd achieves high performance for ASD detection    from natural home videos of the child at risk and maintains high    sensitivity when privacy-preserving mechanisms are applied. These    results suggest that privacy-safeguarded crowdsourced analysis of short    home videos can help enable rapid and mobile machine-learning detection    of developmental delays in children.","","","",""
7,"","WOS:000650601100001","Kalabikhina, Irina Evgenievna; Banin, Evgeniy Petrovich; Abduselimova, Imiliya Abduselimovna; Klimenko, German Andreevich; Kolotusha, Anton Vasilyevich","The Measurement of Demographic Temperature Using the Sentiment Analysis of Data from the Social Network VKontakte","MATHEMATICS",9,9,"MAY","",2021,"ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","","","","","","","","MDPI","","","","","","","Social networks have a huge potential for the reflection of public    opinion, values, and attitudes. In this study, the presented approach    can allow to continuously measure how cold ``the demographic    temperature{''} is based on data taken from the Russian social network    VKontakte. This is the first attempt to analyze the sentiment of    Russian-language comments on social networks to determine the    demographic temperature (ratio of positive and negative comments) in    certain socio-demographic groups of social network users. The authors    use generated data from the comments to posts from 314 pro-natalist    groups (with child-born reproductive attitudes) and eight anti-natalist    groups (with child-free reproductive attitudes) on the demographic    topic, which have 9 million of users from all over Russia. The algorithm    of the sentiment analysis for demographic tasks is presented in the    article. In particularly, it was found that comments under posts are    more suitable for analyzing the sentiment of statements than the texts    of posts. Using the available data in two types of groups since 2014, we    find an asynchronous structural shift in comments of the corpuses of    pro-natalist and anti-natalist thematic groups. Interpretations of the    evidences are offered in the discussion part of the article. An    additional result of our work is two open Russian-language datasets of    comments on social networks.","","reproductive attitudes; demographic temperature; sentiment analysis; big data; Latent Dirichlet Allocation; VKontakte","",""
7,"","WOS:000652780200001","Tang, Tong Boon; Chong, Jie Sheng; Kiguchi, Masashi; Funane, Tsukasa; Lu, Cheng-Kai","Detection of Emotional Sensitivity Using fNIRS Based Dynamic Functional Connectivity","IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING",29,,"","894-904",2021,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","In this study, we proposed an analytical framework to identify dynamic    task-based functional connectivity (FC) features as new biomarkers of    emotional sensitivity in nursing students, by using a combination of    unsupervised and supervised machine learning techniques. The dynamic FC    was measured by functional Near-Infrared Spectroscopy (fNIRS), and    computed using a sliding window correlation (SWC) analysis. A k-means    clustering technique was applied to derive four recurring connectivity    states. The states were characterized by both graph theory and    semi-metric analysis. Occurrence probability and state transition were    extracted as dynamic FC network features, and a Random Forest (RF)    classifier was implemented to detect emotional sensitivity. The proposed    method was trialled on 39 nursing students and 19 registered nurses    during decision-making, where we assumed registered nurses have    developed strategies to cope with emotional sensitivity. Emotional    stimuli were selected from International Affective Digitized Sound    System (IADS) database. Experiment results showed that registered nurses    demonstrated single dominant connectivity state of task-relevance, while    nursing students displayed in two states and had higher level of    task-irrelevant state connectivity. The results also showed that    students were more susceptive to emotional stimuli, and the derived    dynamic FC features provided a stronger discriminating power than heart    rate variability (accuracy of 81.65\% vs 71.03\%) as biomarkers of    emotional sensitivity. This work forms the first study to demonstrate    the stability of fNIRS based dynamic FC states as a biomarker. In    conclusion, the results support that the state distribution of dynamic    FC could help reveal the differentiating factors between the nursing    students and registered nurses during decision making, and it is    anticipated that the biomarkers might be used as indicators when    developing professional training related to emotional sensitivity.","","Sensitivity; Medical services; Task analysis; Feature extraction; Heart rate variability; Emotion recognition; Biomarkers; Emotional sensitivity; dynamic functional connectivity; functional Near-Infrared Spectroscopy; Random Forest; heart rate variability","",""
7,"","WOS:000656125700001","Doi, Hirokazu; Tsumura, Norimichi; Kanai, Chieko; Masui, Kenta; Mitsuhashi, Ryota; Nagasawa, Takumi","Automatic Classification of Adult Males With and Without Autism Spectrum Disorder by Non-contact Measurement of Autonomic Nervous System Activation","FRONTIERS IN PSYCHIATRY",12,,"MAY 17","",2021,"AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND","","","","","","","","FRONTIERS MEDIA SA","","","","","","","People with autism spectrum disorder (ASD) exhibit atypicality in    various domains of behavior. Previous psychophysiological studies have    revealed an atypical pattern of autonomic nervous system (ANS)    activation induced by psychosocial stimulation. Thus, it might be    feasible to develop a novel assessment tool to evaluate the risk of ASD    by measuring ANS activation in response to emotional stimulation. The    present study investigated whether people with ASD could be    automatically classified from neurotypical adults based solely on    physiological data obtained by the recently introduced non-contact    measurement of pulse wave. We video-recorded faces of adult males with    and without ASD while watching emotion-inducing video clips. Features    reflective of ANS activation were extracted from the temporal    fluctuation of facial skin coloration and entered into a    machine-learning algorithm. Though the performance was modest, the    gradient boosting classifier succeeded in classifying people with and    without ASD, which indicates that facial skin color fluctuation contains    information useful for detecting people with ASD. Taking into    consideration the fact that the current study recruited only    high-functioning adults who have relatively mild symptoms and probably    developed some compensatory strategies, ASD screening by non-contact    measurement of pulse wave could be a promising assessment tool to    evaluate ASD risk.","","ASD; autonomic nervous system; emotion; non-contact measurement; digital phenotyping; pulse wave; color","",""
6,"978-3-030-38040-3; 978-3-030-38039-7","WOS:000657957000059","Asif, Mohammad Ahmed; Al Wadhahi, Firas; Rehman, Muhammad Hassan; Al Kalban, Ismail; Achuthan, Geetha","Intelligent Educational System for Autistic Children Using Augmented Reality and Machine Learning","",46,,"","524-534",2020,"GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND","International Conference on Innovative Data Communication Technologies and Application (ICIDCA), Coimbatore, INDIA, OCT 17-18, 2019","","INNOVATIVE DATA COMMUNICATION TECHNOLOGIES AND APPLICATION","","","Lecture Notes on Data Engineering and Communications Technologies","Raj, J. S.; Bashar, A.; Ramson, S. R. J.","SPRINGER INTERNATIONAL PUBLISHING AG","","","","","","","Autism is a severe disorder affecting 1 in 160 children globally. Autism    comprises of several development disabilities such as social,    communicational and behavioural challenges. Children being diagnosed by    the autism mainly face a hard time studying curriculum in inclusive    classrooms based on their IQ level and the autism levels. Although,    different strategies and learning teaching tools are available to    support autistic children, only few systems aid them in learning    efficiently, and are not highly interactive. Thus, the proposed    Intelligent Education System primarily focuses on providing interactive    learning experience to the autistic children with IQ level >50\% and    efficient teaching assistance to their tutors using augmented reality    and machine learning in both English and Arabic The capability of the    education system to perform an action, allows the autistic child to    interact with the playable sand and gain interest. In learning stage,    once the child scribbles on the sandbox, Kinect 3D camera captures and    recognizes the drawn image. After the refinement and recognition of the    image using OpenCV and classification model, the stored set of real    world object are projected on the canvas. Besides, a webcam captures the    facial expression of the child, and emotion detection algorithm    determines the reaction of the child. Based on the child's emotion, the    current object is projected and pronounced three times to enforce better    learning. Once the instructor chooses the language and character to be    taught using the developed mobile application, the system displays it    over the sandbox and further three objects that starts with the    particular character are pronounced and projected. The system is tested    rigorously with large set of users, and the results prove the efficiency    of the system and happiness of the autistic children in better learning.","","Autism spectrum disorder; Autistic children; Education; Machine learning; Augmented reality; Image processing","",""
7,"","WOS:000659548600010","Song, Peng; Zheng, Wenming; Yu, Yanwei; Ou, Shifeng","Speech Emotion Recognition Based on Robust Discriminative Sparse Regression","IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS",13,2,"JUN","343-353",2021,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","Speech emotion recognition has recently attracted much interest due to    the widespread of multimedia data. It generally involves two basic    problems: 1) feature extraction and 2) emotion classification. Most    previous algorithms just focus on solving one of these two problems. In    this article, we aim to deal with these two problems in a joint learning    framework, and present a novel regression algorithm, namely, robust    discriminative sparse regression (RDSR). In RDSR, we propose a sparse    regression algorithm to make our model robust to outliers and noises,    and introduce a feature selection regularization constraint    simultaneously to select the most discriminative and relevant features.    In addition, to well predict the labels, we exploit the local and global    consistency over labels, and incorporate it into the proposed framework.    To solve the objective function of RDSR, we design an efficient    alternative optimization algorithm. Finally, experimental results on    several public emotion data sets verify the effectiveness and the    superiority of our proposed method.","","Feature extraction; Speech recognition; Emotion recognition; Prediction algorithms; Signal processing algorithms; Task analysis; Linear regression; Feature selection; graph Laplacian; regression analysis; semi-supervised learning; speech emotion recognition","",""
7,"","WOS:000675020500001","Banire, Bilikis; Al Thani, Dena; Qaraqe, Marwa; Mansoor, Bilal","Face-Based Attention Recognition Model for Children with Autism Spectrum Disorder","JOURNAL OF HEALTHCARE INFORMATICS RESEARCH",5,4,"DEC","420-445",2021,"CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND","","","","","","","","SPRINGERNATURE","","","","","","","Attention recognition plays a vital role in providing learning support    for children with autism spectrum disorders (ASD). The unobtrusiveness    of face-tracking techniques makes it possible to build automatic systems    to detect and classify attentional behaviors. However, constructing such    systems is a challenging task due to the complexity of attentional    behavior in ASD. This paper proposes a face-based attention recognition    model using two methods. The first is based on geometric feature    transformation using a support vector machine (SVM) classifier, and the    second is based on the transformation of time-domain spatial features to    2D spatial images using a convolutional neural network (CNN) approach.    We conducted an experimental study on different attentional tasks for 46    children (ASD n=20, typically developing children n=26) and explored the    limits of the face-based attention recognition model for participant and    task differences. Our results show that the geometric feature    transformation using an SVM classifier outperforms the CNN approach.    Also, attention detection is more generalizable within typically    developing children than within ASD groups and within low-attention    tasks than within high-attention tasks. This paper highlights the basis    for future face-based attentional recognition for real-time learning and    clinical attention interventions.","","Facial landmarks; Geometric features; Attention recognition; ASD; Machine learning","",""
7,"","WOS:000684796000015","Wu, Xingyu; Jiang, Bingbing; Yu, Kui; Chen, Huanhuan","Separation and recovery Markov boundary discovery and its application in EEG-based emotion recognition","INFORMATION SCIENCES",571,,"SEP","262-278",2021,"STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA","","","","","","","","ELSEVIER SCIENCE INC","","","","","","","In a Bayesian network (BN), the Markov boundary (MB) presents the local    causal structure around a target. Due to the interpretability and    robustness, it has been widely applied to feature selection and BN    structure learning. However, existing MB discovery algorithms might fail    to identify some true positives, leading to poor performance in    real-world applications. To tackle this issue, we introduce a    two-phase-discovery strategy to search more true positives. Based on    this strategy, we propose a more accurate and data-efficient algorithm,    separation and recovery MB discovery algorithm (SRMB). SRMB first    discovers an incomplete parent-child set and spouse set via an MB    separation process, and then retrieves the ignored true positives via an    MB recovery process, which further exploits a symmetry test to improve    accuracy in unfaithful cases. Experiments on standard BN and real-world    data sets demonstrate the effectiveness and superiority of SRMB in terms    of MB discovery, BN structure learning, and feature selection. To    demonstrate the superiority of SRMB in data with distribution shift, we    further apply SRMB to EEG-based emotion recognition tasks, where    distribution shift exists in multiple unstable sessions. We prove that    the most predictive features are from Gamma/Beta frequency bands and are    distributed at the lateral temporal area. (c) 2021 Elsevier Inc. All    rights reserved.","","Bayesian network (BN); Markov boundary (MB); Feature selection; Electroencephalography (EEG); Causality; Markov blanket","",""
7,"","WOS:000685088900001","Ni, Tongguang; Ni, Yuyao; Xue, Jing; Wang, Suhong","A Domain Adaptation Sparse Representation Classifier for Cross-Domain Electroencephalogram-Based Emotion Classification","FRONTIERS IN PSYCHOLOGY",12,,"JUL 29","",2021,"AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND","","","","","","","","FRONTIERS MEDIA SA","","","","","","","The brain-computer interface (BCI) interprets the physiological    information of the human brain in the process of consciousness activity.    It builds a direct information transmission channel between the brain    and the outside world. As the most common non-invasive BCI modality,    electroencephalogram (EEG) plays an important role in the emotion    recognition of BCI; however, due to the individual variability and    non-stationary of EEG signals, the construction of EEG-based emotion    classifiers for different subjects, different sessions, and different    devices is an important research direction. Domain adaptation utilizes    data or knowledge from more than one domain and focuses on transferring    knowledge from the source domain (SD) to the target domain (TD), in    which the EEG data may be collected from different subjects, sessions,    or devices. In this study, a new domain adaptation sparse representation    classifier (DASRC) is proposed to address the cross-domain EEG-based    emotion classification. To reduce the differences in domain    distribution, the local information preserved criterion is exploited to    project the samples from SD and TD into a shared subspace. A common    domain-invariant dictionary is learned in the projection subspace so    that an inherent connection can be built between SD and TD. In addition,    both principal component analysis (PCA) and Fisher criteria are    exploited to promote the recognition ability of the learned dictionary.    Besides, an optimization method is proposed to alternatively update the    subspace and dictionary learning. The comparison of CSFDDL shows the    feasibility and competitive performance for cross-subject and    cross-dataset EEG-based emotion classification problems.","","electroencephalogram; domain adaptation; emotion classification; cross-subject; cross-dataset","",""
7,"","WOS:000691677100041","Aue, Tatjana; Hoeppli, Marie Eve; Scharnowski, Frank; Steyrl, David","Contributions of diagnostic, cognitive, and somatovisceral information to the prediction of fear ratings in spider phobic and non-spider-fearful individuals","JOURNAL OF AFFECTIVE DISORDERS",294,,"NOV 1","296-304",2021,"RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","","","","","","","","ELSEVIER","","","","","","","Background: Physiological responding is a key characteristic of fear    responses. Yet, it is unknown whether the time-consuming measurement of    somatovisceral responses ameliorates the prediction of individual fear    re-sponses beyond the accuracy reached by the consideration of    diagnostic (e.g., phobic vs. non phobic) and cognitive (e.g., risk    estimation) factors, which can be more easily assessed. Method: We    applied a machine learning approach to data of an experiment, in which    spider phobic and non-spider fearful participants (diagnostic factor)    faced pictures of spiders. For each experimental trial, partici-pants    specified their personal risk of encountering the spider (cognitive    factor), as well as their subjective fear (outcome variable) on    quasi-continuous scales, while diverse somatovisceral responses were    registered (heart rate, electrodermal activity, respiration, facial    muscle activity). Results: The machine-learning analyses revealed that    fear ratings were predominantly predictable by the diag-nostic factor.    Yet, when allowing for learning of individual patterns in the data,    somatovisceral responses contributed additional information on the fear    ratings, yielding a prediction accuracy of 81\% explained variance.    Moreover, heart rate prior to picture onset, but not heart rate    reactivity increased predictive power. Limitations: Fear was solely    assessed by verbal reports, only 27 females were considered, and no    generalization to other anxiety disorders is possible. Conclusions:    After training the algorithm to learn about individual-specific    responding, somatovisceral patterns can be successfully exploited. Our    findings further point to the possibility that the expectancy-related    autonomic state throughout the experiment predisposes an individual to    experience specific levels of fear, with less influ-ence of the actual    visual stimulations.","","Subjective fear; Phobia; Threat; Psychophysiology; Machine learning","",""
7,"","WOS:000694490600001","Hsu, Shin-Min; Chen, Sue-Huei; Huang, Tsung-Ren","Personal Resilience Can Be Well Estimated from Heart Rate Variability and Paralinguistic Features during Human-Robot Conversations","SENSORS",21,17,"SEP","",2021,"ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","","","","","","","","MDPI","","","","","","","Mental health is as crucial as physical health, but it is    underappreciated by mainstream biomedical research and the public.    Compared to the use of AI or robots in physical healthcare, the use of    AI or robots in mental healthcare is much more limited in number and    scope. To date, psychological resilience-the ability to cope with a    crisis and quickly return to the pre-crisis state-has been identified as    an important predictor of psychological well-being but has not been    commonly considered by AI systems (e.g., smart wearable devices) or    social robots to personalize services such as emotion coaching. To    address the dearth of investigations, the present study explores the    possibility of estimating personal resilience using physiological and    speech signals measured during human-robot conversations. Specifically,    the physiological and speech signals of 32 research participants were    recorded while the participants answered a humanoid social robot's    questions about their positive and negative memories about three periods    of their lives. The results from machine learning models showed that    heart rate variability and paralinguistic features were the overall best    predictors of personal resilience. Such predictability of personal    resilience can be leveraged by AI and social robots to improve user    understanding and has great potential for various mental healthcare    applications in the future.","","automatic personality recognition; human-robot interaction; personal resilience; physiological signals; speech signals","",""
6,"978-1-4503-6708-0","WOS:000695438100048","Huffaker, Jordan S.; Kummerfeld, Jonathan K.; Lasecki, Walter S.; Ackerman, Mark S.","Crowdsourced Detection of Emotionally Manipulative Language","",,,"","",2020,"1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES","CHI Conference on Human Factors in Computing Systems (CHI), ELECTR NETWORK, APR 25-30, 2020","","PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20)","","","","","ASSOC COMPUTING MACHINERY","","","","Assoc Comp Machinery; ACM SIGCHI","","","Detecting rhetoric that manipulates readers' emotions requires    distinguishing intrinsically emotional content (IEC; e.g., a parent    losing a child) from emotionally manipulative language (EML; e.g., using    fear-inducing language to spread anti-vaccine propaganda). However, this    remains an open classification challenge for both automatic and    crowdsourcing approaches. Machine Learning approaches only work in    narrow domains where labeled training data is available, and non-expert    annotators tend to conflate IEC with EML. We introduce an approach,    anchor comparison, that leverages workers' ability to identify and    remove instances of EML in text to create a paraphrased ``anchor    text{''}, which is then used as a comparison point to classify EML in    the original content. We evaluate our approach with a dataset of    news-style text snippets and show that precision and recall can be tuned    for system builders' needs. Our contribution is a crowdsourcing approach    that enables non-expert disentanglement of social references from    content.","","Crowdsourcing; Media Manipulation; Rhetoric; Emotion","",""
7,"","WOS:000705501000003","Kalantarian, Haik; Washington, Peter; Schwartz, Jessey; Daniels, Jena; Haber, Nick; Wall, Dennis P.","Guess What? Towards Understanding Autism from Structured Video Using Facial Affect","JOURNAL OF HEALTHCARE INFORMATICS RESEARCH",3,1, SI,"MAR 15","43-66",2019,"CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND","","","","","","","","SPRINGERNATURE","","","","","","","Autism Spectrum Disorder (ASD) is a condition affecting an estimated 1    in 59 children in the United States. Due to delays in diagnosis and    imbalances in coverage, it is necessary to develop new methods of care    delivery that can appropriately empower children and caregivers by    capitalizing on mobile tools and wearable devices for use outside of    clinical settings. In this paper, we present a mobile charades-style    game, Guess What?, used for the acquisition of structured video from    children with ASD for behavioral disease research. We then apply face    tracking and emotion recognition algorithms to videos acquired through    Guess What? game play. By analyzing facial affect in response to various    prompts, we demonstrate that engagement and facial affect can be    quantified and measured using real-time image processing algorithms: an    important first-step for future therapies, at-home screenings, and    outcome measures based on home video. Our study of eight subjects    demonstrates the efficacy of this system for deriving highly emotive    structured video from children with ASD through an engaging gamified    mobile platform, while revealing the most efficacious prompts and    categories for producing diverse emotion in participants.","","Autism; Emotion; Mobile","",""
7,"","WOS:000707073100001","Khullar, Vikas; Singh, Harjit Pal; Bala, Manju","Meltdown/Tantrum Detection System for Individuals with Autism Spectrum Disorder","APPLIED ARTIFICIAL INTELLIGENCE",35,15,"DEC 15","1708-1732",2021,"530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA","","","","","","","","TAYLOR \& FRANCIS INC","","","","","","","The intensive and explosive behavioral problems associated with Autism    Spectrum Disorder (ASD) are treated as `meltdown or tantrum,' and it may    lead to hyperactivity, impulsivity, aggression, self-injury, and    irritability. The present work aims to propose and implement a    noninvasive real-time deep learning based Meltdown/Tantrum Detection    System (MTDS) for ASD individuals. The noninvasive physiological signals    (such that heart rate, skin temperature, and galvanic skin response)    were synthetically recorded with a specially designed hardware    prototype. The recorded physiological signals were transmitted to an    internet connected server where deep learning algorithms such as CNN,    LSTM, and CNN-LSTM based Meltdown/Tantrum Detection System (MTDS) were    implemented. The trained deep learning model was capable of detecting    abnormal states of meltdown or tantrum through real-time received    physiological signals. The proposed MTDS system was trained and tested    with deep learning algorithms such as CNN, LSTM and hybrid CNN-LSTM, and    it was found that hybrid CNN-LSTM was outperformed with an average    training and testing accuracy of 96\% with low MAE (0.10 for training    and 0.04 for testing). Furthermore, 86\% of the ASD caregivers favored    the proposed MTDS system.","","","",""
7,"","WOS:000708405100003","Pauli, Ruth; Kohls, Gregor; Tino, Peter; Rogers, Jack C.; Baumann, Sarah; Ackermann, Katharina; Bernhard, Anka; Martinelli, Anne; Jansen, Lucres; Oldenhof, Helena; Gonzalez-Madruga, Karen; Smaragdi, Areti; Gonzalez-Torres, Miguel Angel; Kerexeta-Lizeaga, Inaki; Boonmann, Cyril; Kersten, Linda; Bigorra, Aitana; Hervas, Amaia; Stadler, Christina; Fernandez-Rivas, Aranzazu; Popma, Arne; Konrad, Kerstin; Herpertz-Dahlmann, Beate; Fairchild, Graeme; Freitag, Christine M.; Rotshtein, Pia; De Brito, Stephane A.","Machine learning classification of conduct disorder with high versus low levels of callous-unemotional traits based on facial emotion recognition abilities","EUROPEAN CHILD \& ADOLESCENT PSYCHIATRY",,,"","",,"ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES","","","","","","","","SPRINGER","","","","","","","Conduct disorder (CD) with high levels of callous-unemotional traits    (CD/HCU) has been theoretically linked to specific difficulties with    fear and sadness recognition, in contrast to CD with low levels of    callous-unemotional traits (CD/LCU). However, experimental evidence for    this distinction is mixed, and it is unclear whether these difficulties    are a reliable marker of CD/HCU compared to CD/LCU. In a large sample (N    = 1263, 9-18 years), we combined univariate analyses and machine    learning classifiers to investigate whether CD/HCU is associated with    disproportionate difficulties with fear and sadness recognition over    other emotions, and whether such difficulties are a reliable    individual-level marker of CD/HCU. We observed similar emotion    recognition abilities in CD/HCU and CD/LCU. The CD/HCU group    underperformed relative to typically developing (TD) youths, but    difficulties were not specific to fear or sadness. Classifiers did not    distinguish between youths with CD/HCU versus CD/LCU (52\% accuracy),    although youths with CD/HCU and CD/LCU were reliably distinguished from    TD youths (64\% and 60\%, respectively). In the subset of classifiers    that performed well for youths with CD/HCU, fear and sadness were the    most relevant emotions for distinguishing them from youths with CD/LCU    and TD youths, respectively. We conclude that non-specific emotion    recognition difficulties are common in CD/HCU, but are not reliable    individual-level markers of CD/HCU versus CD/LCU. These findings    highlight that a reduced ability to recognise facial expressions of    distress should not be assumed to be a core feature of CD/HCU.","","Emotion recognition; Conduct disorder; Conduct problems; Callous-unemotional traits; Machine learning","",""
7,"","WOS:000719224600003","Uluer, Pinar; Kose, Hatice; Gumuslu, Elif; Barkana, Duygun Erol","Experience with an Affective Robot Assistant for Children with Hearing Disabilities","INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS",,,"","",,"VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS","","","","","","","","SPRINGER","","","","","","","This study presents an assistive robotic system enhanced with emotion    recognition capabilities for children with hearing disabilities. The    system is designed and developed for the audiometry tests and    rehabilitation of children in a clinical setting and includes a social    humanoid robot (Pepper), an interactive interface, gamified audiometry    tests, sensory setup and a machine/deep learning based emotion    recognition module. Three scenarios involving conventional setup, tablet    setup and setup with the robot+tablet are evaluated with 16 children    having cochlear implant or hearing aid. Several machine learning    techniques and deep learning models are used for the classification of    the three test setups and for the classification of the emotions    (pleasant, neutral, unpleasant) of children using the recorded    physiological signals by E4 wristband. The results show that the    collected signals during the tests can be separated successfully and the    positive and negative emotions of children can be better distinguished    when they interact with the robot than in the other two setups. In    addition, the children's objective and subjective evaluations as well as    their impressions about the robot and its emotional behaviors are    analyzed and discussed extensively.","","Social robots; Human-robot interaction; Machine learning; Deep learning; Emotion recognition; Physiological signals","",""
7,"","WOS:000721036500001","Dai, Zhongpeng; Shao, Junneng; Zhou, Hongliang; Chen, Zhilu; Zhang, Siqi; Wang, Huan; Jiang, Haiteng; Yao, Zhijian; Lu, Qing","Disrupted fronto-parietal network and default-mode network gamma interactions distinguishing suicidal ideation and suicide attempt in depression","PROGRESS IN NEURO-PSYCHOPHARMACOLOGY \& BIOLOGICAL PSYCHIATRY",113,,"MAR 8","",2022,"THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND","","","","","","","","PERGAMON-ELSEVIER SCIENCE LTD","","","","","","","Background: Precise suicide risk evaluation struggled in Major    depressive disorder (MDD), especially for patients with only    suicidal-ideation (SI) but without suicide attempt (SA). MDD patients    have deficits in negative emotion processing, which is associated with    the generation of SI and SA. Given the critical role of gamma    oscillations in negative emotion processing, we hypothesize that the    transition from SI to SA in MDD could be characterized by abnormal gamma    interactions. Methods: We recruited 162 participants containing 106 MDD    patients and 56 healthy controls (HCs). Participants performed facial    recognition tasks while magnetoencephalography data were recorded.    Time-frequencyrepresentation (TFR) analysis was conducted to identify    the dominant spectra differences between MDD and HCs, and then source    analysis was applied to localize the region of interests. Furthermore,    frequency-specific functional connectivity network were constructed and    a semi-supervised clustering algorithm was utilized to predict potential    suicide risk. Results: Gamma (50-70 Hz) power was found significantly    increased in MDD, mainly residing in regions from    fronto-parietal-control-network (FPN), visual-network (VN),    default-mode-network (DMN) and salience-network (SN). Based on impaired    gamma functional connectivity network between well-established SA group    and non-SI group, semi-supervised algorithm clustered patients with only    SI into two groups with different suicide risks. Moreover, Inter-network    gamma connectivity between FPN and DMN significantly negatively    correlated with suicide risk and not confounded by depression severity.    Conclusion: Inter-network gamma connectivity with FPN and DMN might be    the key neuropathological interactions underling the progression from SI    to SA. By applying semi-supervised clustering to electrophysiological    data, it is possible to predict individual suicide risk.","","Suicide attempt; Suicidal ideation; Major depression; Gamma band; Magnetoencephalography (MEG); Semi-supervised clustering","",""
7,"","WOS:000722305600004","Li, Jing; Chen, Zejin; Li, Gongfa; Ouyang, Gaoxiang; Li, Xiaoli","Automatic classification of ASD children using appearance-based features from videos","NEUROCOMPUTING",470,,"JAN 22","40-50",2022,"RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","","","","","","","","ELSEVIER","","","","","","","Early diagnosis of Autism Spectrum Disorder (ASD) plays a crucial role    in the intervention of ASD. Traditionally, an ASD child needs to be    diagnosed by a psychiatrist in the hospital, which is expensive,    time-consuming, and influenced by expertise. In this paper, we present    an objective, convenient, and effective method for classifying ASD    children from raw video sequences by integrating the appearance-based    features from facial expressions, head pose, and head trajectory. To    better extract facial expression features, we propose a novel    attention-based facial expression recognition algorithm to focus on key    face areas like eyebrows, mouth, etc. Moreover, we use accumulative    histogram to individually extract temporal and spatial information from    facial expression, head pose and head trajectory of the video sequence.    After fusing these three kinds of features, we feed them to Long    Short-Term Memory (LSTM) and achieve a classification accuracy of 96.7\%    on our self-collected ASD video dataset. (c) 2021 Published by Elsevier    B.V.","","Autism Spectrum Disorder (ASD); Appearance-base Features; Facial Expression Recognition; Head Pose; Long Short-Term Memory (LSTM)","",""
7,"","WOS:000728926000020","Aslam, Abdul Rehman; Bin Altaf, Muhammad Awais","A 10.13 mu J/Classification 2-Channel Deep Neural Network Based SoC for Negative Emotion Outburst Detection of Autistic Children","IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS",15,5,"OCT","1039-1052",2021,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","An electroencephalogram (EEG)-based non-invasive 2-channel    neuro-feedback SoC is presented to predict and report negative emotion    outbursts (NEOB) of Autistic patients. The SoC incorporates    area-and-power efficient dual-channel Analog Front-End (AFE), and a deep    neural network (DNN) emotion classification processor. The    classification processor utilizes only the two-feature vector per    channel to minimize the area and overfitting problems. The 4-layers    customized DNN classification processor is integrated on-sensor to    predict the NEOB. The AFE comprises two entirely shared EEG channels    using sampling capacitors to reduce the area by 30\%. Moreover, it    achieves an overall integrated input-referred noise, NEF, and crosstalk    of 0.55 mu V-RMS, 2.71, and -79 dB, respectively. The 16 mm(2) SoC is    implemented in 0.18 um 1P6M, CMOS process and consumes 10.13 mu    J/classification for 2 channel operation while achieving an average    accuracy of >85\% on multiple emotion databases and real-time testing.","","Feature extraction; Electroencephalography; Hardware; Classification algorithms; Deep learning; Standards; Real-time systems; Autism; classification processor; deep neural network (DNN); electroencephalogram (EEG); emotion detection; neurological disorder","",""
7,"","WOS:000736965200002","Tooley, Ursula A.; Bassett, Danielle S.; Mackey, Allyson P.","Functional brain network community structure in childhood: Unfinished territories and fuzzy boundaries","NEUROIMAGE",247,,"FEB 15","",2022,"525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA","","","","","","","","ACADEMIC PRESS INC ELSEVIER SCIENCE","","","","","","","Adult cortex is organized into distributed functional communities. Yet,    little is known about community architecture of children's brains. Here,    we uncovered the community structure of cortex in childhood using fMRI    data from 670 children aged 9-11 years (48\% female, replication sample=    544 , 56\% female) from the Adolescent Brain and Cognitive Development    study. We first applied a data-driven community detection approach to    cluster cortical regions into communities, then employed a generative    model-based approach called the weighted stochastic block model to    further probe community interactions. Children showed similar community    structure to adults, as defined by Yeo and colleagues in 2011, in    early-developing sensory and motor communities, but differences emerged    in transmodal areas. Children have more cortical territory in the limbic    community, which is involved in emotion processing, than adults. Regions    in association cortex interact more flexibly across communities,    creating uncertainty for the model-based assignment algorithm, and    perhaps reflecting cortical boundaries that are not yet solidified.    Uncertainty was highest for cingulo-opercular areas involved in flexible    deployment of cognitive control. Activation and deactivation patterns    during a working memory task showed that both the data driven approach    and a set of adult communities statistically capture functional    organization in middle childhood. Collectively, our findings suggest    that community boundaries are not solidified by middle childhood.","","Development; Community structure; Networks; Graph theory; Network neuroscience","",""
7,"","WOS:000737014300001","Duville, Mathilde Marie; Alonso-Valerdi, Luz Maria; Ibarra-Zarate, David I.","Mexican Emotional Speech Database Based on Semantic, Frequency, Familiarity, Concreteness, and Cultural Shaping of Affective Prosody","DATA",6,12,"DEC","",2021,"ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","","","","","","","","MDPI","","","","","","","In this paper, the Mexican Emotional Speech Database (MESD) that    contains single-word emotional utterances for anger, disgust, fear,    happiness, neutral and sadness with adult (male and female) and child    voices is described. To validate the emotional prosody of the uttered    words, a cubic Support Vector Machines classifier was trained on the    basis of prosodic, spectral and voice quality features for each case    study: (1) male adult, (2) female adult and (3) child. In addition,    cultural, semantic, and linguistic shaping of emotional expression was    assessed by statistical analysis. This study was registered at BioMed    Central and is part of the implementation of a published study protocol.    Mean emotional classification accuracies yielded 93.3\%, 89.4\% and    83.3\% for male, female and child utterances respectively. Statistical    analysis emphasized the shaping of emotional prosodies by semantic and    linguistic features. A cultural variation in emotional expression was    highlighted by comparing the MESD with the INTERFACE for Castilian    Spanish database. The MESD provides reliable content for linguistic    emotional prosody shaped by the Mexican cultural environment. In order    to facilitate further investigations, a corpus controlled for linguistic    features and emotional semantics, as well as one containing words    repeated across voices and emotions are provided. The MESD is made    freely available.","","affective computing; audio database; cross-cultural; machine learning; Mexican Spanish; emotional speech; paralinguistic information; discrete emotions","",""
6,"978-1-4503-8002-7","WOS:000749408300006","Chauhan, Harshit; Prasad, Anmol; Shukla, Jainendra","Engagement Analysis of ADHD Students using Visual Cues from Eye Tracker","",,,"","27-31",2020,"1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES","International Conference on Multimodal Interaction (ICMI), ELECTR NETWORK, OCT 25-29, 2020","","COMPANION PUBLICATON OF THE 2020 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (ICMI `20 COMPANION)","","","","","ASSOC COMPUTING MACHINERY","","","","Assoc Comp Machinery; ACM SIGCHI","","","In this paper, we focus on finding the correlation between visual    attention and engagement of ADHD students in one-on-one sessions with    specialized educators using visual cues and eye-tracking data. Our goal    is to investigate the extent to which observations of eye-gaze, posture,    emotion and other physiological signals can be used to model the    cognitive state of subjects and to explore the integration of multiple    sensor modalities to improve the reliability of detection of human    displays of awareness and emotion in the context of ADHD affected    children. This is a novel problem since no previous studies have aimed    to identify markers of attentiveness in the context of students affected    with ADHD. The experiment has been designed to collect data in a    controlled environment and later on can be used to generate Machine    Learning models to assist real-world educators. Additionally, we propose    a novel approach for AOI (Area of Interest) detection for eye-tracking    analysis in dynamic scenarios using existing deep learning-based    saliency prediction and fixation prediction models. We aim to use the    processed data to extract the features from a subject's eye-movement    patterns and use Machine Learning models to classify the attention    levels.","","Eye Tracking; Saliency prediction; Gaze detection; Machine Learning","",""
7,"","WOS:000753776900011","Saranya, A.; Anandan, R.","Facial Action Coding and Hybrid Deep Learning Architectures for Autism Detection","INTELLIGENT AUTOMATION AND SOFT COMPUTING",33,2,"","1167-1182",2022,"871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA","","","","","","","","TECH SCIENCE PRESS","","","","","","","Hereditary Autism Spectrum Disorder (ASD) is a neuron disorder that    affects a person's ability for communication, interaction, and also    behaviors. Diagnostics of autism are available throughout all stages of    life, from infancy through adolescence and adulthood. Facial Emotions    detection is considered to be the most parameter for the detection of    Autismdisorders among the different categories of people. Propelled with    a machine and deep learning algorithms, detection of autism disorder    using facial emotions has reached a new dimension and has even been    considered as the precautionary warning system for caregivers. Since    Facial emotions are limited to only seven expressions, detection of ASD    using facial emotions needs improvisation in terms of accurate detection    and diagnosis. In this paper, we empirically relate the facial emotions    to the ASD using the Facial Action Coding Systems (FACS) in which the    different features are extracted by the FACS systems. For feature    extraction, DEEPFACENET uses the FACS integrated Convolutional Neural    Network (FACS-CNN) and hybrid Deep Learning of LSTM (Long Short-Term    Memory) for the classification and detection of autism spectrum    disorders (ASD). The experimentation is carried out using AFFECTNET    databases and validated using Kaggle Autistic facial datasets    (KAFD-2020). The Multi-Layer Perceptron (48.67\%), Convolutional neural    networks (67.75\%), and Long ShortTerm Memory (71.56), the suggested    model showed a considerable increase in recognition rate (92\%), from    this proposed model prove its superiority in detecting autistic facial    emotions among children effectively.","","Autism spectrum disorder; facial emotions; facial action coding; systems; convolutional neural networks; LSTM","",""
7,"","WOS:000757299300029","Lauer, Luisa; Altmeyer, Kristin; Malone, Sarah; Barz, Michael; Bruenken, Roland; Sonntag, Daniel; Peschel, Markus","Investigating the Usability of a Head-Mounted Display Augmented Reality Device in Elementary School Children","SENSORS",21,19,"OCT","",2021,"ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","","","","","","","","MDPI","","","","","","","Augmenting reality via head-mounted displays (HMD-AR) is an emerging    technology in education. The interactivity provided by HMD-AR devices is    particularly promising for learning, but presents a challenge to human    activity recognition, especially with children. Recent technological    advances regarding speech and gesture recognition concerning Microsoft's    HoloLens 2 may address this prevailing issue. In a within-subjects study    with 47 elementary school children (2nd to 6th grade), we examined the    usability of the HoloLens 2 using a standardized tutorial on multimodal    interaction in AR. The overall system usability was rated ``good{''}.    However, several behavioral metrics indicated that specific interaction    modes differed in their efficiency. The results are of major importance    for the development of learning applications in HMD-AR as they partially    deviate from previous findings. In particular, the well-functioning    recognition of children's voice commands that we observed represents a    novelty. Furthermore, we found different interaction preferences in    HMD-AR among the children. We also found the use of HMD-AR to have a    positive effect on children's activity-related achievement emotions.    Overall, our findings can serve as a basis for determining general    requirements, possibilities, and limitations of the implementation of    educational HMD-AR environments in elementary school classrooms.","","head-mounted displays; augmented reality; human activity recognition; usability; elementary education","",""
6,"978-1-4503-8095-9","WOS:000759178502011","Washington, Peter; Kline, Aaron; Mutlu, Onur Cezmi; Leblanc, Emilie; Hou, Cathy; Stockham, Nate; Paskov, Kelley; Chrisman, Brianna; Wall, Dennis","Activity Recognition with Moving Cameras and Few Training Examples: Applications for Detection of Autism-Related Headbanging","",,,"","",2021,"1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES","CHI Conference on Human Factors in Computing Systems, ELECTR NETWORK, MAY 08-13, 2021","","EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21)","","","","","ASSOC COMPUTING MACHINERY","","","","ACM SIGCHI; Assoc Comp Machinery; Bloomberg; Facebook; Google; Kyocera; Microsoft; Monash Univ; Verizon Media","","","Activity recognition computer vision algorithms can be used to detect    the presence of autism-related behaviors, including what are termed    ``restricted and repetitive behaviors{''}, or stimming, by diagnostic    instruments. Examples of stimming include hand flapping, spinning, and    head banging. One of the most significant bottlenecks for implementing    such classifiers is the lack of sufficiently large training sets of    human behavior specific to pediatric developmental delays. The data that    do exist are usually recorded with a handheld camera which is itself    shaky or even moving, posing a challenge for traditional feature    representation approaches for activity detection which capture the    camera's motion as a feature. To address these issues, we first document    the advantages and limitations of current feature representation    techniques for activity recognition when applied to head banging    detection. We then propose a feature representation consisting    exclusively of head pose keypoints. We create a computer vision    classifier for detecting head banging in home videos using a    time-distributed convolutional neural network (CNN) in which a single    CNN extracts features from each frame in the input sequence, and these    extracted features are fed as input to a long short-term memory (LSTM)    network. On the binary task of predicting head banging and no head    banging within videos from the Self Stimulatory Behaviour Dataset    (SSBD), we reach a mean F1-score of 90.77\% using 3-fold cross    validation (with individual fold F1-scores of 83.3\%, 89.0\%, and    100.0\%) when ensuring that no child who appeared in the train set was    in the test set for all folds. This work documents a successful process    for training a computer vision classifier which can detect a particular    human motion pattern with few training examples and even when the camera    recording the source clip is unstable. The process of engineering useful    feature representations by visually inspecting the representations, as    described here, can be a useful practice by designers and developers of    interactive systems detecting human motion patterns for use in mobile    and ubiquitous interactive systems.","","Activity recognition; repetitive motions; motion detection; autism; machine learning","",""
6,"978-1-7281-1179-7","WOS:000760910501152","Duville, Mathilde M.; Alonso-Valerdi, Luz M.; Ibarra-Zarate, David I.","The Mexican Emotional Speech Database (MESD): elaboration and assessment based on machine learning","",,,"","1644-1647",2021,"345 E 47TH ST, NEW YORK, NY 10017 USA","43rd Annual International Conference of the IEEE-Engineering-in-Medicine-and-Biology-Society (IEEE EMBC), ELECTR NETWORK, NOV 01-05, 2021","","2021 43RD ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE \& BIOLOGY SOCIETY (EMBC)","","","IEEE Engineering in Medicine and Biology Society Conference Proceedings","","IEEE","","","","IEEE Engn Med \& Biol Soc; IEEE; Elsevier; Inst Engn \& Technol","","","The Mexican Emotional Speech Database is presented along with the    evaluation of its reliability based on machine learning analysis. The    database contains 864 voice recordings with six different prosodies:    anger, disgust, fear, happiness, neutral, and sadness. Furthermore,    three voice categories are included: female adult, male adult, and    child. The following emotion recognition was reached for each category:    89.4\% 93.9\% and 83.3\% accuracy on female, male and child voices,    respectively.","","","",""
7,"","WOS:000766268600030","Zhang, Tong; Wang, Xuehan; Xu, Xiangmin; Chen, C. L. Philip","GCB-Net: Graph Convolutional Broad Network and Its Application in Emotion Recognition","IEEE TRANSACTIONS ON AFFECTIVE COMPUTING",13,1,"JAN","379-388",2022,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","","","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","","","","","","","In recent years, emotion recognition has become a research focus in the    area of artificial intelligence. Due to its irregular structure, EEG    data can be analyzed by applying graphical based algorithms or models    much more efficiently. In this work, a Graph Convolutional Broad Network    (GCB-net) was designed for exploring the deeper-level information of    graph-structured data. It used the graph convolutional layer to extract    features of graph-structured input and stacks multiple regular    convolutional layers to extract relatively abstract features. The final    concatenation utilized the broad concept, which preserves the outputs of    all hierarchical layers, allowing the model to search features in broad    spaces. To improve the performance of the proposed GCB-net, the broad    learning system (BLS) was applied to enhance its features. For    comparison, two individual experiments were conducted to examine the    efficiency of the proposed GCB-net based on the SJTU emotion EEG dataset    (SEED) and DREAMER dataset respectively. In SEED, compared with other    state-of-art methods, the GCB-net could better promote the accuracy    (reaching 94.24 percent) on the DE feature of the all-frequency band. In    DREAMER dataset, GCB-net performed better than other models with the    same setting. Furthermore, the GCB-net reached high accuracies of 86.99,    89.32 and 89.20 percent on dimensions of Valence, Arousal and Dominance    respectively. The experimental results showed the robust classifying    ability of the GCB-net and BLS in EEG emotion recognition.","","Emotion recognition; graph convolutional neural networks; broad learning system; graph convolutional broad network","",""
7,"","WOS:000766468200002","Na, Wang; Yong, Fang","Music Recognition and Classification Algorithm considering Audio Emotion","SCIENTIFIC PROGRAMMING",2022,,"JAN 20","",2022,"ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND","","","","","","","","HINDAWI LTD","","","","","","","At present, the existing music classification and recognition algorithms    have the problem of low accuracy. Therefore, this paper proposes a music    recognition and classification algorithm considering the characteristics    of audio emotion. Firstly, the emotional features of music are extracted    from the feedforward neural network and parameterized with the mean    square deviation. Gradient descent learning algorithm is used to train    audio emotion features. The neural network models of input layer, output    layer, and hidden layer are established to realize the classification    and recognition of music emotion. Experimental results show that the    algorithm has good effect on music emotion classification. The data    stream driven by the algorithm is higher than 55 MBbs, the anti-attack    ability is 91\%, the data integrity is 83\%, the average accuracy is    85\%, and it has good effectiveness and feasibility.","","","",""
6,"978-3-030-11935-5; 978-3-030-11934-8","WOS:000772239600019","Abou el-Seoud, Samir; Ahmed, Samaa A.","IQ and EQ Enhancement for People with Mental Illness","",917,,"","197-209",2019,"GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND","21st International Conference on Interactive Collaborative Learning (ICL) / 47th IGIP International Conference on Engineering Pedagogy - Teaching and Learning in a Digital World, GREECE, SEP 25-28, 2018","","CHALLENGES OF THE DIGITAL TRANSFORMATION IN EDUCATION, ICL2018, VOL 2","","","Advances in Intelligent Systems and Computing","Auer, M. E.; Tsiatsos, T.","SPRINGER INTERNATIONAL PUBLISHING AG","","","","IGIP; Aristotle Univ Thessaloniki","","","This paper focuses on IQ and EQ enhancement for people with mental    challenges such as Autism, Asparagus syndrome and Down syndrome. People    with autism and asparagus syndrome develop a relatively high IQ but    below average EQ. On the other hand, people with Down syndrome and    intellectual disability have low IQ and relatively high EQ. The system    uses speech recognition and detection to listen and understand the user.    The application uses text to speech (TTS) in addition to an artificial    intelligent chat bot to maintain conversations with the user.    Furthermore the system uses image processing to detect users emotions    and facial expression. The application uses the methodologies used in    several medical institutes and psychiatrist. In addition to IQ and EQ    tests that are performed to detect the enhancement of the users IQ and    EQ. There are two types of Intellectual disability previously known as    mental retardation, namely: mildly mentally retarded (MIMR) and severely    mentally retarded (MOMR). Intellectual disability according to IQ is    rated by scores. An IQ of 130 and above means Very Intelligent, an IQ of    120-129 indicates intelligent, and an IQ 110-119 indicates High Average,    an IQ of 90-109 indicates Average, and an IQ of 80-89 indicates Low    Average. An IQ of 70-79 means MIMR and an IQ 69 and lower means MOMR    {[}1]. In Emotional intelligence, the average human rate is from 90 to    110. A person with an IQ below 90 is considered to have low EQ. The main    goal of this paper is to help users with below average IQ and EQ to seek    normal rates. The developed program uses machine learning, speech    recognition, image processing, and best IQ methodologies available to    perform an EQ test repeatedly until the user reaches the normal EQ    rates. That will allow users with autism to participate in their society    and have a normal life. Many parents cannot afford to take their    children to a specialist or send them to special schools. Consequently,    parents should no longer suffer from self-taking care for their disable    kids. In developing countries, children in Orphanages would not be    diagnosed for their disability nor would have support from the staff.    There are many programs and therapy institutions exist that is    specialized in helping those disable people. However, mostly there are    no computerized programs nor enough personal to support such children,    adults or even parents. The combination of methodologies and the    repetitive EQ and IQ tests ensures the effectiveness and efficiency of    the product.","","Autism; IQ improvement; EQ improvement; AI; Intellectual disabilities","",""
7,"","WOS:000777495000001","Chang, Hongli; Zong, Yuan; Zheng, Wenming; Tang, Chuangao; Zhu, Jie; Li, Xuejun","Depression Assessment Method: An EEG Emotion Recognition Framework Based on Spatiotemporal Neural Network","FRONTIERS IN PSYCHIATRY",12,,"MAR 15","",2022,"AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND","","","","","","","","FRONTIERS MEDIA SA","","","","","","","The main characteristic of depression is emotional dysfunction,    manifested by increased levels of negative emotions and decreased levels    of positive emotions. Therefore, accurate emotion recognition is an    effective way to assess depression. Among the various signals used for    emotion recognition, electroencephalogram (EEG) signal has attracted    widespread attention due to its multiple advantages, such as rich    spatiotemporal information in multi-channel EEG signals. First, we use    filtering and Euclidean alignment for data preprocessing. In the feature    extraction, we use short-time Fourier transform and Hilbert-Huang    transform to extract time-frequency features, and convolutional neural    networks to extract spatial features. Finally, bi-directional long    short-term memory explored the timing relationship. Before performing    the convolution operation, according to the unique topology of the EEG    channel, the EEG features are converted into 3D tensors. This study has    achieved good results on two emotion databases: SEED and Emotional BCI    of 2020 WORLD ROBOT COMPETITION. We applied this method to the    recognition of depression based on EEG and achieved a recognition rate    of more than 70\% under the five-fold cross-validation. In addition, the    subject-independent protocol on SEED data has achieved a    state-of-the-art recognition rate, which exceeds the existing research    methods. We propose a novel EEG emotion recognition framework for    depression detection, which provides a robust algorithm for real-time    clinical depression detection based on EEG.","","depression; emotion recognition; electroencephalogram (EEG); convolutional neural network (CNN); long-short term memory network (LSTM)","",""
7,"","WOS:000791192100011","Wang, Mingxun; Luo, Gang; Chen, Hao","Practice of Music Therapy for Autistic Children Based on Music Data Mining","MATHEMATICAL PROBLEMS IN ENGINEERING",2022,,"APR 6","",2022,"ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND","","","","","","","","HINDAWI LTD","","","","","","","For children with autism, music therapy has aroused great concern with    its novelty and better influence. Music therapy, as one of the effective    treatment methods, has an important influence on the social interaction,    behavior, and emotion of autistic children. This study attempts to    explore a form of applying highly specialized impromptu music therapy to    the personal treatment of autistic children in schools for the disabled,    as well as the design method of specific music activities. Based on    music data mining, the machine learning method is introduced to model    music emotion features, and various algorithms are compared to find a    model with higher recognition rate, and, at the same time, the antinoise    ability and generalization ability of the model are further improved.    Finally, a music emotion cognitive model with better performance is    established. The results show that the model can effectively promote the    overall development of autistic children's cognitive movement, social    communication, language communication, and cognition.","","","",""
7,"","WOS:000798019900036","Padminivalli V, S. J. R. K.; Rao, M. V. P. Chandra Sekhara","A Review on Text Mining Techniques for the Prediction of Psychological Behaviour using Social Media","INTERNATIONAL JOURNAL OF EARLY CHILDHOOD SPECIAL EDUCATION",14,3,"","2462-2467",2022,"INST FINE ARTS, ESKISEHIR, 26470, TURKEY","","","","","","","","ANADOLU UNIV","","","","","","","Text mining (TM)or text data mining mainly focuses on analyzing    different types of data that is available in various social networking    sites and also from other types. Social media reduced the gaps between    individuals. Prior to its advent, it was not only difficult to    communicate but also was time taking process. Social media service is    the fast and inexpensive means to communicate with the people. Using    text mining algorithms many applications such as emotion recognition,    sentiment analysis, users behavior analysis, stress based analysis and    psychological analysis can be developed. To improve the analysis of data    several machine learning (ML) algorithms can be adopted to overcome the    issues in text mining process. This paper discusses and analyzes various    text mining algorithms, text analysis and their performances.","","text mining; machine learning; sentiment analysis","",""
7,"","WOS:000805848000003","Yang, Jingwen; Chen, Zelin; Qiu, Guoxin; Li, Xiangyu; Li, Caixia; Yang, Kexin; Chen, Zhuanggui; Gao, Leyan; Lu, Shuo","Exploring the relationship between children's facial emotion processing characteristics and speech communication ability using deep learning on eye tracking and speech performance measures","COMPUTER SPEECH AND LANGUAGE",76,,"NOV","",2022,"24-28 OVAL RD, LONDON NW1 7DX, ENGLAND","","","","","","","","ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD","","","","","","","The ability of efficient facial emotion recognition (FER) plays a    significant role in successful human communication and is closely    associated with multiple speech communication disorders (SCD) in    children. Despite the relevance, little is known about how speech    communication abilities (SCA) and FER are correlated or of their    underlying mechanism. To address this, we monitored eye movements of 115    children while watching human faces with different emotions and designed    a machine-learning based SCD prediction model to explore the underlying    pattern of eye movements during the FER task as well as their    correlation with SCA. Strong and detailed correlations were found    between different dimensions of SCA and various eye-movement features. A    group of FER gazing patterns was found to be highly sensitive to the    possibility of children's SCD. The SCD prediction model reached an    accuracy as high as 88.9\%, which offers a possible technique to fast    screen SCD for children.","","Facial emotion recognition; Speech communication disorder; Eye tracking; Linguistic aspects; Machine-learning","",""
7,"","WOS:000816333900001","Ramirez-Melendez, Rafael; Matamoros, Elisabet; Hernandez, Davinia; Mirabel, Julia; Sanchez, Elisabet; Escude, Nuria","Music-Enhanced Emotion Identification of Facial Emotions in Autistic Spectrum Disorder Children: A Pilot EEG Study","BRAIN SCIENCES",12,6,"JUN","",2022,"ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","","","","","","","","MDPI","","","","","","","The Autistic Spectrum Disorder (ASD) is characterized by a difficulty in    expressing and interpreting others' emotions. In particular, people with    ASD have difficulties when interpreting emotions encoded in facial    expressions. In the past, music interventions have been shown to improve    autistic individuals' emotional and social skills. The present study    describes a pilot study to explore the usefulness of music as a tool for    improving autistic children's emotion recognition in facial expressions.    Twenty-five children (mean age = 8.8 y, SD = 1.24) with high-functioning    ASD and normal hearing participated in the study consisting of four    weekly sessions of 15 min each. Twenty-five participants were randomly    divided into an experimental group (N = 14) and a control group (N =    11). During each session, participants in the experimental group were    exposed to images of facial expressions for four emotions (happy, sad,    angry, and fear). Images were shown in three conditions, with the second    condition consisting of music of congruent emotion with the shown    images. Participants in the control group were shown only images in all    three conditions. For six participants in each group, EEG data were    acquired during the sessions, and instantaneous emotional responses    (arousal and valence values) were extracted from the EEG data. Inter-    and intra-session emotion identification improvement was measured in    terms of verbal response accuracy, and EEG response differences were    analyzed. A comparison of the verbal responses of the experimental group    pre- and post-intervention showed a significant (p = 0.001) average    improvement in emotion identification accuracy responses of 26\% (SD =    3.4). Furthermore, emotional responses of the experimental group at the    end of the study showed a higher correlation with the emotional stimuli    being presented, compared with their emotional responses at the    beginning of the study. No similar verbal responses improvement or    EEG-stimuli correlation was found in the control group. These results    seem to indicate that music can be used to improve both emotion    identification in facial expressions and emotion induction through    facial stimuli in children with high-functioning ASD.","","autistic spectrum disorder (ASD); emotions; affective facial expressions; music; brain activity; EEG","",""
7,"","WOS:000818943600004","Yasin, Sana; Draz, Umar; Ali, Tariq; Shahid, Kashaf; Abid, Amna; Bibi, Rukhsana; Irfan, Muhammad; Huneif, Mohammed A.; Almedhesh, Sultan A.; Alqahtani, Seham M.; Abdulwahab, Alqahtani; Alzahrani, Mohammed Jamaan; Alshehri, Dhafer Batti; Abdullah, Alshehri Ali; Rahman, Saifur","Automated Speech Recognition System to Detect Babies' Feelings through Feature Analysis","CMC-COMPUTERS MATERIALS \& CONTINUA",73,2,"","4349-4367",2022,"871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA","","","","","","","","TECH SCIENCE PRESS","","","","","","","Diagnosing a baby???s feelings poses a challenge for both doctors and    parents because babies cannot explain their feelings through expression    or speech. Understanding the emotions of babies and their associated    expressions during different sensations such as hunger, pain, etc., is a    complicated task. In infancy, all communication and feelings are    propagated through cry speech, which is a natural phenomenon. Several    clinical methods can be used to diagnose a baby???s diseases, but    nonclinical methods of diagnosing a baby???s feelings are lacking. As    such, in this study, we aimed to identify babies??? feelings and    emotions through their cry using a nonclinical method. Changes in the    cry sound can be identified using our method and used to assess the    baby???s feelings. We considered the frequency of the cries from the    energy of the sound. The feelings represented by the infant???s cry are    judged to represent certain sensations expressed by the child using the    optimal frequency of the recognition of a real-world audio sound. We    used machine learning and artificial intelligence to distinguish cry    tones in real time through feature analysis. The experimental group    consisted of 50\% each male and female babies, and we determined the    relevancy of the results against different parameters. This application    produced real-time results after recognizing a child???s cry sounds. The    novelty of our work is that we, for the first time, successfully derived    the feelings of young children through the cry-speech of the child,    showing promise for end-user applications.","","Cry-to-speak; machine learning; artificial intelligence; cry speech detection; babies","",""
7,"","WOS:000821530700005","Giacomucci, Giulia; Galdo, Giulia; Polito, Cristina; Berti, Valentina; Padiglioni, Sonia; Mazzeo, Salvatore; Chiaro, Eleonora; De Cristofaro, Maria Teresa; Bagnoli, Silvia; Nacmias, Benedetta; Sorbi, Sandro; Bessi, Valentina","Unravelling neural correlates of empathy deficits in Subjective Cognitive Decline, Mild Cognitive Impairment and Alzheimer's Disease","BEHAVIOURAL BRAIN RESEARCH",428,,"JUN 25","",2022,"RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","","","","","","","","ELSEVIER","","","","","","","Empathy is the ability to understand (cognitive empathy) and to feel    (affective empathy) what others feel. The aim of the study was to assess    empathy deficit and neuronal correlates in Subjective Cognitive Decline    (SCD), Mild Cognitive Impairment (MCI) and Alzheimer's Disease (AD)    dementia. Twenty-four SCD, 41 MCI and 46 CE patients were included.    Informer-rated Interpersonal Reactivity Index was used to explore    cognitive (Perspective Taking-PT, Fantasy-FT) and affective (Empathic    Concern-EC, Personal Distress-PD) empathy, before (T0) and after (T1)    cognitive symptoms' onset. Emotion recognition ability was tested    through Ekman-60 Faces Test. Cerebral FDG-PET SPM analysis was used to    explore neural correlates underlying empathy deficits. FT-T1 scores were    lower in AD compared to SCD (13.0 +/- 8.0 vs 19.1 +/- 4,7 p = 0.008),    PD-T1 score were higher in AD compared to MCI and to SCD (27.00 +/-    10.00 vs 25.3 +/- 5.9 vs 20.5 +/- 5.6, p = 0.001). A positive    correlation was found between PT-T1 and metabolic disfunction of right    middle gyms (MFG) in MCI and AD. In AD group, a positive correlation    between PT-T1 and insula and superior temporal gyms (STG) metabolism was    detected. A negative correlation was found between PD-T1 and superior    parietal lobule metabolism in MCI, and between PT-T1 and STG metabolism    in AD. Impairment of cognitive empathy starts at MCI stage. Increase of    PD starts from preclinical phases and seems to be to be dissociated from    cognitive decline. Loss of PT is related to a progressive involvement    starting from right MFG in prodromal stage, extending to insula and STG    in dementia. Heightened emotional contagion is probably related to    derangement of mirror neurons systems in parietal regions in prodromal    stages, and to impairment of temporal emotion inhibition system in    advanced phases. Further studies are needed to clarify if alterations in    emotional contagion might be a predictive feature of a cognitive decline    driven by AD.","","Alzheimer's Disease; Mild Cognitive Impairment; Subjective Cognitive Decline; Empathy","",""
7,"","WOS:000821578500036","Sri, D. Vijaya; Nihitha, S. Vasavi; Amulya, T. N. K.; Reddy, U. Maheshwar","RECOGNITION OF EMOTION IN TEXTUAL TWEETS USING SVM AND NAIVE BAYES ALGORITHMS","INTERNATIONAL JOURNAL OF EARLY CHILDHOOD SPECIAL EDUCATION",14,4,"","1379-1390",2022,"INST FINE ARTS, ESKISEHIR, 26470, TURKEY","","","","","","","","ANADOLU UNIV","","","","","","","we proposed a emotion recognition system where it recognize the emotions    in tweets. As emotions play a vital role in our lives. As we can see    that many people use social media where they use the platform for many    purposes, some of them tweet in a good way and some of them in a    bullying way. Emotion and opinions of different people can be carried    out on tweets to analyze public opinion on a news and social events that    are taking place in present society. In this project, by using machine    learning algorithms we have implemented emotion recognition by    classifying tweets as positive and negative. By recognizing these    positive and negative tweets we can identify people emotions where we    can reduce the forged statements. Initially we have divided our dataset    into train and test dataset, where it is used to train the model and by    comparing the train data with the test data, the model recognizes the    emotions in tweets. By using svm and naive bayes algorithms we classify    the text based on twitter into different emotions and predicted emojis    like love, fear, anger, sadness, joy. Based on the performance analysis    we predicted optimal result with 79\% and 81\% F1 score.","","LR-SGD; Emotion; Machine Learning models; TF; TF-IDF","",""
7,"","WOS:000821586800009","Wan, Guobin; Deng, Fuhao; Jiang, Zijian; Song, Sifan; Hu, Di; Chen, Lifu; Wang, Haibo; Li, Miaochun; Chen, Gong; Yan, Ting; Su, Jionglong; Zhang, Jiaming","FECTS: A Facial Emotion Cognition and Training System for Chinese Children with Autism Spectrum Disorder","COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE",2022,,"APR 27","",2022,"ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND","","","","","","","","HINDAWI LTD","","","","","","","Traditional training methods such as card teaching, assistive    technologies (e.g., augmented reality/virtual reality games and    smartphone apps), DVDs, human-computer interactions, and human-robot    interactions are widely applied in autistic rehabilitation training in    recent years. In this article, we propose a novel framework for    human-computer/robot interaction and introduce a preliminary    intervention study for improving the emotion recognition of Chinese    children with an autism spectrum disorder. The core of the framework is    the Facial Emotion Cognition and Training System (FECTS, including six    tasks to train children with ASD to match, infer, and imitate the facial    expressions of happiness, sadness, fear, and anger) based on Simon    Baron-Cohen's E-S (empathizing-systemizing) theory. Our system may be    implemented on PCs, smartphones, mobile devices such as PADs, and    robots. The training record (e.g., a tracked record of emotion    imitation) of the Chinese autistic children interacting with the device    implemented using our FECTS will be uploaded and stored in the database    of a cloud-based evaluation system. Therapists and parents can access    the analysis of the emotion learning progress of these autistic children    using the cloud-based evaluation system. Deep-learning algorithms of    facial expressions recognition and attention analysis will be deployed    in the back end (e.g., devices such as a PC, a robotic system, or a    cloud system) implementing our FECTS, which can perform real-time    tracking of the imitation quality and attention of the autistic children    during the expression imitation phase. In this preliminary clinical    study, a total of 10 Chinese autistic children aged 3-8 are recruited,    and each of them received a single 20-minute training session every day    for four consecutive days. Our preliminary results validated the    feasibility of the developed FECTS and the effectiveness of our    algorithms based on Chinese children with an autism spectrum disorder.    To verify that our FECTS can be further adapted to children from other    countries, children with different cultural/sociological/linguistic    contexts should be recruited in future studies.","","","",""
7,"","WOS:000823293300008","Washington, Peter; Kalantarian, Haik; Kent, John; Husic, Arman; Kline, Aaron; Leblanc, Emilie; Hou, Cathy; Mutlu, Onur Cezmi; Dunlap, Kaitlyn; Penev, Yordan; Varma, Maya; Stockham, Nate Tyler; Chrisman, Brianna; Paskov, Kelley; Sun, Min Woo; Jung, Jae-Yoon; Voss, Catalin; Haber, Nick; Wall, Dennis Paul","Improved Digital Therapy for Developmental Pediatrics Using Domain-Specific Artificial Intelligence: Machine Learning Study","JMIR PEDIATRICS AND PARENTING",5,2,"APR-JUN","",2022,"130 QUEENS QUAY East, Unit 1100, TORONTO, ON M5A 0P6, CANADA","","","","","","","","JMIR PUBLICATIONS, INC","","","","","","","Background: Automated emotion classification could aid those who    struggle to recognize emotions, including children with developmental    behavioral conditions such as autism. However, most computer vision    emotion recognition models are trained on adult emotion and therefore    underperform when applied to child faces.    Objective: We designed a strategy to gamify the collection and labeling    of child emotion-enriched images to boost the performance of automatic    child emotion recognition models to a level closer to what will be    needed for digital health care approaches.    Methods: We leveraged our prototype therapeutic smartphone game,    GuessWhat, which was designed in large part for children with    developmental and behavioral conditions, to gamify the secure collection    of video data of children expressing a variety of emotions prompted by    the game. Independently, we created a secure web interface to gamify the    human labeling effort, called HollywoodSquares, tailored for use by any    qualified labeler. We gathered and labeled 2155 videos, 39,968 emotion    frames, and 106,001 labels on all images. With this drastically expanded    pediatric emotion-centric database (>30 times larger than existing    public pediatric emotion data sets), we trained a convolutional neural    network (CNN) computer vision classifier of happy, sad, surprised,    fearful, angry, disgust, and neutral expressions evoked by children.    Results: The classifier achieved a 66.9\% balanced accuracy and 67.4\%    Fl-score on the entirety of the Child Affective Facial Expression (CAFE)    as well as a 79.1\% balanced accuracy and 78\% Fl-score on CAFE Subset    A, a subset containing at least 60\% human agreement on emotions labels.    This performance is at least 10\% higher than all previously developed    classifiers evaluated against CAFE, the best of which reached a 56\%    balanced accuracy even when combining ``anger{''} and ``disgust{''} into    a single class.    Conclusions: This work validates that mobile games designed for    pediatric therapies can generate high volumes of domain-relevant data    sets to train state-of-the-art classifiers to perform tasks helpful to    precision health efforts.","CAFE","computer vision; emotion recognition; affective computing; autism spectrum disorder; pediatrics; mobile health; digital therapy; convolutional neural network; machine learning; artificial intelligence","",""
7,"","WOS:000829543400008","Torres, Juan Manuel Mayor; Clarkson, Tessa; Hauschild, Kathryn M.; Luhmann, Christian C.; Lerner, Matthew D.; Riccardi, Giuseppe","Facial Emotions Are Accurately Encoded in the Neural Signal of Those With Autism Spectrum Disorder: A Deep Learning Approach","BIOLOGICAL PSYCHIATRY-COGNITIVE NEUROSCIENCE AND NEUROIMAGING",7,7,"JUL","688-695",2022,"RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS","","","","","","","","ELSEVIER","","","","","","","BACKGROUND: Individuals with autism spectrum disorder (ASD) exhibit    frequent behavioral deficits in facial emotion recognition (FER). It    remains unknown whether these deficits arise because facial emotion    information is not encoded in their neural signal or because it is    encodes but fails to translate to FER behavior (deployment). This    distinction has functional implications, including constraining when    differences in social information processing occur in ASD, and guiding    interventions (i.e., developing prosthetic FER vs. reinforcing existing    skills).METHODS: We utilized a discriminative and contemporary machine    learning approach-deep convolutional neural networks-to classify facial    emotions viewed by individuals with and without ASD (N = 88) from    concurrently recorded electroencephalography signals.RESULTS: The    convolutional neural network classified facial emotions with high    accuracy for both ASD and non-ASD groups, even though individuals with    ASD performed more poorly on the concurrent FER task. In fact,    convolutional neural network accuracy was greater in the ASD group and    was not related to behavioral performance. This pattern of results    replicated across three independent participant samples. Moreover,    feature importance analyses suggested that a late temporal window of    neural activity (1000-1500 ms) may be uniquely important in facial    emotion classification for individuals with ASD. CONCLUSIONS: Our    results reveal for the first time that facial emotion information is    encoded in the neural signal of individuals with (and without) ASD.    Thus, observed difficulties in behavioral FER associated with ASD likely    arise from difficulties in decoding or deployment of facial emotion    information within the neural signal. Interventions should focus on    capitalizing on this intact encoding rather than promoting compensation    or FER prostheses.","","","",""
7,"","WOS:000831761800001","Matveev, Yuri; Matveev, Anton; Frolova, Olga; Lyakso, Elena; Ruban, Nersisson","Automatic Speech Emotion Recognition of Younger School Age Children","MATHEMATICS",10,14,"JUL","",2022,"ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND","","","","","","","","MDPI","","","","","","","This paper introduces the extended description of a database that    contains emotional speech in the Russian language of younger school age    (8-12-year-old) children and describes the results of validation of the    database based on classical machine learning algorithms, such as Support    Vector Machine (SVM) and Multi-Layer Perceptron (MLP). The validation is    performed using standard procedures and scenarios of the validation    similar to other well-known databases of children's emotional acting    speech. Performance evaluation of automatic multiclass recognition on    four emotion classes ``Neutral (Calm)-Joy-Sadness-Anger{''} shows the    superiority of SVM performance and also MLP performance over the results    of perceptual tests. Moreover, the results of automatic recognition on    the test dataset which was used in the perceptual test are even better.    These results prove that emotions in the database can be reliably    recognized both by experts and automatically using classical machine    learning algorithms such as SVM and MLP, which can be used as baselines    for comparing emotion recognition systems based on more sophisticated    modern machine learning methods and deep neural networks. The results    also confirm that this database can be a valuable resource for    researchers studying affective reactions in speech communication during    child-computer interactions in the Russian language and can be used to    develop various edutainment, health care, etc. applications.","","speech emotion recognition; child speech; younger school age","",""
7,"","WOS:000834208300040","Reshma, Vennamaneni; Deekshitha, Chatla; Keerthi, Gangarapu; Joy, Buddila Anitha; Chakravarthy, Kanukuntla","Behavior Analysis For Mentally Affected People Through Face Emotion Detection","INTERNATIONAL JOURNAL OF EARLY CHILDHOOD SPECIAL EDUCATION",14,5,"","1418-1426",2022,"INST FINE ARTS, ESKISEHIR, 26470, TURKEY","","","","","","","","ANADOLU UNIV","","","","","","","A person's emotional, psychological, and social well-being could all be    reflected in their mental health. It determines one's thoughts,    emotions, and reactions to situations. Working successfully and reaching    one's full potential requires good mental state. psychological health is    important at every age, from childhood through adulthood. A number of    factors, including stress, social anxiety, depression, obsessive    compulsive disorder, substance abuse, and personality disorders, affect    one's mental state. It's more important than ever to recognise the signs    of a mental condition to maintain a healthy sense of balance in your    life. Additionally, machine learning algorithms and artificial    intelligence (AI) are prone to fully utilising their capabilities to    predict the onset of mental states. The Mean Opinion Score was    frequently used in real-time deployments to validate the labels that    were acquired as a result of clustering. The classifiers created using    these cluster labels may be able to predict a person's mental state.    Target demographics were established for the population, including high    school students, college students, and dealing professionals. The study    assesses how the aforementioned machine learning algorithms have    affected the target demographics and offers suggestions for additional    investigation.","","Behavior Analysis; psychological state; Surveillance tool; Machine learning algorithms; computer science","",""
7,"","WOS:000835046400004","Ren, Panhong; Nie, Mengjian","Design and Analysis of a Smart Sensor-Based Early Warning Intervention Network for School Sports Bullying among Left-Behind Children","JOURNAL OF SENSORS",2022,,"JUL 18","",2022,"ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND","","","","","","","","HINDAWI LTD","","","","","","","This paper constructs a sports bullying early warning intervention    system using smart sensors to conduct in-depth research and analysis on    early warning intervention of school sports bullying behaviors among    left-behind children. Unlike daily behavior recognition based on motion    sensors, school sports bullying actions are very random and difficult to    be described by a specific motion trajectory. For the characteristics of    violent actions and daily actions, action features in the time and    frequency domains are extracted and action categories are recognized by    BP neural networks; for complex actions, it is proposed to decompose    complex actions into basic actions to improve the recognition rate. The    algorithm of combining action features and speech features to achieve    violence recognition is proposed. For the complexity of audio data    features, this paper firstly preprocesses the audio data with    preweighting, framing, and windowing and secondly extracts the MFCC    feature parameters from the audio data and then builds a deep    convolutional neural network to design the violence emotion recognition    algorithm. The simulation results show that the algorithm effectively    improves the accuracy rate of violent action recognition to 91.25\% and    the recall rate of violent action recognition to 92.13\%. Finally, the    LDA dimensionality reduction algorithm is introduced to address the    problem of the high complexity of the algorithm due to the high number    of feature dimensions. The LDA dimensionality reduction algorithm    reduces the number of feature dimensions to 7 dimensions, which reduces    the system running time by about 52\% and improves the recognition rate    of specific complex actions by about 12.1\% while ensuring the overall    system performance.","","","",""
7,"","WOS:000838718500042","Moctezuma, Luis Alfredo; Abe, Takashi; Molinas, Marta","Two-dimensional CNN-based distinction of human emotions from EEG channels selected by multi-objective evolutionary algorithm","SCIENTIFIC REPORTS",12,1,"MAR 3","",2022,"HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY","","","","","","","","NATURE PORTFOLIO","","","","","","","In this study we explore how different levels of emotional intensity    (Arousal) and pleasantness (Valence) are reflected in    electroencephalographic (EEG) signals. We performed the experiments on    EEG data of 32 subjects from the DEAP public dataset, where the subjects    were stimulated using 60-s videos to elicitate different levels of    Arousal/Valence and then self-reported the rating from 1 to 9 using the    self-assessment Manikin (SAM). The EEG data was pre-processed and used    as input to a convolutional neural network (CNN). First, the 32 EEG    channels were used to compute the maximum accuracy level obtainable for    each subject as well as for creating a single model using data from all    the subjects. The experiment was repeated using one channel at a time,    to see if specific channels contain more information to discriminate    between low vs high arousal/valence. The results indicate than using one    channel the accuracy is lower compared to using all the 32 channels. An    optimization process for EEG channel selection is then designed with the    Non-dominated Sorting Genetic Algorithm II (NSGA-II) with the objective    to obtain optimal channel combinations with high accuracy recognition.    The genetic algorithm evaluates all possible combinations using a    chromosome representation for all the 32 channels, and the EEG data from    each chromosome in the different populations are tested iteratively    solving two unconstrained objectives; to maximize classification    accuracy and to reduce the number of required EEG channels for the    classification process. Best combinations obtained from a Pareto-front    suggests that as few as 8-10 channels can fulfill this condition and    provide the basis for a lighter design of EEG systems for emotion    recognition. In the best case, the results show accuracies of up to 1.00    for low vs high arousal using eight EEG channels, and 1.00 for low vs    high valence using only two EEG channels. These results are encouraging    for research and healthcare applications that will require automatic    emotion recognition with wearable EEG.","","","",""
7,"","WOS:000849465700001","Cai, Qian; Cui, Guo-Chong; Wang, Hai-Xian","EEG-based Emotion Recognition Using Multiple Kernel Learning","MACHINE INTELLIGENCE RESEARCH",,,"","",,"CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND","","","","","","","","SPRINGERNATURE","","","","","","","Emotion recognition based on electroencephalography (EEG) has a wide    range of applications and has great potential value, so it has received    increasing attention from academia and industry in recent years.    Meanwhile, multiple kernel learning (MKL) has also been favored by    researchers for its data-driven convenience and high accuracy. However,    there is little research on MKL in EEG-based emotion recognition.    Therefore, this paper is dedicated to exploring the application of MKL    methods in the field of EEG emotion recognition and promoting the    application of MKL methods in EEG emotion recognition. Thus, we proposed    a support vector machine (SVM) classifier based on the MKL algorithm    EasyMKL to investigate the feasibility of MKL algorithms in EEG-based    emotion recognition problems. We designed two data partition methods,    random division to verify the validity of the MKL method and sequential    division to simulate practical applications. Then, tri-categorization    experiments were performed for neutral, negative and positive emotions    based on a commonly used dataset, the Shanghai Jiao Tong University    emotional EEG dataset (SEED). The average classification accuracies for    random division and sequential division were 92.25\% and 74.37\%,    respectively, which shows better classification performance than the    traditional single kernel SVM. The final results show that the MKL    method is obviously effective, and the application of MKL in EEG emotion    recognition is worthy of further study. Through the analysis of the    experimental results, we discovered that the simple mathematical    operations of the features on the symmetrical electrodes could not    effectively integrate the spatial information of the EEG signals to    obtain better performance. It is also confirmed that higher frequency    band information is more correlated with emotional state and contributes    more to emotion recognition. In summary, this paper explores research on    MKL methods in the field of EEG emotion recognition and provides a new    way of thinking for EEG-based emotion recognition research.","","Emotion recognition; electroencephalography (EEG); multiple kernel learning; machine learning; brain computer interface","",""
6,"978-1-6654-3568-0","WOS:000853024300009","Valles, Damian; Matin, Rezwan","An Audio Processing Approach using Ensemble Learning for Speech-Emotion Recognition for Children with ASD","",,,"","55-61",2021,"345 E 47TH ST, NEW YORK, NY 10017 USA","IEEE World AI IoT Congress (AIIoT), ELECTR NETWORK, MAY 10-13, 2021","","2021 IEEE WORLD AI IOT CONGRESS (AIIOT)","","","","","IEEE","","","","Smart; IEEE Reg 1; IEEE USA; Inst Engn \& Management; Univ Engn \& Management","","","Children with Autism Spectrum Disorder (ASD) find it difficult to detect    human emotions in social interactions. A speech emotion recognition    system was developed in this work, which aims to help these children to    better identify the emotions of their communication partner. The system    was developed using machine learning and deep learning techniques.    Through the use of ensemble learning, multiple machine learning    algorithms were joined to provide a final prediction on the recorded    input utterances. The ensemble of models includes a Support Vector    Machine (SVM), a Multi-Layer Perceptron (MLP), and a Recurrent Neural    Network (RNN). All three models were trained on the Ryerson Audio-Visual    Database of Emotional Speech and Songs (RAVDESS), the Toronto Emotional    Speech Set (TESS), and the Crowd-sourced Emotional Multimodal Actors    Dataset (CREMA-D). A fourth dataset was used, which was created by    adding background noise to the clean speech files from the datasets    previously mentioned. The paper describes the audio processing of the    samples, the techniques used to include the background noise, and the    feature extraction coefficients considered for the development and    training of the models. This study presents the performance evaluation    of the individual models to each of the datasets, inclusion of the    background noises, and the combination of using all of the samples in    all three datasets. The evaluation was made to select optimal    hyperparameters configuration of the models to evaluate the performance    of the ensemble learning approach through majority voting. The overall    performance of the ensemble learning reached a peak accuracy of 663\%,    reaching a higher performance emotion classification accuracy than the    MLP model which reached 65.7\%.","","Ensemble; SVM; MLP; RNN; Autism; emotions","",""
