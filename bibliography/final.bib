
@InProceedings{WOS:000783834000109,
  author                     = {Pulido-Castro, Sergio and Palacios-Quecan, Nubia and Ballen-Cardenas, Michelle P. and Cancino-Suarez, Sandra and Rizo-Arevalo, Alejandra and Lopez Lopez, Juan M.},
  booktitle                  = {2021 IEEE URUCON},
  title                      = {Ensemble of Machine Learning Models for an Improved Facial Emotion Recognition},
  year                       = {2021},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {IEEE URUCON Conference (IEEE URUCON), Montevideo, URUGUAY, NOV 24-26, 2021},
  organization               = {IEEE; IEEE Consejo Cono Sur; IEEE Power \& Energy Soc; IEEE Comp Soc; IEEE Commun Soc; IEEE Broadcast Technol Soc; IEEE Instrumentat \& Measurement Soc; IEEE Technol \& Engn Management Soc; IEEE Engn Med \& Biol Soc; IEEE Circuits \& Syst Soc; IEEE Control Syst Soc; IEEE Educ Soc; IEEE Signal Proc Soc; IEEE Solid State Circuits Soc},
  pages                      = {512-516},
  publisher                  = {IEEE},
  status                     = {Aceito},
  abstract                   = {The creation of algorithms that predict emotional recognition is a
   subject that has been of particular interest by researchers around the
   world for the last few years, as many computer vision-based systems make
   use of this information to get an approximation of the emotional state
   of an individual. This study aims to develop a real-time emotional
   recognition algorithm based on the facial expression. Our main
   contributions are the following: This algorithm was tested in a
   computational tool designed to stimulate the imitation and recognition
   of emotions of children with Autism Spectrum Disorder based on their
   facial expressions. By designing an ensemble of machine learning models
   which separates emotions into different sets, we are able to improve the
   recognition accuracy. Additionally, the selection of relevant features
   greatly reduces the execution time of the algorithm, making it feasible
   for real-time recognition. Testing of different label combinations is
   yet to be performed in order to further improve the recognition
   accuracy.},
  affiliation                = {Pulido-Castro, S (Corresponding Author), Escuela Colombiana Ingn Julio Garavito, Biomed Engn, Bogota, Colombia. Pulido-Castro, Sergio; Palacios-Quecan, Nubia; Cancino-Suarez, Sandra; Lopez Lopez, Juan M., Escuela Colombiana Ingn Julio Garavito, Biomed Engn, Bogota, Colombia. Ballen-Cardenas, Michelle P.; Rizo-Arevalo, Alejandra, Corp Univ Minuto de Dios UNIMINUTO, Psychol Program, Bogota, Colombia.},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BS9SK},
  doi                        = {10.1109/URUCON53396.2021.9647375},
  isbn                       = {978-1-6654-2443-1},
  keywords                   = {Computer vision; Feature relevance; Emotion recognition; Facial detection; Machine learning},
  language                   = {English},
  number-of-cited-references = {12},
  orcid-numbers              = {Rizo Arevalo, Alejandra/0000-0003-0060-9011 Lopez Lopez, Juan Manuel/0000-0002-3595-3317 Palacios-Quecan, Nubia/0000-0002-7275-0877 Cancino Suarez, Sandra Liliana/0000-0002-0255-8580},
  priority                   = {prio1},
  research-areas             = {Computer Science; Engineering},
  times-cited                = {0},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000783834000109},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Computer Science, Software Engineering; Computer Science, Theory \& Methods; Engineering, Multidisciplinary},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@InProceedings{WOS:000772182600055,
  author                     = {Kumar, Akhilesh and Kumar, Awadhesh},
  booktitle                  = {ADVANCED NETWORK TECHNOLOGIES AND INTELLIGENT COMPUTING, ANTIC 2021},
  title                      = {Analysis of Machine Learning Algorithms for Facial Expression Recognition},
  year                       = {2022},
  address                    = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
  editor                     = {Woungang, I and Dhurandher, SK and Pattanaik, KK and Verma, A and Verma, P},
  note                       = {1st International Conference on Advanced Network Technologies and Intelligent Computing (ANTIC), ELECTR NETWORK, DEC 17-18, 2021},
  organization               = {Banaras Hindu Univ, Inst Sci, Dept Comp Sci},
  pages                      = {730-750},
  publisher                  = {SPRINGER INTERNATIONAL PUBLISHING AG},
  series                     = {Communications in Computer and Information Science},
  volume                     = {1534},
  status                     = {Aceito},
  abstract                   = {People can identify emotion from facial expressions easily, but it is
   more difficult to do it with a computer. It is now feasible to identify
   feelings from images because of recent advances in computational
   intelligence. Emotional responses are those mental states of thoughts
   that develop without conscious effort and are naturally associated with
   facial muscles, resulting in different facial expressions such as happy,
   sad, angry, contempt, fear, surprise etc. Emotions play an important
   role in nonverbal cues that represent a person's interior thoughts.
   Intimate robots are expanding in every domain, whether it is completing
   requirements of elderly people, addressing psychiatric patients, child
   rehabilitation, or even childcare, as the human-robot interface is
   grabbing on every day with the increased demand for automation in every
   industry. We evaluate and test machine learning algorithms on the FER
   2013 data set to recognize human emotion from facial expressions, with
   some of them achieving the highest accuracy and the others failing to
   detect emotions. Many researchers have used various machine learning
   methods to identify human emotions during the last few years. In this
   research article, we analyze eight frequently used machine learning
   techniques on the FER 2013 dataset to determine which method performs
   best at categorizing human facial expression. After analyzing the
   results, it is found that the accuracy of some of the algorithms is
   quite satisfactory, with 37\% for Logistic Regression, 33\% for
   K-neighbors classifier, 100\% for Decision Tree Classifier, 78\% for
   Random Forests, 57\% for Ada-Boost, 100\% for Gaussian NB, 33\% for LDA
   (Linear Discriminant Analysis), and 99\% for QDA (Quadratic Discriminant
   Analysis). Furthermore, the experimental results show that the Decision
   Tree and Gaussian NB Classifier can correctly identify all of the
   emotions in the FER 2013 dataset with 100\% classification accuracy,
   while Quadratic Discriminant Analysis can do so with 99\% accuracy.},
  affiliation                = {Kumar, A (Corresponding Author), Banaras Hindu Univ, Dept Comp Sci, Varanasi, Uttar Pradesh, India. Kumar, Akhilesh; Kumar, Awadhesh, Banaras Hindu Univ, Dept Comp Sci, Varanasi, Uttar Pradesh, India.},
  author-email               = {akhilesh.kumar17@bhu.ac.in},
  da                         = {2022-09-28},
  doc-delivery-number        = {BS8HG},
  doi                        = {10.1007/978-3-030-96040-7\_55},
  eissn                      = {1865-0937},
  funding-acknowledgement    = {IoE, BHU {[}6031]},
  funding-text               = {This research is supported by seed grant under IoE, BHU {[}grant no. R/Dev/D/IoE/SEED GRANT/2020-21/Scheme No. 6031].},
  isbn                       = {978-3-030-96040-7; 978-3-030-96039-1},
  issn                       = {1865-0929},
  keywords                   = {Facial expression; Emotion; Machine learning; KNN; QDA; Decision tree; Gaussian NB; Random Forest},
  keywords-plus              = {REPRESENTATION},
  language                   = {English},
  number-of-cited-references = {50},
  priority                   = {prio1},
  research-areas             = {Computer Science},
  times-cited                = {0},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000772182600055},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory \& Methods},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@InProceedings{WOS:000542980800097,
  author                     = {Rani, Pooja},
  booktitle                  = {2019 FIFTH INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP 2019)},
  title                      = {Emotion Detection of Autistic Children Using Image Processing},
  year                       = {2019},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  editor                     = {Gupta, PK and Gandotra, E and Tyagi, V and Ghrera, SP and Sehgal, VK},
  note                       = {5th International Conference on Image Information Processing (ICIIP), Waknaghat, INDIA, NOV 15-17, 2019},
  organization               = {IEEE; IEEE Jaypee Univ Informat Technol, Student Branch; IEEE Delhi Sect; Comp Soc India; Jaypee Grp; CSIR; Jaypee Univ Informat Technol, Dept CSE \& IT},
  pages                      = {532-535},
  publisher                  = {IEEE},
  status                     = {Aceito},
  abstract                   = {Facial Emotion Detection is an approach towards detecting human emotions
   through facial expressions. Autism Spectrum Disorder is an advance
   neurobehavioral disorder. Autistic people have repetitive, rude
   behavior. They are not ready to do social communication. People with
   this syndrome have problems with emotion recognition. This paper works
   on detecting the emotions of autistic children from the expression of
   their faces. This paper works on four emotions. These emotions are sad,
   happy, neutral, and angry. To detect the emotion of autistic children is
   performed with image processing and machine learning algorithms. The
   features are extracted from the faces of autistic children with local
   binary pattern. Machine learning algorithms are used for classification
   of emotions. Machine learning classifiers used in classification process
   are support vector machine and neural network.},
  affiliation                = {Rani, P (Corresponding Author), Cent Univ Punjab, Dept Comp Sci \& Technol, Bathinda, Punjab, India. Rani, Pooja, Cent Univ Punjab, Dept Comp Sci \& Technol, Bathinda, Punjab, India.},
  author-email               = {poojat320@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {BP2KT},
  isbn                       = {978-1-7281-0899-5},
  keywords                   = {Machine Learning; Support Vector Machine; Neural Network; Emotion Detection; Image Processing; Local Binary Pattern},
  language                   = {English},
  number-of-cited-references = {9},
  priority                   = {prio2},
  research-areas             = {Computer Science; Engineering; Imaging Science \& Photographic Technology},
  times-cited                = {3},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000542980800097},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {0},
  web-of-science-categories  = {Computer Science, Theory \& Methods; Engineering, Electrical \& Electronic; Imaging Science \& Photographic Technology},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000708405100003,
  author                     = {Pauli, Ruth and Kohls, Gregor and Tino, Peter and Rogers, Jack C. and Baumann, Sarah and Ackermann, Katharina and Bernhard, Anka and Martinelli, Anne and Jansen, Lucres and Oldenhof, Helena and Gonzalez-Madruga, Karen and Smaragdi, Areti and Gonzalez-Torres, Miguel Angel and Kerexeta-Lizeaga, Inaki and Boonmann, Cyril and Kersten, Linda and Bigorra, Aitana and Hervas, Amaia and Stadler, Christina and Fernandez-Rivas, Aranzazu and Popma, Arne and Konrad, Kerstin and Herpertz-Dahlmann, Beate and Fairchild, Graeme and Freitag, Christine M. and Rotshtein, Pia and De Brito, Stephane A.},
  journal                    = {EUROPEAN CHILD \& ADOLESCENT PSYCHIATRY},
  title                      = {Machine learning classification of conduct disorder with high versus low levels of callous-unemotional traits based on facial emotion recognition abilities},
  issn                       = {1018-8827},
  abstract                   = {Conduct disorder (CD) with high levels of callous-unemotional traits
   (CD/HCU) has been theoretically linked to specific difficulties with
   fear and sadness recognition, in contrast to CD with low levels of
   callous-unemotional traits (CD/LCU). However, experimental evidence for
   this distinction is mixed, and it is unclear whether these difficulties
   are a reliable marker of CD/HCU compared to CD/LCU. In a large sample (N
   = 1263, 9-18 years), we combined univariate analyses and machine
   learning classifiers to investigate whether CD/HCU is associated with
   disproportionate difficulties with fear and sadness recognition over
   other emotions, and whether such difficulties are a reliable
   individual-level marker of CD/HCU. We observed similar emotion
   recognition abilities in CD/HCU and CD/LCU. The CD/HCU group
   underperformed relative to typically developing (TD) youths, but
   difficulties were not specific to fear or sadness. Classifiers did not
   distinguish between youths with CD/HCU versus CD/LCU (52\% accuracy),
   although youths with CD/HCU and CD/LCU were reliably distinguished from
   TD youths (64\% and 60\%, respectively). In the subset of classifiers
   that performed well for youths with CD/HCU, fear and sadness were the
   most relevant emotions for distinguishing them from youths with CD/LCU
   and TD youths, respectively. We conclude that non-specific emotion
   recognition difficulties are common in CD/HCU, but are not reliable
   individual-level markers of CD/HCU versus CD/LCU. These findings
   highlight that a reduced ability to recognise facial expressions of
   distress should not be assumed to be a core feature of CD/HCU.},
  address                    = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
  affiliation                = {Pauli, R; De Brito, SA (Corresponding Author), Univ Birmingham, Ctr Human Brain Hlth, Sch Psychol, Birmingham B15 2TT, W Midlands, England. Pauli, Ruth; Rogers, Jack C.; Rotshtein, Pia; De Brito, Stephane A., Univ Birmingham, Ctr Human Brain Hlth, Sch Psychol, Birmingham B15 2TT, W Midlands, England. Kohls, Gregor; Konrad, Kerstin, Rhein Westfal TH Aachen, Dept Child \& Adolescent Psychiat Psychosomat \& Ps, Child Neuropsychol Sect, Aachen, Germany. Kohls, Gregor, TU, Dept Child \& Adolescent Psychiat, Fac Med, Dresden, Germany. Tino, Peter, Univ Birmingham, Sch Comp Sci, Birmingham, W Midlands, England. Rogers, Jack C., Univ Birmingham, Sch Psychol, Inst Mental Hlth, Birmingham, W Midlands, England. Baumann, Sarah; Herpertz-Dahlmann, Beate, Univ Hosp RWTH Aachen, Dept Child \& Adolescent Psychiat, Psychosomat \& Psychotherapy, Aachen, Germany. Ackermann, Katharina; Bernhard, Anka; Martinelli, Anne; Freitag, Christine M., Goethe Univ, Univ Hosp Frankfurt, Dept Child \& Adolescent Psychiat, Frankfurt, Germany. Ackermann, Katharina, Hamburg Univ, Fac Educ, Hamburg, Germany. Jansen, Lucres; Oldenhof, Helena; Popma, Arne, Vrije Univ Amsterdam Med Ctr, Dept Child \& Adolescent Psychiat, Amsterdam, Netherlands. Gonzalez-Madruga, Karen, Kings Coll London, Inst Psychiat Psychol \& Neurosci, Dept Child \& Adolescent Psychiat, London, England. Smaragdi, Areti, Child Dev Inst, Toronto, ON, Canada. Gonzalez-Torres, Miguel Angel; Kerexeta-Lizeaga, Inaki; Fernandez-Rivas, Aranzazu, Basurto Univ Hosp, Psychiat Serv, Bilbao, Spain. Boonmann, Cyril; Kersten, Linda; Stadler, Christina, Univ Basel, Univ Psychiat Hosp, Dept Child \& Adolescent Psychiat, Basel, Switzerland. Bigorra, Aitana; Hervas, Amaia, Univ Hosp Mutua Terrassa, Barcelona, Spain. Konrad, Kerstin, RWTH Aachen \& Res Ctr Juelich, JARA Brain Inst Mol Neurosci \& Neuroimaging 2, Julich, Germany. Fairchild, Graeme, Univ Bath, Dept Psychol, Bath, Avon, England.},
  author-email               = {r.pauli@bham.ac.uk s.a.debrito@bham.ac.uk},
  da                         = {2022-09-28},
  doc-delivery-number        = {WI5MW},
  doi                        = {10.1007/s00787-021-01893-5},
  earlyaccessdate            = {OCT 2021},
  eissn                      = {1435-165X},
  funding-acknowledgement    = {European Commission {[}602407]; Biotechnology and Biological Sciences Research Council's Midlands Integrative Biosciences Training Partnership (BBSRC MIBTP); Japanese Society for the Promotion of Science {[}JSPS -S19103]; Leverhulme Trust {[}IAF-2019-032]},
  funding-text               = {This study was conducted by the FemNAT-CD consortium (Neurobiology and Treatment of Adolescent Female Conduct Disorder: The Central Role of Emotion Processing, coordinator Christine M. Freitag). This collaborative project is funded by the European Commission under the 7th Framework Health Program, Grant Agreement no. 602407. Ruth Pauli was supported by the Biotechnology and Biological Sciences Research Council's Midlands Integrative Biosciences Training Partnership (BBSRC MIBTP). During the writing of the manuscript, Stephane A. De Brito was supported by a short-term Invitational Fellowship from the Japanese Society for the Promotion of Science (JSPS -S19103) and an International Academic Fellowship from the Leverhulme Trust (IAF-2019-032).},
  journal-iso                = {Eur. Child Adolesc. Psych.},
  keywords                   = {Emotion recognition; Conduct disorder; Conduct problems; Callous-unemotional traits; Machine learning},
  keywords-plus              = {EXPRESSION RECOGNITION; DEFICITS; CHILDREN; ADOLESCENTS; SCHIZOPHRENIA; RELIABILITY; IMPAIRMENT; VALIDITY; FEAR},
  language                   = {English},
  number-of-cited-references = {38},
  oa                         = {hybrid},
  orcid-numbers              = {Martinelli, Anne/0000-0002-7158-9778 Boonmann, Cyril/0000-0002-2862-884X Jansen, Lucres/0000-0001-8475-4050 , Jack/0000-0003-2667-8867 De Brito, Stephane/0000-0002-9082-6185 Rotshtein, Pia/0000-0002-0301-9511},
  publisher                  = {SPRINGER},
  research-areas             = {Psychology; Pediatrics; Psychiatry},
  researcherid-numbers       = {Martinelli, Anne/AAX-3135-2021 Boonmann, Cyril/V-4867-2019 Jansen, Lucres/AFM-9327-2022},
  times-cited                = {1},
  type                       = {Article; Early Access},
  unique-id                  = {WOS:000708405100003},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {6},
  web-of-science-categories  = {Psychology, Developmental; Pediatrics; Psychiatry},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000448549000002,
  author                     = {Han, Tian and Zhang, Jincheng and Zhang, Zhu and Sun, Guobing and Ye, Liang and Ferdinando, Hany and Alasaarela, Esko and Seppanen, Tapio and Yu, Xiaoyang and Yang, Shuchang},
  journal                    = {EURASIP JOURNAL ON WIRELESS COMMUNICATIONS AND NETWORKING},
  title                      = {Emotion recognition and school violence detection from children speech},
  year                       = {2018},
  issn                       = {1687-1472},
  month                      = {OCT 4},
  status                     = {Rejeitado - Escopo},
  abstract                   = {School violence is a serious problem all over the world, and violence
   detection is significant to protect juveniles. School violence can be
   detected from the biological signals of victims, and emotion recognition
   is an important way to detect violence events. In this research, a
   violence simulation experiment was designed and performed for school
   violence detection system. Emotional voice from the experiment was
   extracted and analyzed. Consecutive elimination process (CEP) algorithm
   was proposed for emotion recognition in this paper. After parameters
   optimization, SVM was chosen as the classifier and the algorithm was
   validated by Berlin database which is an emotional speech database of
   adults, and the mean accuracy for seven emotions was 79.05\%. The
   emotional speech database of children extracted in violence simulation
   was also classified by SVM classifier with proposed CEP algorithm, and
   the mean accuracy was 66.13\%. The results showed that high
   classification performance could be achieved with the CEP algorithm. The
   classification result was also compared with database of adults, and the
   results indicated that children and adults voice should be treated
   differently in speech emotion recognition researches. The accuracy of
   children database is lower than adult database; the accuracy of violence
   detection will be improved by other signals in the system.},
  address                    = {CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND},
  affiliation                = {Han, T (Corresponding Author), Harbin Univ Sci \& Technol, Dept Internet Things Engn, Harbin, Heilongjiang, Peoples R China. Han, T (Corresponding Author), Univ Oulu, Optoelect \& Measurement Tech Unit, Oulu, Finland. Han, Tian; Zhang, Jincheng; Yang, Shuchang, Harbin Univ Sci \& Technol, Dept Internet Things Engn, Harbin, Heilongjiang, Peoples R China. Han, Tian; Zhang, Zhu; Sun, Guobing; Ye, Liang; Ferdinando, Hany; Alasaarela, Esko, Univ Oulu, Optoelect \& Measurement Tech Unit, Oulu, Finland. Zhang, Zhu, Harbin Univ Sci \& Technol, Dept Commun Engn, Harbin, Heilongjiang, Peoples R China. Yu, Xiaoyang, Harbin Univ Sci \& Technol, Dept Measurement Control Technol \& Instrumentat, Harbin, Heilongjiang, Peoples R China. Seppanen, Tapio, Univ Oulu, Physiol Signal Anal Team, Oulu, Finland. Sun, Guobing, Heilongjiang Univ, Dept Automat, Harbin, Heilongjiang, Peoples R China. Ferdinando, Hany, Petra Christian Univ, Dept Elect Engn, Surabaya, Indonesia.},
  article-number             = {235},
  author-email               = {hantian@hrbust.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {GY4QP},
  doi                        = {10.1186/s13638-018-1253-8},
  eissn                      = {1687-1499},
  funding-acknowledgement    = {Heilongjiang Provincial Science and Technology Department of China; Heilongjiang Provincial Education Department of China; Harbin Municipal Science and Technology Bureau of China},
  funding-text               = {The research presented in this paper was supported by the Heilongjiang Provincial Science and Technology Department of China, Heilongjiang Provincial Education Department of China, and Harbin Municipal Science and Technology Bureau of China.},
  journal-iso                = {EURASIP J. Wirel. Commun. Netw.},
  keywords                   = {Emotion recognition; Children speech; Violence simulation},
  language                   = {English},
  number-of-cited-references = {15},
  oa                         = {Green Published, gold},
  publisher                  = {SPRINGEROPEN},
  research-areas             = {Engineering; Telecommunications},
  times-cited                = {4},
  type                       = {Article},
  unique-id                  = {WOS:000448549000002},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {13},
  web-of-science-categories  = {Engineering, Electrical \& Electronic; Telecommunications},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000637495200001,
  author                     = {Drimalla, Hanna and Baskow, Irina and Behnia, Behnoush and Roepke, Stefan and Dziobek, Isabel},
  journal                    = {MOLECULAR AUTISM},
  title                      = {Imitation and recognition of facial emotions in autism: a computer vision approach},
  year                       = {2021},
  issn                       = {2040-2392},
  month                      = {APR 6},
  number                     = {1},
  volume                     = {12},
  status                     = {Aceito},
  abstract                   = {Background Imitation of facial expressions plays an important role in
   social functioning. However, little is known about the quality of facial
   imitation in individuals with autism and its relationship with defining
   difficulties in emotion recognition. Methods We investigated imitation
   and recognition of facial expressions in 37 individuals with autism
   spectrum conditions and 43 neurotypical controls. Using a novel
   computer-based face analysis, we measured instructed imitation of facial
   emotional expressions and related it to emotion recognition abilities.
   Results Individuals with autism imitated facial expressions if
   instructed to do so, but their imitation was both slower and less
   precise than that of neurotypical individuals. In both groups, a more
   precise imitation scaled positively with participants' accuracy of
   emotion recognition. Limitations Given the study's focus on adults with
   autism without intellectual impairment, it is unclear whether the
   results generalize to children with autism or individuals with
   intellectual disability. Further, the new automated facial analysis,
   despite being less intrusive than electromyography, might be less
   sensitive. Conclusions Group differences in emotion recognition,
   imitation and their interrelationships highlight potential for treatment
   of social interaction problems in individuals with autism.},
  address                    = {CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
  affiliation                = {Drimalla, H (Corresponding Author), Humboldt Univ, Dept Psychol, Unter Linden 6, D-10099 Berlin, Germany. Drimalla, Hanna; Baskow, Irina; Dziobek, Isabel, Humboldt Univ, Dept Psychol, Unter Linden 6, D-10099 Berlin, Germany. Drimalla, Hanna; Dziobek, Isabel, Humboldt Univ, Berlin Sch Mind \& Brain, Clin Psychol Social Interact, Unter Linden 6, D-10099 Berlin, Germany. Drimalla, Hanna, Univ Potsdam, Hasso Plattner Inst, Digital Hlth Ctr, Neuen Palais 10, D-14469 Potsdam, Germany. Baskow, Irina; Behnia, Behnoush; Roepke, Stefan, Charite Univ Med Berlin, Dept Psychiat \& Psychotherapy, Campus Benjamin Franklin,Hindenburgdamm 30, D-12203 Berlin, Germany. Baskow, Irina; Behnia, Behnoush; Roepke, Stefan, Free Univ Berlin, Campus Benjamin Franklin,Hindenburgdamm 30, D-12203 Berlin, Germany. Baskow, Irina; Behnia, Behnoush; Roepke, Stefan, Humboldt Univ, Campus Benjamin Franklin,Hindenburgdamm 30, D-12203 Berlin, Germany. Drimalla, Hanna, Bielefeld Univ, Fac Technol, Multimodal Behav Proc, Inspirat 1, D-33619 Bielefeld, Germany.},
  article-number             = {27},
  author-email               = {drimalla@uni-bielefeld.de},
  da                         = {2022-09-28},
  doc-delivery-number        = {RJ3JW},
  doi                        = {10.1186/s13229-021-00430-0},
  funding-acknowledgement    = {Berlin School of Mind and Brain, Humboldt-Universitat zu Berlin, Berlin, Germany; Projekt DEAL},
  funding-text               = {Open Access funding enabled and organized by Projekt DEAL. The research was supported by the Berlin School of Mind and Brain, Humboldt-Universitat zu Berlin, Berlin, Germany.},
  journal-iso                = {Mol. Autism},
  keywords                   = {Autism; Imitation; Facial expression; Emotion recognition; Automated analysis},
  keywords-plus              = {SPECTRUM QUOTIENT AQ; FUNCTIONING AUTISM; EXPRESSIONS; MIMICRY; FACE; DEFICITS; COMMUNICATION; INDIVIDUALS; DISORDERS; INTENSITY},
  language                   = {English},
  number-of-cited-references = {75},
  oa                         = {Green Published, gold},
  orcid-numbers              = {Drimalla, Hanna/0000-0003-3783-7237},
  priority                   = {prio3},
  publisher                  = {BMC},
  research-areas             = {Genetics \& Heredity; Neurosciences \& Neurology},
  times-cited                = {7},
  type                       = {Article},
  unique-id                  = {WOS:000637495200001},
  usage-count-last-180-days  = {8},
  usage-count-since-2013     = {26},
  web-of-science-categories  = {Genetics \& Heredity; Neurosciences},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@article{ WOS:000474919900007,
Author = {Haines, Nathaniel and Bell, Ziv and Crowell, Sheila and Hahn, Hunter and
   Kamara, Dana and McDonough-Caplan, Heather and Shader, Tiffany and
   Beauchaine, Theodore P.},
Title = {Using automated computer vision and machine learning to code facial
   expressions of affect and arousal: Implications for emotion
   dysregulation research},
Journal = {DEVELOPMENT AND PSYCHOPATHOLOGY},
Year = {2019},
Volume = {31},
Number = {3, SI},
Pages = {871-886},
Month = {AUG},
Abstract = {As early as infancy, caregivers' facial expressions shape children's
   behaviors, help them regulate their emotions, and encourage or dissuade
   their interpersonal agency. In childhood and adolescence, proficiencies
   in producing and decoding facial expressions promote social competence,
   whereas deficiencies characterize several forms of psychopathology. To
   date, however, studying facial expressions has been hampered by the
   labor-intensive, time-consuming nature of human coding. We describe a
   partial solution: automated facial expression coding (AFEC), which
   combines computer vision and machine learning to code facial expressions
   in real time. Although AFEC cannot capture the full complexity of human
   emotion, it codes positive affect, negative affect, and arousal-core
   Research Domain Criteria constructs-as accurately as humans, and it
   characterizes emotion dysregulation with greater specificity than other
   objective measures such as autonomic responding. We provide an example
   in which we use AFEC to evaluate emotion dynamics in mother-daughter
   dyads engaged in conflict. Among other findings, AFEC (a) shows
   convergent validity with a validated human coding scheme, (b)
   distinguishes among risk groups, and (c) detects developmental increases
   in positive dyadic affect correspondence as teen daughters age. Although
   more research is needed to realize the full potential of AFEC, findings
   demonstrate its current utility in research on emotion dysregulation.},
Publisher = {CAMBRIDGE UNIV PRESS},
Address = {32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA},
Type = {Article},
Language = {English},
Affiliation = {Beauchaine, TP (Corresponding Author), Ohio State Univ, Dept Psychol, 1835 Neil Ave, Columbus, OH 43210 USA.
   Haines, Nathaniel; Bell, Ziv; Hahn, Hunter; Kamara, Dana; McDonough-Caplan, Heather; Shader, Tiffany; Beauchaine, Theodore P., Ohio State Univ, Dept Psychol, 1835 Neil Ave, Columbus, OH 43210 USA.
   Crowell, Sheila, Univ Utah, Dept Psychol, Salt Lake City, UT 84112 USA.
   Crowell, Sheila, Univ Utah, Dept Psychiat, Salt Lake City, UT USA.},
DOI = {10.1017/S0954579419000312},
Article-Number = {PII S0954579419000312},
ISSN = {0954-5794},
EISSN = {1469-2198},
Keywords = {arousal; emotion dysregulation; facial expression; negative valence
   system; positive valence system},
Keywords-Plus = {NONSUICIDAL SELF-INJURY; NONVERBAL-COMMUNICATION; CONTEXT MATTERS;
   RECOGNITION; CHILDREN; BEHAVIOR; PERSONALITY; ADOLESCENTS; MECHANISMS;
   DEPRESSION},
Research-Areas = {Psychology},
Web-of-Science-Categories  = {Psychology, Developmental},
Author-Email = {beauchaine.1@osu.edu},
ResearcherID-Numbers = {Beauchaine, Theodore P/CAF-4478-2022},
Funding-Acknowledgement = {National Institutes of Health {[}DE025980, MH074196]; National
   Institutes of Health Science of Behavior Change (SoBC) Common Fund},
Funding-Text = {Work on this article was supported by Grants DE025980 (to T.P.B.) and
   MH074196 (to S.C.) and from the National Institutes of Health, and by
   the National Institutes of Health Science of Behavior Change (SoBC)
   Common Fund.},
Number-of-Cited-References = {139},
Times-Cited = {6},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {15},
Journal-ISO = {Dev. Psychopathol.},
Doc-Delivery-Number = {II0UF},
Web-of-Science-Index = {Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000474919900007},
OA = {Green Accepted},
DA = {2022-09-28},
}

@article{ WOS:000821578500036,
Author = {Sri, D. Vijaya and Nihitha, S. Vasavi and Amulya, T. N. K. and Reddy, U.
   Maheshwar},
Title = {RECOGNITION OF EMOTION IN TEXTUAL TWEETS USING SVM AND NAIVE BAYES
   ALGORITHMS},
Journal = {INTERNATIONAL JOURNAL OF EARLY CHILDHOOD SPECIAL EDUCATION},
Year = {2022},
Volume = {14},
Number = {4},
Pages = {1379-1390},
Abstract = {we proposed a emotion recognition system where it recognize the emotions
   in tweets. As emotions play a vital role in our lives. As we can see
   that many people use social media where they use the platform for many
   purposes, some of them tweet in a good way and some of them in a
   bullying way. Emotion and opinions of different people can be carried
   out on tweets to analyze public opinion on a news and social events that
   are taking place in present society. In this project, by using machine
   learning algorithms we have implemented emotion recognition by
   classifying tweets as positive and negative. By recognizing these
   positive and negative tweets we can identify people emotions where we
   can reduce the forged statements. Initially we have divided our dataset
   into train and test dataset, where it is used to train the model and by
   comparing the train data with the test data, the model recognizes the
   emotions in tweets. By using svm and naive bayes algorithms we classify
   the text based on twitter into different emotions and predicted emojis
   like love, fear, anger, sadness, joy. Based on the performance analysis
   we predicted optimal result with 79\% and 81\% F1 score.},
Publisher = {ANADOLU UNIV},
Address = {INST FINE ARTS, ESKISEHIR, 26470, TURKEY},
Type = {Article},
Language = {English},
Affiliation = {Sri, DV (Corresponding Author), Lakireddy Bali Reddy Coll Engn, Dept Informat Technol, Mylavaram, India.
   Sri, D. Vijaya; Nihitha, S. Vasavi; Amulya, T. N. K.; Reddy, U. Maheshwar, Lakireddy Bali Reddy Coll Engn, Dept Informat Technol, Mylavaram, India.},
DOI = {10.9756/INTJECSE/V14I4.182},
ISSN = {1308-5581},
Keywords = {LR-SGD; Emotion; Machine Learning models; TF; TF-IDF},
Keywords-Plus = {SENTIMENT ANALYSIS; TWITTER},
Research-Areas = {Education \& Educational Research},
Web-of-Science-Categories  = {Education, Special},
Author-Email = {devineni66@gmail.com
   nihitha.s10901@gmail.com
   tanneeruamulya283@gmail.com
   maheshupputuri2000@gmail.com},
Number-of-Cited-References = {39},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Int. J. Early Child. Spec. Educ.},
Doc-Delivery-Number = {2S1RZ},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000821578500036},
DA = {2022-09-28},
}

@Article{WOS:000697827200029,
  author                     = {Jaison, Asha and Deepa, C.},
  journal                    = {BIOSCIENCE BIOTECHNOLOGY RESEARCH COMMUNICATIONS},
  title                      = {A Review on Facial Emotion Recognition and Classification Analysis with Deep Learning},
  year                       = {2021},
  issn                       = {0974-6455},
  number                     = {5, SI},
  pages                      = {154-161},
  volume                     = {14},
  status                     = {Aceito},
  abstract                   = {Automatic face expression recognition is an exigent research subject and
   a challenge in computer vision. It is an interdisciplinary domain
   standing at the crossing of behavioural science, psychology, neurology,
   and artificial intelligence. Human-robot interaction is getting more
   significant with the automation of every field, like treating autistic
   patients, child therapy, babysitting, etc. In all the cases robots need
   to understand the present state of mind for better decision making. It
   is difficult for machine learning techniques to recognize the
   expressions of people since there will be significant changes in the way
   of their expressions. The emotions expressed through the human face have
   its importance in making arguments and decisions on different subjects.
   Machine Learning with Computer Vision and Deep Learning can be used to
   recognize facial expressions from the preloaded or real time images with
   human faces. DNN (Deep Neural Networking) is one among the hottest areas
   of research and is found to be very effective in classification of
   images with a high degree of accuracy. In the proposed work, the popular
   dataset CK+ is analysed for comparison. The dataset FER 2013 and
   home-brewed data sets are used in the work for calculating the accuracy
   of the model created. The results are obtained in such a way that DCNN
   approach is very efficient in facial emotion recognition. Experiments
   and study show that the dataset, FER 2013 is a high-quality dataset with
   equal efficiency as the other two popular datasets. This paper aims to
   ameliorate the accuracy of classification of facial emotion.},
  address                    = {C-52 HOUSING BOARD COLONY, KOHE FIZA, BHOPAL, MADHYA PRADESH 462 001, INDIA},
  affiliation                = {Jaison, A (Corresponding Author), Sri Ramakrishna Coll Arts \& Sci, Dept Informat Technol, Coimbatore, Tamil Nadu, India. Jaison, Asha; Deepa, C., Sri Ramakrishna Coll Arts \& Sci, Dept Informat Technol, Coimbatore, Tamil Nadu, India.},
  author-email               = {asha.jaison@gmail.com deepapkd@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {UT0PI},
  doi                        = {10.21786/bbrc/14.5/29},
  journal-iso                = {Biosci. Biotechnol. Res. Commun.},
  keywords                   = {FACIAL EMOTION RECOGNITION (FER); DEEP CONVOLUTIONAL NEURAL NET WORK (DCNN); OPEN CV (OPEN-SOURCE COMPUTER VISION LIBRARY); VGG 16},
  keywords-plus              = {EXPRESSION RECOGNITION; FACE; NETWORK},
  language                   = {English},
  number-of-cited-references = {34},
  oa                         = {Bronze},
  priority                   = {prio2},
  publisher                  = {SOC SCIENCE \& NATURE},
  research-areas             = {Biotechnology \& Applied Microbiology},
  times-cited                = {0},
  type                       = {Review},
  unique-id                  = {WOS:000697827200029},
  usage-count-last-180-days  = {4},
  usage-count-since-2013     = {14},
  web-of-science-categories  = {Biotechnology \& Applied Microbiology},
  web-of-science-index       = {Emerging Sources Citation Index (ESCI)},
}

@Article{WOS:000831761800001,
  author                     = {Matveev, Yuri and Matveev, Anton and Frolova, Olga and Lyakso, Elena and Ruban, Nersisson},
  journal                    = {MATHEMATICS},
  title                      = {Automatic Speech Emotion Recognition of Younger School Age Children},
  year                       = {2022},
  month                      = {JUL},
  number                     = {14},
  volume                     = {10},
  status                     = {Rejeitado - Escopo},
  abstract                   = {This paper introduces the extended description of a database that
   contains emotional speech in the Russian language of younger school age
   (8-12-year-old) children and describes the results of validation of the
   database based on classical machine learning algorithms, such as Support
   Vector Machine (SVM) and Multi-Layer Perceptron (MLP). The validation is
   performed using standard procedures and scenarios of the validation
   similar to other well-known databases of children's emotional acting
   speech. Performance evaluation of automatic multiclass recognition on
   four emotion classes ``Neutral (Calm)-Joy-Sadness-Anger{''} shows the
   superiority of SVM performance and also MLP performance over the results
   of perceptual tests. Moreover, the results of automatic recognition on
   the test dataset which was used in the perceptual test are even better.
   These results prove that emotions in the database can be reliably
   recognized both by experts and automatically using classical machine
   learning algorithms such as SVM and MLP, which can be used as baselines
   for comparing emotion recognition systems based on more sophisticated
   modern machine learning methods and deep neural networks. The results
   also confirm that this database can be a valuable resource for
   researchers studying affective reactions in speech communication during
   child-computer interactions in the Russian language and can be used to
   develop various edutainment, health care, etc. applications.},
  address                    = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  affiliation                = {Matveev, Y (Corresponding Author), St Petersburg State Univ, Child Speech Res Grp, Dept Higher Nervous Activ \& Psychophysiol, St Petersburg 199034, Russia. Matveev, Yuri; Matveev, Anton; Frolova, Olga; Lyakso, Elena, St Petersburg State Univ, Child Speech Res Grp, Dept Higher Nervous Activ \& Psychophysiol, St Petersburg 199034, Russia. Ruban, Nersisson, Vellore Inst Technol, Sch Elect Engn, Vellore 632014, Tamil Nadu, India.},
  article-number             = {2373},
  author-email               = {yunmatveev@gmail.com aush.tx@gmail.com olchel@yandex.ru lyakso@gmail.com ruban.ice@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {3H0VP},
  doi                        = {10.3390/math10142373},
  eissn                      = {2227-7390},
  funding-acknowledgement    = {Russian Foundation for Basic Research {[}19-57-45008-IND]; Russian Science Foundation {[}22-45-02007]; RFBR {[}19-57-45008]; RSF {[}22-45-02007]},
  funding-text               = {The research was financially supported by the Russian Foundation for Basic Research (projects 19-57-45008-IND) and Russian Science Foundation (project 22-45-02007). Database collection and selection the feature set, powerful to distinguish between different emotions were financially supported by the RFBR project 19-57-45008; automatic emotion speech classification was financially supported by the RSF project 22-45-02007.},
  journal-iso                = {Mathematics},
  keywords                   = {speech emotion recognition; child speech; younger school age},
  keywords-plus              = {FEATURES; CLASSIFICATION; DATABASES},
  language                   = {English},
  number-of-cited-references = {76},
  oa                         = {gold},
  orcid-numbers              = {NERSISSON, RUBAN/0000-0003-1695-3618},
  publisher                  = {MDPI},
  research-areas             = {Mathematics},
  researcherid-numbers       = {NERSISSON, RUBAN/D-3764-2019},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000831761800001},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {3},
  web-of-science-categories  = {Mathematics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@InProceedings{WOS:000569373500037,
  author                     = {de Morais, Felipe and Kautzmann, Tiago R. and Bittencourt, Ig I. and Jaques, Patricia A.},
  booktitle                  = {TRANSFORMING LEARNING WITH MEANINGFUL TECHNOLOGIES, EC-TEL 2019},
  title                      = {EmAP-ML: A Protocol of Emotions and Behaviors Annotation for Machine Learning Labels},
  year                       = {2019},
  address                    = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
  editor                     = {Scheffel, M and Broisin, J and PammerSchindler, V and Ioannou, A and Schneider, J},
  note                       = {14th European Conference on Technology Enhanced Learning (EC-TEL), Delft Univ Technol, Leiden Delft Erasmus Ctr Educ \& Learning, Delft, NETHERLANDS, SEP 16-19, 2019},
  organization               = {Int Assoc Mobile Learning},
  pages                      = {495-509},
  publisher                  = {SPRINGER INTERNATIONAL PUBLISHING AG},
  series                     = {Lecture Notes in Computer Science},
  volume                     = {11722},
  status                     = {Aceito},
  abstract                   = {The detection of students' emotions in computer-based learning
   environments is a complex task. Although emotions can be detected from
   sensors, a less intrusive method is to train supervised machine learning
   algorithms for the emotions prediction based on the log of students'
   actions on the system. For these algorithms to work as expected, they
   need to be trained with a large amount of reliable ground truth labels.
   Generally, labels are generated by students themselves or by coders
   monitoring students, watching videos from the students, or reviewing
   logs of students' actions. Younger learners (i.e., children) are unable
   to label their emotions properly. Still, it is difficult for a coder to
   identify students' emotions only from their face since the emotional
   facial expression is generally subtle in a learning setting. This
   article describes EmAP-ML (Emotions Annotation Protocol for Machine
   Learning), a protocol for coders to annotate students' learning emotions
   and behaviors based on video records, which contains facial expressions,
   ambient audio, and computer screen. The screen and ambient audio records
   allow coders to infer students' appraisal (an evaluation that elicits an
   emotion) to identify emotions even when the facial expression is subtle.
   This protocol was evaluated by two coders who annotated videos obtained
   from 55 students while using a tutoring system, having achieved an
   agreement coefficient of 0.62, measured through Cohen's Kappa
   statistics.},
  affiliation                = {de Morais, F (Corresponding Author), Univ Vale Rio dos Sinos UNISINOS, Sao Leopoldo, RS, Brazil. de Morais, Felipe; Kautzmann, Tiago R.; Jaques, Patricia A., Univ Vale Rio dos Sinos UNISINOS, Sao Leopoldo, RS, Brazil. Bittencourt, Ig I., Univ Fed Alagoas UFAL, Maceio, Alagoas, Brazil.},
  author-email               = {felipmorais@edu.unisinos.br tkautzmann@edu.unisinos.br ig.ibert@ic.ufal.br pjaques@unisinos.br},
  da                         = {2022-09-28},
  doc-delivery-number        = {BP9HH},
  doi                        = {10.1007/978-3-030-29736-7\_37},
  eissn                      = {1611-3349},
  funding-acknowledgement    = {Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior - Brasil (CAPES) {[}001]; STICAMSUD {[}18-STIC-03]; FAPERGS {[}17/2551-0001203-8]; CNPq from Brazil},
  funding-text               = {This study was financed in part by the Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior - Brasil (CAPES) - Finance Code 001, STICAMSUD 18-STIC-03, FAPERGS (granting 17/2551-0001203-8) and CNPq from Brazil.},
  isbn                       = {978-3-030-29736-7; 978-3-030-29735-0},
  issn                       = {0302-9743},
  keywords                   = {Annotation Protocol; Learning emotions and behaviors; Affective computing; Machine learning; Educational Data Mining},
  language                   = {English},
  number-of-cited-references = {28},
  orcid-numbers              = {Bittencourt, Ig Ibert/0000-0001-5676-2280 de Morais, Felipe/0000-0002-8510-4516 Roberto Kautzmann, Tiago/0000-0002-6017-8340 Augustin Jaques Maillard, Patricia/0000-0002-2933-1052},
  priority                   = {prio2},
  research-areas             = {Computer Science; Education \& Educational Research},
  researcherid-numbers       = {Bittencourt, Ig Ibert/AAU-1882-2021 Augustin Jaques Maillard, Patricia/F-5544-2013},
  times-cited                = {0},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000569373500037},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {0},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Hardware \& Architecture; Computer Science, Information Systems; Computer Science, Theory \& Methods; Education \& Educational Research},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S); Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)},
}

@Article{WOS:000823293300008,
  author                     = {Washington, Peter and Kalantarian, Haik and Kent, John and Husic, Arman and Kline, Aaron and Leblanc, Emilie and Hou, Cathy and Mutlu, Onur Cezmi and Dunlap, Kaitlyn and Penev, Yordan and Varma, Maya and Stockham, Nate Tyler and Chrisman, Brianna and Paskov, Kelley and Sun, Min Woo and Jung, Jae-Yoon and Voss, Catalin and Haber, Nick and Wall, Dennis Paul},
  journal                    = {JMIR PEDIATRICS AND PARENTING},
  title                      = {Improved Digital Therapy for Developmental Pediatrics Using Domain-Specific Artificial Intelligence: Machine Learning Study},
  year                       = {2022},
  issn                       = {2561-6722},
  month                      = {APR-JUN},
  number                     = {2},
  volume                     = {5},
  status                     = {Aceito},
  abstract                   = {Background: Automated emotion classification could aid those who
   struggle to recognize emotions, including children with developmental
   behavioral conditions such as autism. However, most computer vision
   emotion recognition models are trained on adult emotion and therefore
   underperform when applied to child faces.
   Objective: We designed a strategy to gamify the collection and labeling
   of child emotion-enriched images to boost the performance of automatic
   child emotion recognition models to a level closer to what will be
   needed for digital health care approaches.
   Methods: We leveraged our prototype therapeutic smartphone game,
   GuessWhat, which was designed in large part for children with
   developmental and behavioral conditions, to gamify the secure collection
   of video data of children expressing a variety of emotions prompted by
   the game. Independently, we created a secure web interface to gamify the
   human labeling effort, called HollywoodSquares, tailored for use by any
   qualified labeler. We gathered and labeled 2155 videos, 39,968 emotion
   frames, and 106,001 labels on all images. With this drastically expanded
   pediatric emotion-centric database (>30 times larger than existing
   public pediatric emotion data sets), we trained a convolutional neural
   network (CNN) computer vision classifier of happy, sad, surprised,
   fearful, angry, disgust, and neutral expressions evoked by children.
   Results: The classifier achieved a 66.9\% balanced accuracy and 67.4\%
   Fl-score on the entirety of the Child Affective Facial Expression (CAFE)
   as well as a 79.1\% balanced accuracy and 78\% Fl-score on CAFE Subset
   A, a subset containing at least 60\% human agreement on emotions labels.
   This performance is at least 10\% higher than all previously developed
   classifiers evaluated against CAFE, the best of which reached a 56\%
   balanced accuracy even when combining ``anger{''} and ``disgust{''} into
   a single class.
   Conclusions: This work validates that mobile games designed for
   pediatric therapies can generate high volumes of domain-relevant data
   sets to train state-of-the-art classifiers to perform tasks helpful to
   precision health efforts.},
  address                    = {130 QUEENS QUAY East, Unit 1100, TORONTO, ON M5A 0P6, CANADA},
  affiliation                = {Washington, P (Corresponding Author), Stanford Univ, Dept Pediat Syst Med, Stanford, CA 94305 USA. Washington, P (Corresponding Author), Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA. Washington, Peter, Stanford Univ, Dept Pediat Syst Med, Stanford, CA 94305 USA. Washington, Peter, Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA.},
  article-number             = {e26760},
  author-email               = {peterwashington@stanford.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {2U6UN},
  doi                        = {10.2196/26760},
  funding-acknowledgement    = {National Institutes of Health {[}1R01EB025025-01, 1R01LM013364-01, 1R21HD091500-01, 1R01LM013083]; National Science Foundation {[}2014232]; Hartwell Foundation; Bill and Melinda Gates Foundation; Coulter Foundation; Lucile Packard Foundation; Auxiliaries Endowment; Islamic Development Bank Transform Fund; Weston Havens Foundation; Stanford's Human-Centered Artificial Intelligence Program; Precision Health and Integrated Diagnostics Center; Beckman Center; Bio-X Center; Predictives and Diagnostics Accelerator; Spark Program in Translational Research; Wu Tsai Neurosciences Institute's Neuroscience:Translate Program; Stanford Interdisciplinary Graduate Fellowship (SIGF); Spectrum; MediaX},
  funding-text               = {We would like to acknowledge all the nine high school and undergraduate emotion annotators: Natalie Park, Chris Harjadi, Meagan Tsou, Belle Bankston, Hadley Daniels, Sky Ng-Thow-Hing, Bess Olshen, Courtney McCormick, and Jennifer Yu. The work was supported in part by funds to DPW from the National Institutes of Health (grants 1R01EB025025-01, 1R01LM013364-01, 1R21HD091500-01, and 1R01LM013083) , the National Science Foundation (Award 2014232) , The Hartwell Foundation, Bill and Melinda Gates Foundation, Coulter Foundation, Lucile Packard Foundation, Auxiliaries Endowment, the Islamic Development Bank Transform Fund, the Weston Havens Foundation, and program grants from Stanford's Human-Centered Artificial Intelligence Program, Precision Health and Integrated Diagnostics Center, Beckman Center, Bio-X Center, Predictives and Diagnostics Accelerator, Spectrum, Spark Program in Translational Research, MediaX, and from the Wu Tsai Neurosciences Institute's Neuroscience:Translate Program. We also acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer, and Peter Sullivan. PW would like to acknowledge support from Mr. Schroeder and the Stanford Interdisciplinary Graduate Fellowship (SIGF) as the Schroeder Family Goldman Sachs Graduate Fellow.},
  journal-iso                = {JMIR Pediatr. Parent.},
  keywords                   = {computer vision; emotion recognition; affective computing; autism spectrum disorder; pediatrics; mobile health; digital therapy; convolutional neural network; machine learning; artificial intelligence},
  keywords-plus              = {FACIAL EXPRESSIONS; EMOTION RECOGNITION; CHILDREN; FEASIBILITY},
  language                   = {English},
  number-of-cited-references = {62},
  oa                         = {gold, Green Published},
  orcid-numbers              = {husic, arman/0000-0002-9180-5212 washington, peter/0000-0003-3276-4411 Penev, Yordan/0000-0001-8520-9417 Hou, Cathy/0000-0001-6766-5128 Kent, John/0000-0002-7989-6596},
  priority                   = {prio1},
  publisher                  = {JMIR PUBLICATIONS, INC},
  research-areas             = {Pediatrics},
  times-cited                = {1},
  type                       = {Article},
  unique-id                  = {WOS:000823293300008},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {2},
  web-of-science-categories  = {Pediatrics},
  web-of-science-index       = {Emerging Sources Citation Index (ESCI)},
}

@Article{WOS:000849465700001,
  author                     = {Cai, Qian and Cui, Guo-Chong and Wang, Hai-Xian},
  journal                    = {MACHINE INTELLIGENCE RESEARCH},
  title                      = {EEG-based Emotion Recognition Using Multiple Kernel Learning},
  issn                       = {2731-538X},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Emotion recognition based on electroencephalography (EEG) has a wide
   range of applications and has great potential value, so it has received
   increasing attention from academia and industry in recent years.
   Meanwhile, multiple kernel learning (MKL) has also been favored by
   researchers for its data-driven convenience and high accuracy. However,
   there is little research on MKL in EEG-based emotion recognition.
   Therefore, this paper is dedicated to exploring the application of MKL
   methods in the field of EEG emotion recognition and promoting the
   application of MKL methods in EEG emotion recognition. Thus, we proposed
   a support vector machine (SVM) classifier based on the MKL algorithm
   EasyMKL to investigate the feasibility of MKL algorithms in EEG-based
   emotion recognition problems. We designed two data partition methods,
   random division to verify the validity of the MKL method and sequential
   division to simulate practical applications. Then, tri-categorization
   experiments were performed for neutral, negative and positive emotions
   based on a commonly used dataset, the Shanghai Jiao Tong University
   emotional EEG dataset (SEED). The average classification accuracies for
   random division and sequential division were 92.25\% and 74.37\%,
   respectively, which shows better classification performance than the
   traditional single kernel SVM. The final results show that the MKL
   method is obviously effective, and the application of MKL in EEG emotion
   recognition is worthy of further study. Through the analysis of the
   experimental results, we discovered that the simple mathematical
   operations of the features on the symmetrical electrodes could not
   effectively integrate the spatial information of the EEG signals to
   obtain better performance. It is also confirmed that higher frequency
   band information is more correlated with emotional state and contributes
   more to emotion recognition. In summary, this paper explores research on
   MKL methods in the field of EEG emotion recognition and provides a new
   way of thinking for EEG-based emotion recognition research.},
  address                    = {CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND},
  affiliation                = {Cui, GC; Wang, HX (Corresponding Author), Southeast Univ, Sch Biol Sci \& Med Engn, Key Lab Child Dev \& Learning Sci, Minist Educ, Nanjing 210096, Peoples R China. Wang, HX (Corresponding Author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230094, Peoples R China. Cai, Qian, Nanjing Audit Univ, Sch Stat \& Data Sci, Nanjing 211815, Peoples R China. Cui, Guo-Chong; Wang, Hai-Xian, Southeast Univ, Sch Biol Sci \& Med Engn, Key Lab Child Dev \& Learning Sci, Minist Educ, Nanjing 210096, Peoples R China. Wang, Hai-Xian, Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230094, Peoples R China.},
  author-email               = {caiq@nau.edu.cn gc\_cui@seu.edu.cn hxwang@seu.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {4G8VC},
  doi                        = {10.1007/s11633-022-1352-1},
  earlyaccessdate            = {SEP 2022},
  eissn                      = {2731-5398},
  funding-acknowledgement    = {National Natural Science Foundation of China {[}62176054]; University Synergy Innovation Program of Anhui Province, China {[}GXXT-2020-015]},
  funding-text               = {This work was supported by National Natural Science Foundation of China (No. 62176054), and University Synergy Innovation Program of Anhui Province, China (No. GXXT-2020-015).},
  journal-iso                = {Mach. Intell. Res.},
  keywords                   = {Emotion recognition; electroencephalography (EEG); multiple kernel learning; machine learning; brain computer interface},
  keywords-plus              = {ENTROPY; CLASSIFICATION},
  language                   = {English},
  number-of-cited-references = {51},
  publisher                  = {SPRINGERNATURE},
  research-areas             = {Automation \& Control Systems; Computer Science},
  times-cited                = {0},
  type                       = {Article; Early Access},
  unique-id                  = {WOS:000849465700001},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Automation \& Control Systems; Computer Science, Artificial Intelligence},
  web-of-science-index       = {Emerging Sources Citation Index (ESCI)},
}

@Article{WOS:000520552800001,
  author                     = {Law, Effie Lai-Chong and Soleimani, Samaneh and Watkins, Dawn and Barwick, Joanna},
  journal                    = {BEHAVIOUR \& INFORMATION TECHNOLOGY},
  title                      = {Automatic voice emotion recognition of child-parent conversations in natural settings},
  year                       = {2021},
  issn                       = {0144-929X},
  month                      = {AUG 18},
  number                     = {11},
  pages                      = {1072-1089},
  volume                     = {40},
  status                     = {Rejeitado - Escopo},
  abstract                   = {While voice communication of emotion has been researched for decades,
   the accuracy of automatic voice emotion recognition (AVER) is yet to
   improve. In particular, the intergenerational communication has been
   under-researched, as indicated by the lack of an emotion corpus on
   child-parent conversations. In this paper, we presented our work of
   applying Support-Vector Machines (SVMs), established machine learning
   models, to analyze 20 pairs of child-parent dialogues on everyday life
   scenarios. Among many issues facing the emerging work of AVER, we
   explored two critical ones: the methodological issue of optimising its
   performance against computational costs, and the conceptual issue on the
   state of emotionally neutral. We used the minimalistic/extended acoustic
   feature set extracted with OpenSMILE and a small/large set of annotated
   utterances for building models, and analyzed the prevalence of the class
   neutral. Results indicated that the bigger the combined sets, the better
   the training outcomes. Regardless, the classification models yielded
   modest average recall when applied to the child-parent data, indicating
   their low generalizability. Implications for improving AVER and its
   potential uses are drawn.},
  address                    = {2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND},
  affiliation                = {Law, ELC (Corresponding Author), Univ Leicester, Sch Informat, Leicester, Leics, England. Law, Effie Lai-Chong; Soleimani, Samaneh, Univ Leicester, Sch Informat, Leicester, Leics, England. Watkins, Dawn; Barwick, Joanna, Univ Leicester, Sch Law, Leicester, Leics, England.},
  author-email               = {lcl9@le.ac.uk},
  da                         = {2022-09-28},
  doc-delivery-number        = {UY1WD},
  doi                        = {10.1080/0144929X.2020.1741684},
  earlyaccessdate            = {MAR 2020},
  eissn                      = {1362-3001},
  funding-acknowledgement    = {Economic and Social Research Council {[}ES/M000443/1]},
  funding-text               = {This work was supported by the Economic and Social Research Council {[}grant number ES/M000443/1]. We would like to express our gratitude to the children and parents who took part in the study. We would also like to thank Dr. Leandro Minku, University of Birmingham, for his advice with the machine learning techniques, and anonymous reviewers for comments that greatly improved the manuscript.},
  journal-iso                = {Behav. Inf. Technol.},
  keywords                   = {Vocal emotion; child-parent conversation; recognition accuracy; emotion corpora; emotion neutrality; IEMOCAP},
  keywords-plus              = {SPEECH; EXPRESSION; FRAMEWORK; SIGNALS; CORPUS; MODEL; LINKS; MIND},
  language                   = {English},
  number-of-cited-references = {101},
  publisher                  = {TAYLOR \& FRANCIS LTD},
  research-areas             = {Computer Science; Engineering},
  times-cited                = {2},
  type                       = {Article},
  unique-id                  = {WOS:000520552800001},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {18},
  web-of-science-categories  = {Computer Science, Cybernetics; Ergonomics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@InProceedings{WOS:000760910503083,
  author                     = {Arabian, H. and Wagner-Hartl, V and Chase, J. Geoffrey and Moeller, K.},
  booktitle                  = {2021 43RD ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE \& BIOLOGY SOCIETY (EMBC)},
  title                      = {Facial Emotion Recognition Focused on Descriptive Region Segmentation},
  year                       = {2021},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {43rd Annual International Conference of the IEEE-Engineering-in-Medicine-and-Biology-Society (IEEE EMBC), ELECTR NETWORK, NOV 01-05, 2021},
  organization               = {IEEE Engn Med \& Biol Soc; IEEE; Elsevier; Inst Engn \& Technol},
  pages                      = {3415-3418},
  publisher                  = {IEEE},
  series                     = {IEEE Engineering in Medicine and Biology Society Conference Proceedings},
  status                     = {Aceito},
  abstract                   = {Facial emotion recognition (FER) is useful in many different
   applications and could offer significant benefit as part of feedback
   systems to train children with Autism Spectrum Disorder (ASD) who
   struggle to recognize facial expressions and emotions. This project
   explores the potential of real time FER based on the use of local
   regions of interest combined with a machine learning approach. Histogram
   of Oriented Gradients (HOG) was implemented for feature extraction,
   along with 3 different classifiers, 2 based on k-Nearest Neighbor and 1
   using Support Vector Machine (SVM) classification. Model performance was
   compared using accuracy of randomly selected validation sets after
   training on random training sets of the Oulu-CASIA database. Image
   classes were distributed evenly, and accuracies of up to 98.44\% were
   observed with small variation depending on data distributions. The
   region selection methodology provided a compromise between accuracy and
   number of extracted features, and validated the hypothesis a focus on
   smaller informative regions performs just as well as the entire image.},
  affiliation                = {Arabian, H (Corresponding Author), Inst Tech Med ITeM, D-78054 Villingen Schwenningen, Germany. Arabian, H.; Moeller, K., Inst Tech Med ITeM, D-78054 Villingen Schwenningen, Germany. Wagner-Hartl, V, Hsch Furtwangen Univ, Dept Ind Technol, D-78532 Tuttlingen, Germany. Wagner-Hartl, V, Hsch Furtwangen Univ, Engn Psychol, D-78532 Tuttlingen, Germany. Chase, J. Geoffrey, Univ Canterbury, Ctr Bioengn, Dept Mech Engn, Christchurch, New Zealand.},
  author-email               = {H.Arabian@hs-furtwangen.de},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BS7HB},
  doi                        = {10.1109/EMBC46164.2021.9629742},
  eissn                      = {1558-4615},
  funding-acknowledgement    = {German Federal Ministry of Research and Education (BMBF) {[}13FH5I06IA -PersonaMed]},
  funding-text               = {Partial support by a grant from the German Federal Ministry of Research and Education (BMBF) under project No. 13FH5I06IA -PersonaMed is gratefully acknowledged.},
  isbn                       = {978-1-7281-1179-7},
  issn                       = {1557-170X},
  keywords                   = {Autism Spectrum Disorder (ASD); Facial Emotion Recognition (FER); Feature Extraction (FE); Machine Learning; Oulu-CASIA},
  language                   = {English},
  number-of-cited-references = {12},
  orcid-numbers              = {Arabian, Herag/0000-0002-1492-1121 Wagner-Hartl, Verena/0000-0002-0599-1291},
  priority                   = {prio3},
  research-areas             = {Engineering},
  times-cited                = {0},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000760910503083},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Engineering, Biomedical; Engineering, Electrical \& Electronic},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000629356500006,
  author                     = {Yu, Guiping},
  journal                    = {COMPLEXITY},
  title                      = {Emotion Monitoring for Preschool Children Based on Face Recognition and Emotion Recognition Algorithms},
  year                       = {2021},
  issn                       = {1076-2787},
  month                      = {MAR 2},
  volume                     = {2021},
  status                     = {Aceito},
  abstract                   = {In this paper, we study the face recognition and emotion recognition
   algorithms to monitor the emotions of preschool children. For previous
   emotion recognition focusing on faces, we propose to obtain more
   comprehensive information from faces, gestures, and contexts. Using the
   deep learning approach, we design a more lightweight network structure
   to reduce the number of parameters and save computational resources.
   There are not only innovations in applications, but also algorithmic
   enhancements. And face annotation is performed on the dataset, while a
   hierarchical sampling method is designed to alleviate the data imbalance
   phenomenon that exists in the dataset. A new feature descriptor, called
   ``oriented gradient histogram from three orthogonal planes,{''} is
   proposed to characterize facial appearance variations. A new efficient
   geometric feature is also proposed to capture facial contour variations,
   and the role of audio methods in emotion recognition is explored.
   Multifeature fusion can be used to optimally combine different features.
   The experimental results show that the method is very effective compared
   to other recent methods in dealing with facial expression recognition
   problems about videos in both laboratory-controlled environments and
   outdoor environments. The method performed experiments on expression
   detection in a facial expression database. The experimental results are
   compared with data from previous studies and demonstrate the
   effectiveness of the proposed new method.},
  address                    = {ADAM HOUSE, 3RD FL, 1 FITZROY SQ, LONDON, WIT 5HE, ENGLAND},
  affiliation                = {Yu, GP (Corresponding Author), Eastern Liaoning Univ, Normal Coll, Dandong 118000, Liaoning, Peoples R China. Yu, Guiping, Eastern Liaoning Univ, Normal Coll, Dandong 118000, Liaoning, Peoples R China.},
  article-number             = {6654455},
  author-email               = {133044@elnu.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {QX4ZN},
  doi                        = {10.1155/2021/6654455},
  eissn                      = {1099-0526},
  journal-iso                = {Complexity},
  keywords-plus              = {MULTISENSOR DATA FUSION; DEEP},
  language                   = {English},
  number-of-cited-references = {26},
  oa                         = {gold},
  priority                   = {prio3},
  publisher                  = {WILEY-HINDAWI},
  research-areas             = {Mathematics; Science \& Technology - Other Topics},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000629356500006},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {6},
  web-of-science-categories  = {Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000549823900015,
  author                     = {Jarraya, Salma Kammoun and Masmoudi, Marwa and Hammami, Mohamed},
  journal                    = {IEEE ACCESS},
  title                      = {Compound Emotion Recognition of Autistic Children During Meltdown Crisis Based on Deep Spatio-Temporal Analysis of Facial Geometric Features},
  year                       = {2020},
  issn                       = {2169-3536},
  pages                      = {69311-69326},
  volume                     = {8},
  status                     = {Aceito},
  abstract                   = {An important contribution to computer vision applications has been made
   by recognizing human emotion. Although it is very significant, this work
   considers the security of autistic people while in meltdown crisis by
   introducing a new system to warn caregivers through facial expressions
   detection. A precautionary approach has been taken to deal with meltdown
   crisis. Certainly, the indications of Meltdown are linked to abnormal
   facial expressions related to compound emotions. Actually, researchers
   thought long ago that Human Facial Expressions (HFE) are not able to
   express more than the seven basics emotions. HFE have been considered by
   psychologists as very complicated one, which can indicate two or even
   more emotions known as compound or mixed ones. A few studies have been
   done concerning Compound Emotion (CE). As well as, many difficult tasks
   to detect Compound Emotion Recognition (CER). In this paper, we
   empirically assess a group of deep spatio-temporal geometric features of
   micro-expressions of autistic children during a meltdown crisis. To
   achieve this goal, we make a comparison of the CER performance and
   diverse collections of micro-expressions features to select the features
   which best differentiates autistic children CE in meltdown crisis from
   normal state, and the best classifier performance. We record autistic
   children videos in normal and meltdown crisis using Kinect camera in
   serious circumstances. The experimental evaluation shows that the deep
   spatio-temporal geometric features and Recurrent Neural Network RNN with
   3 hidden layer using Information Gain Feature Selection methods provide
   best performance (85.8\%).},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Masmoudi, M (Corresponding Author), Univ Sfax, Mir Cl Lab, Sfax 3029, Tunisia. Jarraya, Salma Kammoun, King Abdulaziz Univ, Fac Comp \& Informat Technol, CS Dept, Jeddah 21589, Saudi Arabia. Jarraya, Salma Kammoun; Masmoudi, Marwa; Hammami, Mohamed, Univ Sfax, Mir Cl Lab, Sfax 3029, Tunisia. Hammami, Mohamed, Univ Sfax, Dept Comp Sci, Fac Sci, Sfax 3029, Tunisia.},
  author-email               = {marwa.masmoudi19@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {MM0AY},
  doi                        = {10.1109/ACCESS.2020.2986654},
  funding-acknowledgement    = {Deanship of Scientific Research (DSR), King Abdulaziz University, Jeddah {[}DF-350-165-1441]},
  funding-text               = {This project was funded by the Deanship of Scientific Research (DSR), King Abdulaziz University, Jeddah, under grant No. (DF-350-165-1441). The authors, therefore, gratefully acknowledge DSR technical and financial support.},
  journal-iso                = {IEEE Access},
  keywords                   = {Compounds; Emotion recognition; Autism; Cameras; Feature extraction; Face; Computer vision; Autism; deep spatio-temporal features; meltdown crisis; facial expressions; compound emotions},
  keywords-plus              = {SELECTION TECHNIQUES; EXPRESSIONS},
  language                   = {English},
  number-of-cited-references = {41},
  oa                         = {gold},
  orcid-numbers              = {Kammoun, Salma/0000-0003-1086-6599 Masmoudi, Marwa/0000-0002-5248-8525},
  priority                   = {prio1},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Computer Science; Engineering; Telecommunications},
  researcherid-numbers       = {Kammoun, Salma/N-9090-2014},
  times-cited                = {9},
  type                       = {Article},
  unique-id                  = {WOS:000549823900015},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {7},
  web-of-science-categories  = {Computer Science, Information Systems; Engineering, Electrical \& Electronic; Telecommunications},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{ WOS:000659548600010,
Author = {Song, Peng and Zheng, Wenming and Yu, Yanwei and Ou, Shifeng},
Title = {Speech Emotion Recognition Based on Robust Discriminative Sparse
   Regression},
Journal = {IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS},
Year = {2021},
Volume = {13},
Number = {2},
Pages = {343-353},
Month = {JUN},
Abstract = {Speech emotion recognition has recently attracted much interest due to
   the widespread of multimedia data. It generally involves two basic
   problems: 1) feature extraction and 2) emotion classification. Most
   previous algorithms just focus on solving one of these two problems. In
   this article, we aim to deal with these two problems in a joint learning
   framework, and present a novel regression algorithm, namely, robust
   discriminative sparse regression (RDSR). In RDSR, we propose a sparse
   regression algorithm to make our model robust to outliers and noises,
   and introduce a feature selection regularization constraint
   simultaneously to select the most discriminative and relevant features.
   In addition, to well predict the labels, we exploit the local and global
   consistency over labels, and incorporate it into the proposed framework.
   To solve the objective function of RDSR, we design an efficient
   alternative optimization algorithm. Finally, experimental results on
   several public emotion data sets verify the effectiveness and the
   superiority of our proposed method.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Song, P (Corresponding Author), Yantai Univ, Sch Comp \& Control Engn, Yantai 264005, Peoples R China.
   Zheng, WM (Corresponding Author), Southeast Univ, Res Ctr Learning Sci, Key Lab Child Dev \& Learning Sci, Minist Educ, Nanjing 210096, Peoples R China.
   Song, Peng, Yantai Univ, Sch Comp \& Control Engn, Yantai 264005, Peoples R China.
   Zheng, Wenming, Southeast Univ, Res Ctr Learning Sci, Key Lab Child Dev \& Learning Sci, Minist Educ, Nanjing 210096, Peoples R China.
   Yu, Yanwei, Ocean Univ China, Dept Comp Sci \& Technol, Qingdao 266100, Peoples R China.
   Ou, Shifeng, Yantai Univ, Sch Optoelect Informat Sci \& Technol, Yantai 264005, Peoples R China.},
DOI = {10.1109/TCDS.2020.2990928},
ISSN = {2379-8920},
EISSN = {2379-8939},
Keywords = {Feature extraction; Speech recognition; Emotion recognition; Prediction
   algorithms; Signal processing algorithms; Task analysis; Linear
   regression; Feature selection; graph Laplacian; regression analysis;
   semi-supervised learning; speech emotion recognition},
Keywords-Plus = {FEATURE-SELECTION; LINEAR-REGRESSION; FACE RECOGNITION; NEURAL-NETWORK;
   CLASSIFICATION; FEATURES; SIGNALS},
Research-Areas = {Computer Science; Robotics; Neurosciences \& Neurology},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Robotics; Neurosciences},
Author-Email = {pengsong@ytu.edu.cn
   wenming\_zheng@seu.edu.cn
   yuyanwei0530@gmail.com
   ousfeng@126.com},
ORCID-Numbers = {Yu, Yanwei/0000-0001-6941-2132},
Funding-Acknowledgement = {National Key Research and Development Program of China
   {[}2018YFB1305200]; National Natural Science Foundation of China
   {[}61703360, 61921004, 61773331]; Fundamental Research Funds for the
   Central Universities {[}CDLS-2019-01]},
Funding-Text = {This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFB1305200; in part by the
   National Natural Science Foundation of China under Grant 61703360, Grant
   61921004, and Grant 61773331; and in part by the Fundamental Research
   Funds for the Central Universities under Grant CDLS-2019-01.},
Number-of-Cited-References = {73},
Times-Cited = {4},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {13},
Journal-ISO = {IEEE Trans. Cogn. Dev. Syst.},
Doc-Delivery-Number = {SP3BU},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000659548600010},
DA = {2022-09-28},
}

@InProceedings{WOS:000759178502011,
  author                     = {Washington, Peter and Kline, Aaron and Mutlu, Onur Cezmi and Leblanc, Emilie and Hou, Cathy and Stockham, Nate and Paskov, Kelley and Chrisman, Brianna and Wall, Dennis},
  booktitle                  = {EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21)},
  title                      = {Activity Recognition with Moving Cameras and Few Training Examples: Applications for Detection of Autism-Related Headbanging},
  year                       = {2021},
  address                    = {1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES},
  note                       = {CHI Conference on Human Factors in Computing Systems, ELECTR NETWORK, MAY 08-13, 2021},
  organization               = {ACM SIGCHI; Assoc Comp Machinery; Bloomberg; Facebook; Google; Kyocera; Microsoft; Monash Univ; Verizon Media},
  publisher                  = {ASSOC COMPUTING MACHINERY},
  status                     = {Aceito},
  abstract                   = {Activity recognition computer vision algorithms can be used to detect
   the presence of autism-related behaviors, including what are termed
   ``restricted and repetitive behaviors{''}, or stimming, by diagnostic
   instruments. Examples of stimming include hand flapping, spinning, and
   head banging. One of the most significant bottlenecks for implementing
   such classifiers is the lack of sufficiently large training sets of
   human behavior specific to pediatric developmental delays. The data that
   do exist are usually recorded with a handheld camera which is itself
   shaky or even moving, posing a challenge for traditional feature
   representation approaches for activity detection which capture the
   camera's motion as a feature. To address these issues, we first document
   the advantages and limitations of current feature representation
   techniques for activity recognition when applied to head banging
   detection. We then propose a feature representation consisting
   exclusively of head pose keypoints. We create a computer vision
   classifier for detecting head banging in home videos using a
   time-distributed convolutional neural network (CNN) in which a single
   CNN extracts features from each frame in the input sequence, and these
   extracted features are fed as input to a long short-term memory (LSTM)
   network. On the binary task of predicting head banging and no head
   banging within videos from the Self Stimulatory Behaviour Dataset
   (SSBD), we reach a mean F1-score of 90.77\% using 3-fold cross
   validation (with individual fold F1-scores of 83.3\%, 89.0\%, and
   100.0\%) when ensuring that no child who appeared in the train set was
   in the test set for all folds. This work documents a successful process
   for training a computer vision classifier which can detect a particular
   human motion pattern with few training examples and even when the camera
   recording the source clip is unstable. The process of engineering useful
   feature representations by visually inspecting the representations, as
   described here, can be a useful practice by designers and developers of
   interactive systems detecting human motion patterns for use in mobile
   and ubiquitous interactive systems.},
  affiliation                = {Washington, P (Corresponding Author), Stanford Univ, Stanford, CA 94305 USA. Washington, Peter; Kline, Aaron; Mutlu, Onur Cezmi; Leblanc, Emilie; Hou, Cathy; Stockham, Nate; Paskov, Kelley; Chrisman, Brianna; Wall, Dennis, Stanford Univ, Stanford, CA 94305 USA.},
  author-email               = {peterwashington@stanford.edu akline@stanford.edu cezmi@stanford.edu eleblanc@stanford.edu cathyhou@stanford.edu stockham@stanford.edu kpaskov@stanford.edu briannac@stanford.edu dpwall@stanford.edu},
  book-group-author          = {ACM},
  da                         = {2022-09-28},
  doc-delivery-number        = {BS7DM},
  doi                        = {10.1145/3411763.3451701},
  funding-acknowledgement    = {National Institutes of Health {[}1R01EB025025-01, 1R21HD091500-01, 1R01LM013083]; National Science Foundation {[}2014232]; Hartwell Foundation; Bill and Melinda Gates Foundation; Coulter Foundation; Weston Havens Foundation; Lucile Packard Foundation; Stanford's Human Centered Artifcial Intelligence Program; Stanford's Precision Health and Integrated Diagnostics Center (PHIND); Stanford's Beckman Center; Stanford's Bio-X Center; Predictives and Diagnostics Accelerator (SPADA) Spectrum; Stanford's Spark Program in Translational Research; Stanford's Wu Tsai Neurosciences Institute's Neuroscience: Translate Program; Stanford Interdisciplinary Graduate Fellowship (SIGF)},
  funding-text               = {This work was supported in part by funds to DPW from the National Institutes of Health (1R01EB025025-01, 1R21HD091500-01, 1R01LM013083), the National Science Foundation (Award 2014232), The Hartwell Foundation, Bill and Melinda Gates Foundation, Coulter Foundation, Lucile Packard Foundation, the Weston Havens Foundation, and program grants from Stanford's Human Centered Artifcial Intelligence Program, Stanford's Precision Health and Integrated Diagnostics Center (PHIND), Stanford's Beckman Center, Stanford's Bio-X Center, Predictives and Diagnostics Accelerator (SPADA) Spectrum, Stanford's Spark Program in Translational Research, and from Stanford's Wu Tsai Neurosciences Institute's Neuroscience: Translate Program. We also acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer and Peter Sullivan. P.W. would like to acknowledge support from Mr. Schroeder and the Stanford Interdisciplinary Graduate Fellowship (SIGF) as the Schroeder Family Goldman Sachs Graduate Fellow.},
  isbn                       = {978-1-4503-8095-9},
  keywords                   = {Activity recognition; repetitive motions; motion detection; autism; machine learning},
  keywords-plus              = {FACIAL EMOTION; FEASIBILITY; CHILDREN},
  language                   = {English},
  number-of-cited-references = {73},
  oa                         = {Bronze, Green Submitted},
  orcid-numbers              = {washington, peter/0000-0003-3276-4411 Leblanc, Emilie/0000-0002-3492-3554},
  priority                   = {prio2},
  research-areas             = {Computer Science},
  times-cited                = {4},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000759178502011},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {2},
  web-of-science-categories  = {Computer Science, Cybernetics; Computer Science, Interdisciplinary Applications; Computer Science, Theory \& Methods},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@InProceedings{WOS:000503454900009,
  author                     = {Alshamsi, Humaid and VetonKepuska and Alshamsi, Hazza and Meng, Hongying},
  booktitle                  = {2018 9TH IEEE ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS \& MOBILE COMMUNICATION CONFERENCE (UEMCON)},
  title                      = {Automated Speech Emotion Recognition on Smart Phones},
  year                       = {2018},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  editor                     = {Chakrabarti, S and Saha, HN},
  note                       = {9th Annual IEEE Ubiquitous Computing, Electronics and Mobile Communication Conference (UEMCON), New York, NY, NOV 08-10, 2018},
  organization               = {IEEE; Columbia Univ; Int Engn \& Management; IEEE Reg 1; IEEE USA; IEEE New York Sect; Univ Engn \& Management},
  pages                      = {44-50},
  publisher                  = {IEEE},
  status                     = {Rejeitado - Escopo},
  abstract                   = {The emergence of Speech Emotion Recognition (SER) as a focal point of
   research into speech processing reflects its significance in the field
   of Human-Computer Interaction (HCI). It is core required functionality
   for a variety of applications and a high degree of accuracy is critical;
   activities as diverse as evaluating levels of emotion in children in
   care and measuring customer satisfaction. The extent of demand for
   accurate SER is reflected in the significant number of papers that have
   been published and studies performed. An innovative approach to speech
   recognition is presented in this paper centered on a cloud model
   alongside the conventional system for measuring emotion in speech. There
   are multiple stages involved in detecting and identifying emotions in
   speech from audio clips. The initial pre-processing stage detects the
   speech in the audio file and applies noise reduction. Next, the system
   uses Mel-frequency cepstral coefficient (MFCC) algorithms to extract
   features.This process results in the creation of testing and training
   datasets populated with the emotions: Neutral, Happiness, Sadness, Fear,
   Surprise, Disgust and Anger. The classification stage utilizes Support
   Vector Machine (SVM) classifiers to identify the emotion. An additional
   step implements a Confusion Matrix (CM) method to assess how these
   classifiers performed. Testing was executed against RAVDESS and SAVEE
   databases, where the detection rate achieved against the RAVDESS
   database was 95.3\%.},
  affiliation                = {Alshamsi, H (Corresponding Author), Florida Inst Technol, Dept Elect \& Comp Engn, Melbourne, FL 32901 USA. Alshamsi, Humaid; VetonKepuska; Alshamsi, Hazza; Meng, Hongying, Florida Inst Technol, Dept Elect \& Comp Engn, Melbourne, FL 32901 USA. Meng, Hongying, Brunel Univ London, Dept Elect \& Comp Engn, London UB8 3PH, England.},
  da                         = {2022-09-28},
  doc-delivery-number        = {BO1ZG},
  isbn                       = {978-1-5386-7693-6},
  keywords                   = {Speech Processing; Mel-frequency cepstral coefficient; Speech Emotion Recognition; Mobile Computing},
  keywords-plus              = {OF-THE-LITERATURE; FEATURES},
  language                   = {English},
  number-of-cited-references = {25},
  orcid-numbers              = {Meng, Hongying/0000-0002-8836-1382},
  research-areas             = {Computer Science; Engineering; Telecommunications},
  researcherid-numbers       = {Meng, Hongying/O-5192-2014},
  times-cited                = {4},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000503454900009},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {0},
  web-of-science-categories  = {Computer Science, Theory \& Methods; Engineering, Electrical \& Electronic; Telecommunications},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{ WOS:000766468200002,
Author = {Na, Wang and Yong, Fang},
Title = {Music Recognition and Classification Algorithm considering Audio Emotion},
Journal = {SCIENTIFIC PROGRAMMING},
Year = {2022},
Volume = {2022},
Month = {JAN 20},
Abstract = {At present, the existing music classification and recognition algorithms
   have the problem of low accuracy. Therefore, this paper proposes a music
   recognition and classification algorithm considering the characteristics
   of audio emotion. Firstly, the emotional features of music are extracted
   from the feedforward neural network and parameterized with the mean
   square deviation. Gradient descent learning algorithm is used to train
   audio emotion features. The neural network models of input layer, output
   layer, and hidden layer are established to realize the classification
   and recognition of music emotion. Experimental results show that the
   algorithm has good effect on music emotion classification. The data
   stream driven by the algorithm is higher than 55 MBbs, the anti-attack
   ability is 91\%, the data integrity is 83\%, the average accuracy is
   85\%, and it has good effectiveness and feasibility.},
Publisher = {HINDAWI LTD},
Address = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Yong, F (Corresponding Author), Beihua Univ, Sch Mus, Jilin 132013, Jilin, Peoples R China.
   Na, Wang; Yong, Fang, Beihua Univ, Sch Mus, Jilin 132013, Jilin, Peoples R China.},
DOI = {10.1155/2022/3138851},
Article-Number = {3138851},
ISSN = {1058-9244},
EISSN = {1875-919X},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Software Engineering},
Author-Email = {wn\_0523@126.com
   jl\_fy2855@beihua.edu.cn},
Funding-Acknowledgement = {Jilin Province Department of Education 13th Five-Year Plan, ``A research
   on the Application of Music,erapy in Rehabilitation Education of Special
   Children{''} {[}JJKH20180407SK]; Jilin Province Department of Education
   13th Five-Year Plan, ``,e Exploration and Research on Vocal Music
   Teaching Mode in Normal Universities in the New Era{''}
   {[}JJKH20170090SK]},
Funding-Text = {,is work was supported by Jilin Province Department of Education 13th
   Five-Year Plan, ``A research on the Application of Music,erapy in
   Rehabilitation Education of Special Children{''} (JJKH20180407SK) and
   Jilin Province Department of Education 13th Five-Year Plan, ``,e
   Exploration and Research on Vocal Music Teaching Mode in Normal
   Universities in the New Era{''} (JJKH20170090SK).},
Number-of-Cited-References = {33},
Times-Cited = {1},
Usage-Count-Last-180-days = {16},
Usage-Count-Since-2013 = {16},
Journal-ISO = {Sci. Program.},
Doc-Delivery-Number = {ZP5ND},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000766468200002},
OA = {gold},
DA = {2022-09-28},
}

@Article{WOS:000477906900017,
  author                     = {Wei, Pengcheng and Zhao, Yu},
  journal                    = {PERSONAL AND UBIQUITOUS COMPUTING},
  title                      = {A novel speech emotion recognition algorithm based on wavelet kernel sparse classifier in stacked deep auto-encoder model},
  year                       = {2019},
  issn                       = {1617-4909},
  month                      = {JUL},
  number                     = {3-4, SI},
  pages                      = {521-529},
  volume                     = {23},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Since the contextual information has an important impact on the
   speaker's emotional state, how to use emotion-related context
   information to conduct feature learning is a key problem. The existing
   speech emotion recognition algorithms achieve the relatively high
   recognition rate; these algorithms are not very good application to the
   real-life speech emotion recognition systems. Therefore, in order to
   address the abovementioned issues, a novel speech emotion recognition
   algorithm based on improved stacked kernel sparse deep model is proposed
   in this paper, which is based on auto-encoder, denoising auto-encoder,
   and sparse auto-encoder to improve the Chinese speech emotion
   recognition. The first layer of the structure uses a denoising
   auto-encoder to learn a hidden feature with a larger dimension than the
   dimension of the input features, and the second layer employs a sparse
   auto-encoder to learn sparse features. Finally, a wavelet-kernel sparse
   SVM classifier is applied to classify the features. The proposed
   algorithm is evaluated on the testing dataset, which contains the speech
   emotion data of spontaneous, non-prototypical, and long-term. The
   experimental results show that the proposed algorithm outperforms the
   existing state-of-the-art algorithms in speech emotion recognition.},
  address                    = {236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND},
  affiliation                = {Wei, PC (Corresponding Author), Chongqing Univ Educ, Sch Math \& Informat Engn, Chongqing, Peoples R China. Wei, Pengcheng; Zhao, Yu, Chongqing Univ Educ, Sch Math \& Informat Engn, Chongqing, Peoples R China.},
  author-email               = {wpc75@163.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {IM3PJ},
  doi                        = {10.1007/s00779-019-01246-9},
  eissn                      = {1617-4917},
  funding-acknowledgement    = {Chongqing Big Data Engineering Laboratory for Children, Chongqing Electronics Engineering Technology Research Center for Interactive Learning; Project of Science and Technology Research Program of Chongqing Education Commission of China {[}KJZD-K201801601]},
  funding-text               = {This work was supported by Chongqing Big Data Engineering Laboratory for Children, Chongqing Electronics Engineering Technology Research Center for Interactive Learning, and Project of Science and Technology Research Program of Chongqing Education Commission of China (No. KJZD-K201801601).},
  journal-iso                = {Pers. Ubiquitous Comput.},
  keywords                   = {Contextual information; Emotion recognition; Auto-encoder; Kernel sparse; Sub-utterance-level; Support vector machine; Hidden feature; Deep learning},
  keywords-plus              = {SELECTION},
  language                   = {English},
  number-of-cited-references = {30},
  publisher                  = {SPRINGER LONDON LTD},
  research-areas             = {Computer Science; Telecommunications},
  times-cited                = {12},
  type                       = {Article},
  unique-id                  = {WOS:000477906900017},
  usage-count-last-180-days  = {4},
  usage-count-since-2013     = {21},
  web-of-science-categories  = {Computer Science, Information Systems; Telecommunications},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@InProceedings{WOS:000518657800086,
  author                     = {Li, Sunan and Zheng, Wenming and Zong, Yuan and Lu, Cheng and Tang, Chuangao and Jiang, Xingxun and Liu, Jiateng and Xia, Wanchuang},
  booktitle                  = {ICMI'19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION},
  title                      = {Bi-modality Fusion for Emotion Recognition in the Wild},
  year                       = {2019},
  address                    = {1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES},
  note                       = {21st ACM International Conference on Multimodal Interaction (ICMI), Suzhou, PEOPLES R CHINA, OCT 14-18, 2019},
  organization               = {ACM SIGCHI; Assoc Comp Machinery; Openstream; Alibaba Grp; Microsoft; Baidu; Sensetime; Tencent YouTu Lab; AISpeech},
  pages                      = {589-594},
  publisher                  = {ASSOC COMPUTING MACHINERY},
  status                     = {Aceito},
  abstract                   = {The emotion recognition in the wild has been a hot research topic in the
   field of affective computing. Though some progresses have been achieved,
   the emotion recognition in the wild is still an unsolved problem due to
   the challenge of head movement, face deformation, illumination variation
   etc. To deal with these unconstrained challenges, we propose a
   bimodality fusion method for video based emotion recognition in the
   wild. The proposed framework takes advantages of the visual information
   from facial expression sequences and the speech information from audio.
   The state-of-the-art CNN based object recognition models are employed to
   facilitate the facial expression recognition performance. A bi-direction
   long short term Memory (Bi-LSTM) is employed to capture dynamic
   information of the learned features. Additionally, to take full
   advantages of the facial expression information, the VGG16 network is
   trained on AffectNet dataset to learn a specialized facial expression
   recognition model. On the other hand, the audio based features, like low
   level descriptor (LLD) and deep features obtained by spectrogram image,
   are also developed to improve the emotion recognition performance. The
   best experimental result shows that the overall accuracy of our
   algorithm on the Test dataset of the EmotiW challenge is 62.78\%, which
   outperforms the best result of EmotiW2018 and ranks 2nd at the
   EmotiW2019 challenge.},
  affiliation                = {Li, SN (Corresponding Author), Southeast Univ, Sch Informat Sci \& Engn, Nanjing, Peoples R China. Li, Sunan; Lu, Cheng, Southeast Univ, Sch Informat Sci \& Engn, Nanjing, Peoples R China. Zheng, Wenming, Southeast Univ, Sch Biol Sci \& Med Engn, Minist Educ, Key Lab Child Dev \& Learning Sci, Nanjing, Peoples R China. Zong, Yuan; Tang, Chuangao; Jiang, Xingxun; Liu, Jiateng, Southeast Univ, Sch Biol Sci \& Med Engn, Nanjing, Peoples R China. Xia, Wanchuang, Southeast Univ, Sch Cyber Sci \& Engn, Nanjing, Peoples R China.},
  author-email               = {230189473@seu.edu.cn wenming\_zheng@seu.edu.cn xhzongyuan@seu.edu.cn cheng.lu@seu.edu.cn tcg2016@seu.edu.cn jiangxingxun@seu.edu.cn jiangxingxun@seu.edu.cn 220184474@seu.edu.cn},
  book-group-author          = {Assoc Comp Machinery},
  da                         = {2022-09-28},
  doc-delivery-number        = {BO5QQ},
  doi                        = {10.1145/3340555.3355719},
  funding-acknowledgement    = {National Key Research and Development Program of China {[}2018YFB1305200]; National Basic Research Program of China {[}2015CB351704]; National Natural Science Foundation of China {[}61572009]; Fundamental Research Funds for the Central Universities {[}2242018K3DN01, 2242019K40047]},
  funding-text               = {This work was supported in part by the National Key Research and Development Program of China under Grant 2018YFB1305200, in part by the National Basic Research Program of China under Grant 2015CB351704, in part by the National Natural Science Foundation of China under Grant 61572009, and in part by the Fundamental Research Funds for the Central Universities under Grant 2242018K3DN01 and Grant 2242019K40047.},
  isbn                       = {978-1-4503-6860-5},
  keywords                   = {Emotion Recognition; Deep Learning; Convolutional Neural Networks},
  language                   = {English},
  number-of-cited-references = {23},
  orcid-numbers              = {Tang, Chuangao/0000-0002-3653-136X},
  priority                   = {prio3},
  research-areas             = {Computer Science},
  researcherid-numbers       = {Jiang, Xingxun/AAR-5525-2020 Jiang, Xingxun/CAF-1398-2022},
  times-cited                = {14},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000518657800086},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {3},
  web-of-science-categories  = {Computer Science, Theory \& Methods},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000463786600008,
  author                     = {Li, Yang and Zheng, Wenming and Cui, Zhen and Zong, Yuan and Ge, Sheng},
  journal                    = {NEURAL PROCESSING LETTERS},
  title                      = {EEG Emotion Recognition Based on Graph Regularized Sparse Linear Regression},
  year                       = {2019},
  issn                       = {1370-4621},
  month                      = {APR},
  number                     = {2},
  pages                      = {555-571},
  volume                     = {49},
  status                     = {Rejeitado - Escopo},
  abstract                   = {In this paper, a novel regression model, called graph regularized sparse
   linear regression (GRSLR), is proposed to deal with EEG emotion
   recognition problem. GRSLR extends the conventional linear regression
   method by imposing a graph regularization and a sparse regularization on
   the transform matrix of linear regression, such that it is able to
   simultaneously cope with sparse transform matrix learning while preserve
   the intrinsic manifold of the data samples. To detailed discuss the EEG
   emotion recognition, we collect a set of 14 subjects EEG emotion data
   and provide the experiment results on different features. To evaluate
   the proposed GRSLR model, we conduct experiments on the SEED database
   and RCLS database. The experimental results show that the proposed
   algorithm GRSLR is superior to the classic baselines. The RCLS database
   is made publicly available and other researchers could use it to test
   their own emotion recognition method.},
  address                    = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  affiliation                = {Zheng, WM (Corresponding Author), Southeast Univ, Res Ctr Learning Sci, Minist Educ, Key Lab Child Dev \& Learning Sci, Nanjing 210096, Jiangsu, Peoples R China. Li, Yang; Zheng, Wenming; Zong, Yuan; Ge, Sheng, Southeast Univ, Res Ctr Learning Sci, Minist Educ, Key Lab Child Dev \& Learning Sci, Nanjing 210096, Jiangsu, Peoples R China. Li, Yang, Southeast Univ, Sch Informat Sci \& Engn, Nanjing 210096, Jiangsu, Peoples R China. Cui, Zhen, Nanjing Univ Sci \& Technol, Sch Comp Sci \& Engn, Nanjing 210096, Jiangsu, Peoples R China.},
  author-email               = {wenming\_zheng@seu.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {HS3TY},
  doi                        = {10.1007/s11063-018-9829-1},
  eissn                      = {1573-773X},
  funding-acknowledgement    = {National Basic Research Program of China {[}2015CB351704]; National Natural Science Foundation of China {[}61572009, 61772276, 61602244]; Jiangsu Provincial Key Research and Development Program {[}BE2016616]},
  funding-text               = {This work was supported by the National Basic Research Program of China under Grant 2015CB351704, the National Natural Science Foundation of China under Grant 61572009, Grant 61772276, and Grant 61602244, and the Jiangsu Provincial Key Research and Development Program under Grant BE2016616.},
  journal-iso                = {Neural Process. Lett.},
  keywords                   = {EEG; Emotion recognition; Sparse linear regression},
  keywords-plus              = {FEATURE-EXTRACTION},
  language                   = {English},
  number-of-cited-references = {32},
  orcid-numbers              = {Ge, Zongyuan/0000-0002-5880-8673},
  publisher                  = {SPRINGER},
  research-areas             = {Computer Science},
  times-cited                = {31},
  type                       = {Article},
  unique-id                  = {WOS:000463786600008},
  usage-count-last-180-days  = {6},
  usage-count-since-2013     = {21},
  web-of-science-categories  = {Computer Science, Artificial Intelligence},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000442778600006,
  author                     = {Ye, Liang and Wang, Peng and Wang, Le and Ferdinando, Hany and Seppanen, Tapio and Alasaarela, Esko},
  journal                    = {INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE},
  title                      = {A Combined Motion-Audio School Bullying Detection Algorithm},
  year                       = {2018},
  issn                       = {0218-0014},
  month                      = {DEC},
  number                     = {12},
  volume                     = {32},
  status                     = {Rejeitado - Escopo},
  abstract                   = {School bullying is a common social problem, which affects children both
   mentally and physically, making the prevention of bullying a timeless
   topic all over the world. This paper proposes a method for detecting
   bullying in school based on activity recognition and speech emotion
   recognition. In this method, motion and voice data are gathered by
   movement sensors and a microphone, followed by extraction of a set of
   motion and audio features to distinguish bullying incidents from daily
   life events. Among extracted motion features are both time-domain and
   frequency-domain features, while audio features are computed with
   classical MFCCs. Feature selection is implemented using the wrapper
   approach. At the next stage, these motion and audio features are merged
   to form combined feature vectors for classification, and LDA is used for
   further dimension reduction. A BPNN is trained to recognize bullying
   activities and distinguish them from normal daily life activities. The
   authors also propose an action transition detection method to reduce
   computational complexity for practical use. Thus, the bullying detection
   algorithm will only run, when an action transition event has been
   detected. Simulation results show that the combined motion-audio feature
   vector outperforms separate motion features and acoustic features,
   achieving an accuracy of 82.4\% and a precision of 92.2\%. Moreover,
   with the action transition method, the computation cost can be reduced
   by half.},
  address                    = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
  affiliation                = {Ye, L (Corresponding Author), Harbin Inst Technol, Dept Informat \& Commun Engn, 2 Yikuang St, Harbin 150080, Heilongjiang, Peoples R China. Ye, L (Corresponding Author), Univ Oulu, OPEM Unit, Hlth \& Wellness Measurement Res Grp, Pentti Kaiteran Katu 1, Oulu 90014, Finland. Ye, Liang; Wang, Le, Harbin Inst Technol, Dept Informat \& Commun Engn, 2 Yikuang St, Harbin 150080, Heilongjiang, Peoples R China. Ye, Liang; Ferdinando, Hany; Alasaarela, Esko, Univ Oulu, OPEM Unit, Hlth \& Wellness Measurement Res Grp, Pentti Kaiteran Katu 1, Oulu 90014, Finland. Wang, Peng, China Elect Technol Grp Corp, 8 Guorui Rd, Nanjing 210012, Jiangsu, Peoples R China. Ferdinando, Hany, Petra Christian Univ, Dept Elect Engn, Siwalankerto 121-131, Surabaya 60236, Indonesia. Seppanen, Tapio, Univ Oulu, Physiol Signal Anal Team, Pentti Kaiteran Katu 1, Oulu 90014, Finland.},
  article-number             = {1850046},
  author-email               = {yeliang@hit.edu.cn wphitstudent@163.com 1659412561@qq.com hferdina@ee.oulu.fi tapio@ee.oulu.fi esko.alasaarela@ee.oulu.fi},
  da                         = {2022-09-28},
  doc-delivery-number        = {GR6PM},
  doi                        = {10.1142/S0218001418500465},
  eissn                      = {1793-6381},
  funding-acknowledgement    = {National Natural Science Foundation of China {[}61602127]; North Ostrobothnia Regional Fund of the Finnish Cultural Foundation; {[}2142/E4.4/K/2013]},
  funding-text               = {This work was supported by the National Natural Science Foundation of China (61602127), with significant contributions by the Directorate General of Higher Education, Indonesia (2142/E4.4/K/2013) and by the North Ostrobothnia Regional Fund of the Finnish Cultural Foundation. The authors would like to thank Tuija Huuki and Vappu Sunnari (University of Oulu, Finland) for educational and psychological guidance in the school bullying experiments, teachers Taina Aalto and Pekka Kurttila and principal Maija Laukka (Oulunlahti School, Finland) for arranging the experiments and pupils in the 2nd and 6th grades of Oulunlahti School for acting in the experiments, Seppo Laukka and Antti Siipo (University of Oulu, Finland) for hardware and technical support during the experiments, Tian Han, Zhu Zhang (Harbin University of Science and Technology, China) for assisting the experiments, and Yubo Zhang, Jifu Shi and Zhi Xun (Harbin Institute of Technology, China) for discussion about feature selection.},
  journal-iso                = {Int. J. Pattern Recognit. Artif. Intell.},
  keywords                   = {Activity recognition; speech emotion recognition; movement sensors; school bullying; pattern recognition},
  keywords-plus              = {SPEECH EMOTION RECOGNITION},
  language                   = {English},
  number-of-cited-references = {31},
  oa                         = {Green Accepted},
  publisher                  = {WORLD SCIENTIFIC PUBL CO PTE LTD},
  research-areas             = {Computer Science},
  times-cited                = {9},
  type                       = {Article},
  unique-id                  = {WOS:000442778600006},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {73},
  web-of-science-categories  = {Computer Science, Artificial Intelligence},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000488323400008,
  author                     = {Kalantarian, Haik and Jedoui, Khaled and Washington, Peter and Tariq, Qandeel and Dunlap, Kaiti and Schwartz, Jessey and Wall, Dennis P.},
  journal                    = {ARTIFICIAL INTELLIGENCE IN MEDICINE},
  title                      = {Labeling images with facial emotion and the potential for pediatric healthcare},
  year                       = {2019},
  issn                       = {0933-3657},
  month                      = {JUL},
  pages                      = {77-86},
  volume                     = {98},
  status                     = {Aceito},
  abstract                   = {Autism spectrum disorder (ASD) is a neurodevelopmental disorder
   characterized by repetitive behaviors, narrow interests, and deficits in
   social interaction and communication ability. An increasing emphasis is
   being placed on the development of innovative digital and mobile systems
   for their potential in therapeutic applications outside of clinical
   environments. Due to recent advances in the field of computer vision,
   various emotion classifiers have been developed, which have potential to
   play a significant role in mobile screening and therapy for
   developmental delays that impair emotion recognition and expression.
   However, these classifiers are trained on datasets of predominantly
   neurotypical adults and can sometimes fail to generalize to children
   with autism. The need to improve existing classifiers and develop new
   systems that overcome these limitations necessitates novel methods to
   crowdsource labeled emotion data from children. In this paper, we
   present a mobile charades-style game, Guess What?, from which we derive
   egocentric video with a high density of varied emotion from a 90-second
   game session. We then present a framework for semi-automatic labeled
   frame extraction from these videos using meta information from the game
   session coupled with classification confidence scores. Results show that
   94\%, 81\%, 92\%, and 56\% of frames were automatically labeled
   correctly for categories disgust, neutral, surprise, and scared
   respectively, though performance for angry and happy did not improve
   significantly from the baseline.},
  address                    = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  affiliation                = {Wall, DP (Corresponding Author), Stanford Univ, Dept Pediat \& Biomed Data Sci, Stanford, CA 94305 USA. Kalantarian, Haik; Tariq, Qandeel; Dunlap, Kaiti; Schwartz, Jessey; Wall, Dennis P., Stanford Univ, Dept Pediat, Stanford, CA 94305 USA. Kalantarian, Haik; Tariq, Qandeel; Dunlap, Kaiti; Schwartz, Jessey; Wall, Dennis P., Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA. Jedoui, Khaled, Stanford Univ, Dept Math, Stanford, CA 94305 USA. Washington, Peter, Stanford Univ, Dept Bioengn, Stanford, CA 94305 USA.},
  author-email               = {haik@stanford.edu thekej@stanford.edu peter100@stanford.edu qandeel@stanford.edu kaiti.dunlap@stanford.edu jesseys@stanford.edu dpwall@stanford.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {JB1MA},
  doi                        = {10.1016/j.artmed.2019.06.004},
  eissn                      = {1873-2860},
  funding-acknowledgement    = {NIH {[}1R01EB025025-01, 1R21HD091500-01, R01LM013083-01]; Hartwell Foundation; Bill and Melinda Gates Foundation; Coulter Foundation; Lucile Packard Foundation; Stanford Human Centered Artificial Intelligence Program; Stanford Beckman Center; Stanford Bio-X Center; Predictives and Diagnostics Accelerator (SPADA) Spectrum; Spark Program in Translational Research; Wu Tsai Neurosciences Institute Neuroscience:Translate Program; Thrasher Research Fund; Stanford NLM Clinical Data Science program {[}T-15LM007033-35]; Stanford Precision Health and Integrated Diagnostics Center (PHIND); EUNICE KENNEDY SHRIVER NATIONAL INSTITUTE OF CHILD HEALTH \& HUMAN DEVELOPMENT {[}R21HD091500] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF BIOMEDICAL IMAGING AND BIOENGINEERING {[}R01EB025025] Funding Source: NIH RePORTER; NATIONAL LIBRARY OF MEDICINE {[}R01LM013083, T15LM007033] Funding Source: NIH RePORTER},
  funding-text               = {The work was supported in part by funds to DPW from NIH (1R01EB025025-01, 1R21HD091500-01, and R01LM013083-01), The Hartwell Foundation, Bill and Melinda Gates Foundation, Coulter Foundation, Lucile Packard Foundation, Stanford Human Centered Artificial Intelligence Program, Stanford Precision Health and Integrated Diagnostics Center (PHIND), Stanford Beckman Center, Stanford Bio-X Center, the Predictives and Diagnostics Accelerator (SPADA) Spectrum, the Spark Program in Translational Research, and from the Wu Tsai Neurosciences Institute Neuroscience:Translate Program. We also acknowledge the support of Peter Sullivan. Haik Kalantarian would like to acknowledge support from the Thrasher Research Fund and Stanford NLM Clinical Data Science program (T-15LM007033-35).},
  journal-iso                = {Artif. Intell. Med.},
  keywords                   = {Mobile games; Computer vision; Autism; Emotion; Emotion classification},
  keywords-plus              = {EARLY BEHAVIORAL INTERVENTION; AUTISM; CHILDREN; EXPRESSIONS; DEVICE},
  language                   = {English},
  number-of-cited-references = {42},
  oa                         = {Green Published, hybrid},
  orcid-numbers              = {washington, peter/0000-0003-3276-4411},
  priority                   = {prio1},
  publisher                  = {ELSEVIER},
  research-areas             = {Computer Science; Engineering; Medical Informatics},
  times-cited                = {26},
  type                       = {Article},
  unique-id                  = {WOS:000488323400008},
  usage-count-last-180-days  = {4},
  usage-count-since-2013     = {10},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Engineering, Biomedical; Medical Informatics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@article{ WOS:000527464200001,
Author = {Nag, Anish and Haber, Nick and Voss, Catalin and Tamura, Serena and
   Daniels, Jena and Ma, Jeffrey and Chiang, Bryan and Ramachandran, Shasta
   and Schwartz, Jessey and Winograd, Terry and Feinstein, Carl and Wall,
   Dennis P.},
Title = {Toward Continuous Social Phenotyping: Analyzing Gaze Patterns in an
   Emotion Recognition Task for Children With Autism Through Wearable Smart
   Glasses},
Journal = {JOURNAL OF MEDICAL INTERNET RESEARCH},
Year = {2020},
Volume = {22},
Number = {4},
Month = {APR 22},
Abstract = {Background: Several studies have shown that facial attention differs in
   children with autism. Measuring eye gaze and emotion recognition in
   children with autism is challenging, as standard clinical assessments
   must be delivered in clinical settings by a trained clinician. Wearable
   technologies may be able to bring eye gaze and emotion recognition into
   natural social interactions and settings.
   Objective: This study aimed to test: (1) the feasibility of tracking
   gaze using wearable smart glasses during a facial expression recognition
   task and (2) the ability of these gaze-tracking data, together with
   facial expression recognition responses, to distinguish children with
   autism from neurotypical controls (NCs).
   Methods: We compared the eye gaze and emotion recognition patterns of 16
   children with autism spectrum disorder (ASD) and 17 children without ASD
   via wearable smart glasses fitted with a custom eye tracker. Children
   identified static facial expressions of images presented on a computer
   screen along with nonsocial distractors while wearing Google Glass and
   the eye tracker. Faces were presented in three trials, during one of
   which children received feedback in the form of the correct
   classification. We employed hybrid human-labeling and computer
   vision-enabled methods for pupil tracking and world-gaze translation
   calibration. We analyzed the impact of gaze and emotion recognition
   features in a prediction task aiming to distinguish children with ASD
   from NC participants.
   Results: Gaze and emotion recognition patterns enabled the training of a
   classifier that distinguished ASD and NC groups. However, it was unable
   to significantly outperform other classifiers that used only age and
   gender features, suggesting that further work is necessary to
   disentangle these effects.
   Conclusions: Although wearable smart glasses show promise in identifying
   subtle differences in gaze tracking and emotion recognition patterns in
   children with and without ASD, the present form factor and data do not
   allow for these differences to be reliably exploited by machine learning
   systems. Resolving these challenges will be an important step toward
   continuous tracking of the ASD phenotype.},
Publisher = {JMIR PUBLICATIONS, INC},
Address = {130 QUEENS QUAY E, STE 1102, TORONTO, ON M5A 0P6, CANADA},
Type = {Article},
Language = {English},
Affiliation = {Wall, DP (Corresponding Author), Stanford Univ, Dept Pediat, 1265 Welch Rd, Stanford, CA 94305 USA.
   Wall, DP (Corresponding Author), Stanford Univ, Dept Biomed Data Sci, 1265 Welch Rd, Stanford, CA 94305 USA.
   Wall, DP (Corresponding Author), Stanford Univ, Dept Psychiat \& Behav Sci, 1265 Welch Rd, Stanford, CA 94305 USA.
   Nag, Anish, Univ Calif Berkeley, Berkeley, CA 94720 USA.
   Haber, Nick, Stanford Univ, Grad Sch Educ, Stanford, CA 94305 USA.
   Voss, Catalin; Winograd, Terry, Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   Tamura, Serena, UC San Francisco, San Francisco, CA USA.
   Daniels, Jena, Medable Inc, Palo Alto, CA USA.
   Ma, Jeffrey; Chiang, Bryan; Ramachandran, Shasta; Schwartz, Jessey; Feinstein, Carl; Wall, Dennis P., Stanford Univ, Dept Pediat, 1265 Welch Rd, Stanford, CA 94305 USA.
   Ma, Jeffrey; Chiang, Bryan; Ramachandran, Shasta; Schwartz, Jessey; Feinstein, Carl; Wall, Dennis P., Stanford Univ, Dept Biomed Data Sci, 1265 Welch Rd, Stanford, CA 94305 USA.
   Ma, Jeffrey; Chiang, Bryan; Ramachandran, Shasta; Schwartz, Jessey; Feinstein, Carl; Wall, Dennis P., Stanford Univ, Dept Psychiat \& Behav Sci, 1265 Welch Rd, Stanford, CA 94305 USA.},
DOI = {10.2196/13810},
Article-Number = {e13810},
ISSN = {1438-8871},
Keywords = {autism spectrum disorder; translational medicine; eye tracking; wearable
   technologies; artificial intelligence; machine learning; precision
   health; digital therapy},
Keywords-Plus = {FACIAL AFFECT RECOGNITION; SPECTRUM DISORDERS; FACES; IMPAIRMENT},
Research-Areas = {Health Care Sciences \& Services; Medical Informatics},
Web-of-Science-Categories  = {Health Care Sciences \& Services; Medical Informatics},
Author-Email = {dpwall@stanford.edu},
ORCID-Numbers = {Haber, Nick/0000-0001-8804-7804
   Schwartz, Jessey/0000-0003-4916-7891
   Ma, Jeffrey Jian/0000-0002-3646-3547
   Ramachandran, Shasta/0000-0002-6102-241X
   tamura, serena/0000-0002-3090-2385
   Nag, Anish/0000-0002-0574-3722},
Funding-Acknowledgement = {National Institutes of Health {[}1R01EB025025-01, 1R21HD091500-01];
   Hartwell Foundation; Bill and Melinda Gates Foundation; Coulter
   Foundation; Lucile Packard Foundation; Stanford's Human Centered
   Artificial Intelligence Program; Precision Health and Integrated
   Diagnostics Center; Beckman Center; Bio-X Center; Predictives and
   Diagnostics Accelerator Spectrum,; Spark Program in Translational
   Research; Wu Tsai Neurosciences Institute Neuroscience:Translate Program},
Funding-Text = {The authors would like to sincerely thank their participants, without
   whom this research would not be possible. The work was supported in part
   by funds to DW from National Institutes of Health (1R01EB025025-01 and
   1R21HD091500-01), The Hartwell Foundation, Bill and Melinda Gates
   Foundation, Coulter Foundation, Lucile Packard Foundation, and program
   grants from Stanford's Human Centered Artificial Intelligence Program,
   Precision Health and Integrated Diagnostics Center, Beckman Center,
   Bio-X Center, Predictives and Diagnostics Accelerator Spectrum, Spark
   Program in Translational Research, and from the Wu Tsai Neurosciences
   Institute Neuroscience:Translate Program. They also acknowledge generous
   support from David Orr, Imma Calvo, Bobby Dekesyer, and Peter Sullivan.},
Number-of-Cited-References = {52},
Times-Cited = {13},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {33},
Journal-ISO = {J. Med. Internet Res.},
Doc-Delivery-Number = {LF5MV},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000527464200001},
OA = {Green Published, gold},
DA = {2022-09-28},
}

@Article{WOS:000834208300040,
  author                     = {Reshma, Vennamaneni and Deekshitha, Chatla and Keerthi, Gangarapu and Joy, Buddila Anitha and Chakravarthy, Kanukuntla},
  journal                    = {INTERNATIONAL JOURNAL OF EARLY CHILDHOOD SPECIAL EDUCATION},
  title                      = {Behavior Analysis For Mentally Affected People Through Face Emotion Detection},
  year                       = {2022},
  issn                       = {1308-5581},
  number                     = {5},
  pages                      = {1418-1426},
  volume                     = {14},
  status                     = {Aceito},
  abstract                   = {A person's emotional, psychological, and social well-being could all be
   reflected in their mental health. It determines one's thoughts,
   emotions, and reactions to situations. Working successfully and reaching
   one's full potential requires good mental state. psychological health is
   important at every age, from childhood through adulthood. A number of
   factors, including stress, social anxiety, depression, obsessive
   compulsive disorder, substance abuse, and personality disorders, affect
   one's mental state. It's more important than ever to recognise the signs
   of a mental condition to maintain a healthy sense of balance in your
   life. Additionally, machine learning algorithms and artificial
   intelligence (AI) are prone to fully utilising their capabilities to
   predict the onset of mental states. The Mean Opinion Score was
   frequently used in real-time deployments to validate the labels that
   were acquired as a result of clustering. The classifiers created using
   these cluster labels may be able to predict a person's mental state.
   Target demographics were established for the population, including high
   school students, college students, and dealing professionals. The study
   assesses how the aforementioned machine learning algorithms have
   affected the target demographics and offers suggestions for additional
   investigation.},
  address                    = {INST FINE ARTS, ESKISEHIR, 26470, TURKEY},
  affiliation                = {Reshma, V (Corresponding Author), St Peters Engn Coll, Dept CSE, Hyderabad, TS, India. Reshma, Vennamaneni; Deekshitha, Chatla; Keerthi, Gangarapu; Joy, Buddila Anitha; Chakravarthy, Kanukuntla, St Peters Engn Coll, Dept CSE, Hyderabad, TS, India.},
  author-email               = {reshmarao218@gmail.com deekshithachatla@gmail.com gangarapu.keerthi06@gmail.com anithaanu9572@gmail.com chakravarthy9899@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {3K6TM},
  doi                        = {10.9756/INTJECSE/V14I6.140},
  journal-iso                = {Int. J. Early Child. Spec. Educ.},
  keywords                   = {Behavior Analysis; psychological state; Surveillance tool; Machine learning algorithms; computer science},
  language                   = {English},
  number-of-cited-references = {15},
  priority                   = {prio2},
  publisher                  = {ANADOLU UNIV},
  research-areas             = {Education \& Educational Research},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000834208300040},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {0},
  web-of-science-categories  = {Education, Special},
  web-of-science-index       = {Emerging Sources Citation Index (ESCI)},
}

@Article{WOS:000442238900013,
  author                     = {Sreedharan, Ninu Preetha Nirmala and Ganesan, Brammya and Raveendran, Ramya and Sarala, Praveena and Dennis, Binu and Boothalingam, Rajakumar R.},
  journal                    = {IET BIOMETRICS},
  title                      = {Grey Wolf optimisation-based feature selection and classification for facial emotion recognition},
  year                       = {2018},
  issn                       = {2047-4938},
  month                      = {SEP},
  number                     = {5},
  pages                      = {490-499},
  volume                     = {7},
  status                     = {Aceito},
  abstract                   = {The channels used to convey the human emotions consider actions,
   behaviours, poses, facial expressions, and speech. An immense research
   has been carried out to analyse the relationship between the facial
   emotions and these channels. The goal of this study is to develop a
   system for Facial Emotion Recognition (FER) that can analyse the
   elemental facial expressions of human, such as normal, smile, sad,
   surprise, anger, fear, and disgust. The recognition process of the
   proposed FER system is categorised into four processes, namely
   pre-processing, feature extraction, feature selection, and
   classification. After preprocessing, scale invariant feature transform
   -based feature extraction method is used to extract the features from
   the facial point. Further, a meta-heuristic algorithm called Grey Wolf
   optimisation (GWO) is used to select the optimal features. Subsequently,
   GWO-based neural network (NN) is used to classify the emotions from the
   selected features. Moreover, an effective performance analysis of the
   proposed as well as the conventional methods such as convolutional
   neural network, NN-Levenberg-Marquardt, NN-Gradient Descent,
   NN-Evolutionary Algorithm, NN-firefly, and NN-Particle Swarm
   Optimisation is provided by evaluating few performance measures and
   thereby, the effectiveness of the proposed strategy over the
   conventional methods is validated.},
  address                    = {MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND},
  affiliation                = {Sreedharan, NPN (Corresponding Author), Resbee Info Technol Private Ltd, Thuckalay 629175, India. Sreedharan, Ninu Preetha Nirmala; Ganesan, Brammya; Raveendran, Ramya; Sarala, Praveena; Dennis, Binu; Boothalingam, Rajakumar R., Resbee Info Technol Private Ltd, Thuckalay 629175, India.},
  author-email               = {research@resbee.org},
  da                         = {2022-09-28},
  doc-delivery-number        = {GR0UL},
  doi                        = {10.1049/iet-bmt.2017.0160},
  eissn                      = {2047-4946},
  journal-iso                = {IET Biom.},
  keywords                   = {face recognition; gradient methods; feature extraction; evolutionary computation; particle swarm optimisation; neural nets; optimisation; emotion recognition; Grey Wolf optimisation; feature selection; facial emotion recognition; human emotions; facial emotions; elemental facial expressions; fear; recognition process; FER system; pre-processing; scale invariant feature; feature extraction method; facial point; optimal features; NN-particle swarm optimisation},
  keywords-plus              = {TRAUMATIC BRAIN-INJURY; EXPRESSION RECOGNITION; NEURAL-NETWORKS; INDIVIDUAL-DIFFERENCES; ALGORITHM; SCHIZOPHRENIA; SIFT; ASSOCIATION; CHILDHOOD; DEFICITS},
  language                   = {English},
  number-of-cited-references = {47},
  orcid-numbers              = {R, Rajakumar B/0000-0002-4720-8875 D, Binu/0000-0002-1649-5541},
  priority                   = {prio2},
  publisher                  = {INST ENGINEERING TECHNOLOGY-IET},
  research-areas             = {Computer Science},
  researcherid-numbers       = {R, Rajakumar B/AAJ-6031-2020 D, Binu/AAX-5646-2021},
  times-cited                = {91},
  type                       = {Article},
  unique-id                  = {WOS:000442238900013},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {35},
  web-of-science-categories  = {Computer Science, Artificial Intelligence},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@article{ WOS:000483067100012,
Author = {Martinez, Aleix M.},
Title = {The Promises and Perils of Automated Facial Action Coding in Studying
   Children's Emotions},
Journal = {DEVELOPMENTAL PSYCHOLOGY},
Year = {2019},
Volume = {55},
Number = {9, SI},
Pages = {1965-1981},
Month = {SEP},
Abstract = {Computer vision algorithms have made tremendous advances in recent
   years. We now have algorithms that can detect and recognize objects,
   faces, and even facial actions in still images and video sequences. This
   is wonderful news for researchers that need to code facial articulations
   in large data sets of images and videos, because this task is time
   consuming and can only be completed by expert coders, making it very
   expensive. The availability of computer algorithms that can
   automatically code facial actions in extremely large data sets also
   opens the door to studies in psychology and neuroscience that were not
   previously possible, for example, to study the development of the
   production of facial expressions from infancy to adulthood within and
   across cultures. Unfortunately, there is a lack of methodological
   understanding on how these algorithms should and should not be used, and
   on how to select the most appropriate algorithm for each study. This
   article aims to address this gap in the literature. Specifically, we
   present several methodologies for use in hypothesis-based and
   exploratory studies, explain how to select the computer algorithms that
   best fit to the requirements of our experimental design, and detail how
   to evaluate whether the automatic annotations provided by existing
   algorithms are trustworthy.},
Publisher = {AMER PSYCHOLOGICAL ASSOC},
Address = {750 FIRST ST NE, WASHINGTON, DC 20002-4242 USA},
Type = {Article},
Language = {English},
Affiliation = {Martinez, AM (Corresponding Author), Ohio State Univ, Ctr Cognit \& Brain Sci, 205 Dreese Labs,2015 Neil Ave, Columbus, OH 43210 USA.
   Martinez, AM (Corresponding Author), Ohio State Univ, Dept Elect \& Comp Engn, 205 Dreese Labs,2015 Neil Ave, Columbus, OH 43210 USA.
   Martinez, Aleix M., Ohio State Univ, Ctr Cognit \& Brain Sci, 205 Dreese Labs,2015 Neil Ave, Columbus, OH 43210 USA.},
DOI = {10.1037/dev0000728},
ISSN = {0012-1649},
EISSN = {1939-0599},
Keywords = {facial action coding; facial expression; emotion; computer vision;
   machine learning},
Keywords-Plus = {EXPRESSION RECOGNITION; SPEECH-PERCEPTION; 1ST YEAR; FACE; SEQUENCES;
   PCA; 3D},
Research-Areas = {Psychology},
Web-of-Science-Categories  = {Psychology, Developmental},
Author-Email = {martinez.158@osu.edu},
Funding-Acknowledgement = {National Institutes of Health {[}R01-DC-014498, R01-EY-020834]; Human
   Frontier Science Program {[}RGP0036/2016]; Center for Cognitive and
   Brain Sciences at The Ohio State University; NATIONAL INSTITUTE ON
   DEAFNESS AND OTHER COMMUNICATION DISORDERS {[}R01DC014498] Funding
   Source: NIH RePORTER},
Funding-Text = {The author and the research described in this article were supported by
   the National Institutes of Health, grants R01-DC-014498 and
   R01-EY-020834, the Human Frontier Science Program, grant RGP0036/2016,
   and by the Center for Cognitive and Brain Sciences at The Ohio State
   University. The author thanks Qianli Feng, Fabian Benitez-Quiroz,
   Ramprakash Srinivasan, and Shichuan Du for discussion. The Ohio State
   University is licensing some of the computational tools developed in the
   author's lab.},
Number-of-Cited-References = {110},
Times-Cited = {5},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {12},
Journal-ISO = {Dev. Psychol.},
Doc-Delivery-Number = {IT7PC},
Web-of-Science-Index = {Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000483067100012},
OA = {Bronze, Green Accepted},
DA = {2022-09-28},
}

@Article{WOS:000560656800001,
  author                     = {Song, Peng and Zheng, Wenming},
  journal                    = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title                      = {Feature Selection Based Transfer Subspace Learning for Speech Emotion Recognition},
  year                       = {2020},
  issn                       = {1949-3045},
  month                      = {JUL-SEP},
  number                     = {3},
  pages                      = {373-382},
  volume                     = {11},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Cross-corpus speech emotion recognition has recently received
   considerable attention due to the widespread existence of various
   emotional speech. It takes one corpus as the training data aiming to
   recognize emotions of another corpus, and generally involves two basic
   problems, i.e., feature matching and feature selection. Many previous
   works study these two problems independently, or just focus on solving
   the first problem. In this paper, we propose a novel algorithm, called
   feature selection based transfer subspace learning (FSTSL), to address
   these two problems. To deal with the first problem, a latent common
   subspace is learnt by reducing the difference of different corpora and
   preserving the important properties. Meanwhile, we adopt the l(2,1)-norm
   on the projection matrix to deal with the second problem. Besides, to
   guarantee the subspace to be robust and discriminative, the geometric
   information of data is exploited simultaneously in the proposed FSTSL
   framework. Empirical experiments on cross-corpus speech emotion
   recognition tasks demonstrate that our proposed method can achieve
   encouraging results in comparison with state-of-the-art algorithms.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Song, P (Corresponding Author), Yantai Univ, Sch Comp \& Control Engn, Yantai 264005, Peoples R China. Zheng, WM (Corresponding Author), Southeast Univ, Res Ctr Learning Sci, Minist Educ, Key Lab Child Dev \& Learning Sci, Nanjing 210096, Peoples R China. Song, Peng, Yantai Univ, Sch Comp \& Control Engn, Yantai 264005, Peoples R China. Zheng, Wenming, Southeast Univ, Res Ctr Learning Sci, Minist Educ, Key Lab Child Dev \& Learning Sci, Nanjing 210096, Peoples R China.},
  author-email               = {pengsong@ytu.edu.cn wenming\_zheng@seu.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {NB6XC},
  doi                        = {10.1109/TAFFC.2018.2800046},
  funding-acknowledgement    = {National Natural Science Foundation of China {[}61703360, 61572009]; Natural Science Foundation of Shandong Province {[}ZR2014FQ016]; National Basic Research Program of China {[}2015CB351704]; Fundamental Research Funds for the Southeast University {[}CDLS-2017-02]},
  funding-text               = {This work was supported in part by the National Natural Science Foundation of China under Grants 61703360 and 61572009, the Natural Science Foundation of Shandong Province under Grant ZR2014FQ016, the National Basic Research Program of China under Grant 2015CB351704 and the Fundamental Research Funds for the Southeast University under Grant CDLS-2017-02.},
  journal-iso                = {IEEE Trans. Affect. Comput.},
  keywords                   = {Feature selection; transfer learning; subspace learning; speech emotion recognition},
  keywords-plus              = {FRAMEWORK},
  language                   = {English},
  number-of-cited-references = {71},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Computer Science},
  times-cited                = {27},
  type                       = {Article},
  unique-id                  = {WOS:000560656800001},
  usage-count-last-180-days  = {5},
  usage-count-since-2013     = {37},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Cybernetics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000777495000001,
  author                     = {Chang, Hongli and Zong, Yuan and Zheng, Wenming and Tang, Chuangao and Zhu, Jie and Li, Xuejun},
  journal                    = {FRONTIERS IN PSYCHIATRY},
  title                      = {Depression Assessment Method: An EEG Emotion Recognition Framework Based on Spatiotemporal Neural Network},
  year                       = {2022},
  issn                       = {1664-0640},
  month                      = {MAR 15},
  volume                     = {12},
  status                     = {Rejeitado - Escopo},
  abstract                   = {The main characteristic of depression is emotional dysfunction,
   manifested by increased levels of negative emotions and decreased levels
   of positive emotions. Therefore, accurate emotion recognition is an
   effective way to assess depression. Among the various signals used for
   emotion recognition, electroencephalogram (EEG) signal has attracted
   widespread attention due to its multiple advantages, such as rich
   spatiotemporal information in multi-channel EEG signals. First, we use
   filtering and Euclidean alignment for data preprocessing. In the feature
   extraction, we use short-time Fourier transform and Hilbert-Huang
   transform to extract time-frequency features, and convolutional neural
   networks to extract spatial features. Finally, bi-directional long
   short-term memory explored the timing relationship. Before performing
   the convolution operation, according to the unique topology of the EEG
   channel, the EEG features are converted into 3D tensors. This study has
   achieved good results on two emotion databases: SEED and Emotional BCI
   of 2020 WORLD ROBOT COMPETITION. We applied this method to the
   recognition of depression based on EEG and achieved a recognition rate
   of more than 70\% under the five-fold cross-validation. In addition, the
   subject-independent protocol on SEED data has achieved a
   state-of-the-art recognition rate, which exceeds the existing research
   methods. We propose a novel EEG emotion recognition framework for
   depression detection, which provides a robust algorithm for real-time
   clinical depression detection based on EEG.},
  address                    = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
  affiliation                = {Zheng, WM (Corresponding Author), Southeast Univ, Key Lab Child Dev \& Learning Sci, Minist Educ, Nanjing, Peoples R China. Chang, Hongli; Zong, Yuan; Zheng, Wenming; Tang, Chuangao; Zhu, Jie; Li, Xuejun, Southeast Univ, Key Lab Child Dev \& Learning Sci, Minist Educ, Nanjing, Peoples R China. Chang, Hongli; Zhu, Jie, Southeast Univ, Sch Informat Sci \& Engn, Nanjing, Peoples R China.},
  article-number             = {837149},
  author-email               = {wenming\_zheng@seu.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {0F6VM},
  doi                        = {10.3389/fpsyt.2021.837149},
  funding-acknowledgement    = {National Natural Science Foundation of China {[}62076064, 61921004, 62076195, 81971282]; Zhishan Young Scholarship of Southeast University},
  funding-text               = {Funding This work was supported in part by the National Natural Science Foundation of China under grant 62076064, 61921004, 62076195, and 81971282, and the Zhishan Young Scholarship of Southeast University.},
  journal-iso                = {Front. Psychiatry},
  keywords                   = {depression; emotion recognition; electroencephalogram (EEG); convolutional neural network (CNN); long-short term memory network (LSTM)},
  keywords-plus              = {FACIAL EXPRESSIONS; RESPONSES; COGNITION; SADNESS; ENTROPY; MUSIC},
  language                   = {English},
  number-of-cited-references = {63},
  oa                         = {Green Published, gold},
  publisher                  = {FRONTIERS MEDIA SA},
  research-areas             = {Psychiatry},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000777495000001},
  usage-count-last-180-days  = {34},
  usage-count-since-2013     = {34},
  web-of-science-categories  = {Psychiatry},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000447336700033,
  author                     = {Luis Espinosa-Aranda, Jose and Vallez, Noelia and Maria Rico-Saavedra, Jose and Parra-Patino, Javier and Bueno, Gloria and Sorci, Matteo and Moloney, David and Pena, Dexmont and Deniz, Oscar},
  journal                    = {SYMMETRY-BASEL},
  title                      = {Smart Doll: Emotion Recognition Using Embedded Deep Learning},
  year                       = {2018},
  issn                       = {2073-8994},
  month                      = {SEP},
  number                     = {9},
  volume                     = {10},
  abstract                   = {Computer vision and deep learning are clearly demonstrating a capability
   to create engaging cognitive applications and services. However, these
   applications have been mostly confined to powerful Graphic Processing
   Units (GPUs) or the cloud due to their demanding computational
   requirements. Cloud processing has obvious bandwidth, energy consumption
   and privacy issues. The Eyes of Things (EoT) is a powerful and versatile
   embedded computer vision platform which allows the user to develop
   artificial vision and deep learning applications that analyse images
   locally. In this article, we use the deep learning capabilities of an
   EoT device for a real-life facial informatics application: a doll
   capable of recognizing emotions, using deep learning techniques, and
   acting accordingly. The main impact and significance of the presented
   application is in showing that a toy can now do advanced processing
   locally, without the need of further computation in the cloud, thus
   reducing latency and removing most of the ethical issues involved.
   Finally, the performance of the convolutional neural network developed
   for that purpose is studied and a pilot was conducted on a panel of 12
   children aged between four and ten years old to test the doll.},
  address                    = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  affiliation                = {Deniz, O (Corresponding Author), Univ Castilla La Mancha, VISILAB, ETSI Ind, Avda Camilo Jose Cela S-N, E-13071 Ciudad Real, Spain. Luis Espinosa-Aranda, Jose; Vallez, Noelia; Maria Rico-Saavedra, Jose; Parra-Patino, Javier; Bueno, Gloria; Deniz, Oscar, Univ Castilla La Mancha, VISILAB, ETSI Ind, Avda Camilo Jose Cela S-N, E-13071 Ciudad Real, Spain. Sorci, Matteo, nViso SA, PSE D, Site EPFL, CH-1015 Lausanne, Switzerland. Moloney, David; Pena, Dexmont, Intel R\&D Ireland Ltd, Collinstown Ind Pk, Leixlip W23 CW68, Kildare, Ireland.},
  article-number             = {387},
  author-email               = {josel.espinosa@uclm.es noelia.vallez@uclm.es josemaria.rico@uclm.es javier.parra@uclm.es gloria.bueno@uclm.es matteo.sorci@nviso.ch david.moloney@intel.com dexmont.pena@intel.com JoseL.Espinosa@uclm.es},
  da                         = {2022-09-28},
  doc-delivery-number        = {GW9RV},
  doi                        = {10.3390/sym10090387},
  funding-acknowledgement    = {European Union's Horizon 2020 Research and Innovation Programme {[}643924]},
  funding-text               = {This work has been supported by the European Union's Horizon 2020 Research and Innovation Programme under grant agreement No. 643924 {[}25].},
  journal-iso                = {Symmetry-Basel},
  keywords                   = {facial informatics; deep learning; computer vision; mobile applications; real-time and embedded systems},
  keywords-plus              = {NEURAL-NETWORKS},
  language                   = {English},
  number-of-cited-references = {20},
  oa                         = {gold, Green Submitted},
  orcid-numbers              = {Deniz, Oscar/0000-0002-0841-4131 Bueno, Gloria/0000-0002-7345-4869 Vallez, Noelia/0000-0002-5092-8275 Peña, Dexmont/0000-0002-3855-1364 Espinosa-Aranda, Jose Luis/0000-0001-5377-848X},
  publisher                  = {MDPI},
  research-areas             = {Science \& Technology - Other Topics},
  researcherid-numbers       = {Deniz, Oscar/AAA-3245-2020 Bueno, Gloria/K-6286-2014 Vallez, Noelia/L-3106-2015 Peña, Dexmont/AAK-6961-2020},
  times-cited                = {6},
  type                       = {Article},
  unique-id                  = {WOS:000447336700033},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {8},
  web-of-science-categories  = {Multidisciplinary Sciences},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@InProceedings{WOS:000853024300009,
  author                     = {Valles, Damian and Matin, Rezwan},
  booktitle                  = {2021 IEEE WORLD AI IOT CONGRESS (AIIOT)},
  title                      = {An Audio Processing Approach using Ensemble Learning for Speech-Emotion Recognition for Children with ASD},
  year                       = {2021},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {IEEE World AI IoT Congress (AIIoT), ELECTR NETWORK, MAY 10-13, 2021},
  organization               = {Smart; IEEE Reg 1; IEEE USA; Inst Engn \& Management; Univ Engn \& Management},
  pages                      = {55-61},
  publisher                  = {IEEE},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Children with Autism Spectrum Disorder (ASD) find it difficult to detect
   human emotions in social interactions. A speech emotion recognition
   system was developed in this work, which aims to help these children to
   better identify the emotions of their communication partner. The system
   was developed using machine learning and deep learning techniques.
   Through the use of ensemble learning, multiple machine learning
   algorithms were joined to provide a final prediction on the recorded
   input utterances. The ensemble of models includes a Support Vector
   Machine (SVM), a Multi-Layer Perceptron (MLP), and a Recurrent Neural
   Network (RNN). All three models were trained on the Ryerson Audio-Visual
   Database of Emotional Speech and Songs (RAVDESS), the Toronto Emotional
   Speech Set (TESS), and the Crowd-sourced Emotional Multimodal Actors
   Dataset (CREMA-D). A fourth dataset was used, which was created by
   adding background noise to the clean speech files from the datasets
   previously mentioned. The paper describes the audio processing of the
   samples, the techniques used to include the background noise, and the
   feature extraction coefficients considered for the development and
   training of the models. This study presents the performance evaluation
   of the individual models to each of the datasets, inclusion of the
   background noises, and the combination of using all of the samples in
   all three datasets. The evaluation was made to select optimal
   hyperparameters configuration of the models to evaluate the performance
   of the ensemble learning approach through majority voting. The overall
   performance of the ensemble learning reached a peak accuracy of 663\%,
   reaching a higher performance emotion classification accuracy than the
   MLP model which reached 65.7\%.},
  affiliation                = {Valles, D (Corresponding Author), Texas State Univ, Ingram Sch Engn, San Marcos, TX 78666 USA. Valles, Damian; Matin, Rezwan, Texas State Univ, Ingram Sch Engn, San Marcos, TX 78666 USA.},
  author-email               = {dvalles@txstate.edu r\_m727@txstate.edu},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BT8FJ},
  doi                        = {10.1109/AIIOT52608.2021.9454174},
  isbn                       = {978-1-6654-3568-0},
  keywords                   = {Ensemble; SVM; MLP; RNN; Autism; emotions},
  language                   = {English},
  number-of-cited-references = {25},
  research-areas             = {Computer Science; Telecommunications},
  times-cited                = {0},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000853024300009},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {0},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Telecommunications},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000477045000007,
  author                     = {Goulart, Christiane and Valadao, Carlos and Delisle-Rodriguez, Denis and Funayama, Douglas and Favarato, Alvaro and Baldo, Guilherme and Binotte, Vinicius and Caldeira, Eliete and Bastos-Filho, Teodiano},
  journal                    = {SENSORS},
  title                      = {Visual and Thermal Image Processing for Facial Specific Landmark Detection to Infer Emotions in a Child-Robot Interaction},
  year                       = {2019},
  issn                       = {1424-8220},
  month                      = {JUL 1},
  number                     = {13},
  volume                     = {19},
  abstract                   = {Child-Robot Interaction (CRI) has become increasingly addressed in
   research and applications. This work proposes a system for emotion
   recognition in children, recording facial images by both visual
   (RGB-red, green and blue) and Infrared Thermal Imaging (IRTI) cameras.
   For this purpose, the Viola-Jones algorithm is used on color images to
   detect facial regions of interest (ROIs), which are transferred to the
   thermal camera plane by multiplying a homography matrix obtained through
   the calibration process of the camera system. As a novelty, we propose
   to compute the error probability for each ROI located over thermal
   images, using a reference frame manually marked by a trained expert, in
   order to choose that ROI better placed according to the expert criteria.
   Then, this selected ROI is used to relocate the other ROIs, increasing
   the concordance with respect to the reference manual annotations.
   Afterwards, other methods for feature extraction, dimensionality
   reduction through Principal Component Analysis (PCA) and pattern
   classification by Linear Discriminant Analysis (LDA) are applied to
   infer emotions. The results show that our approach for ROI locations may
   track facial landmarks with significant low errors with respect to the
   traditional Viola-Jones algorithm. These ROIs have shown to be relevant
   for recognition of five emotions, specifically disgust, fear, happiness,
   sadness, and surprise, with our recognition system based on PCA and LDA
   achieving mean accuracy (ACC) and Kappa values of 85.75\% and 81.84\%,
   respectively. As a second stage, the proposed recognition system was
   trained with a dataset of thermal images, collected on 28 typically
   developing children, in order to infer one of five basic emotions
   (disgust, fear, happiness, sadness, and surprise) during a child-robot
   interaction. The results show that our system can be integrated to a
   social robot to infer child emotions during a child-robot interaction.},
  address                    = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  affiliation                = {Goulart, C (Corresponding Author), Fed Univ Espirito Santo UFES, Northeast Network Biotechnol RENORBIO, Postgrad Program Biotechnol, Hlth Sci Ctr, Ave Marechal Campos 1468, BR-29043900 Vitoria, ES, Brazil. Valadao, C (Corresponding Author), Univ Fed Espirito Santo, Postgrad Program Elect Engn, Ave Fernando Ferrari 514, BR-29075910 Vitoria, ES, Brazil. Goulart, Christiane; Bastos-Filho, Teodiano, Fed Univ Espirito Santo UFES, Northeast Network Biotechnol RENORBIO, Postgrad Program Biotechnol, Hlth Sci Ctr, Ave Marechal Campos 1468, BR-29043900 Vitoria, ES, Brazil. Valadao, Carlos; Delisle-Rodriguez, Denis; Binotte, Vinicius; Bastos-Filho, Teodiano, Univ Fed Espirito Santo, Postgrad Program Elect Engn, Ave Fernando Ferrari 514, BR-29075910 Vitoria, ES, Brazil. Delisle-Rodriguez, Denis, Univ Oriente, Ctr Med Biophys, Patricio Lumumba S-N, Santiago De Cuba 90500, Cuba. Funayama, Douglas, Univ Fed Espirito Santo, Comp Engn Dept, Ave Fernando Ferrari 514, BR-29075910 Vitoria, ES, Brazil. Favarato, Alvaro, Univ Fed Espirito Santo, Mech Engn Dept, Ave Fernando Ferrari 514, BR-29075910 Vitoria, ES, Brazil. Baldo, Guilherme; Caldeira, Eliete; Bastos-Filho, Teodiano, Univ Fed Espirito Santo, Elect Engn Dept, Ave Fernando Ferrari 514, BR-29075910 Vitoria, ES, Brazil.},
  article-number             = {2844},
  author-email               = {christiane.ufes@gmail.com carlostvaladao@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {IL1FT},
  doi                        = {10.3390/s19132844},
  funding-acknowledgement    = {FAPES/Brazil {[}72982608, 645/2016]},
  funding-text               = {This research was funded by FAPES/Brazil, grant numbers 72982608 and 645/2016.},
  journal-iso                = {Sensors},
  keywords                   = {Viola-Jones; facial emotion recognition; facial expression recognition; facial detection; facial landmarks; infrared thermal imaging; homography matrix; socially assistive robot},
  keywords-plus              = {EXPRESSION RECOGNITION},
  language                   = {English},
  number-of-cited-references = {74},
  oa                         = {Green Published, gold, Green Submitted},
  orcid-numbers              = {Delisle-Rodriguez, Denis/0000-0002-8937-031X BASTOS, TEODIANO/0000-0002-1185-2773 de Oliveira Caldeira, Eliete Maria/0000-0002-3742-0952},
  publisher                  = {MDPI},
  research-areas             = {Chemistry; Engineering; Instruments \& Instrumentation},
  researcherid-numbers       = {Delisle-Rodriguez, Denis/S-2519-2019 BASTOS, TEODIANO/P-7535-2014},
  times-cited                = {23},
  type                       = {Article},
  unique-id                  = {WOS:000477045000007},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {6},
  web-of-science-categories  = {Chemistry, Analytical; Engineering, Electrical \& Electronic; Instruments \& Instrumentation},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@InProceedings{WOS:000495603500014,
  author                     = {Farzaneh, Amir Hossein and Kim, Yanghee and Zhou, Mengxi and Qi, Xiaojun},
  booktitle                  = {ARTIFICIAL INTELLIGENCE IN EDUCATION, AIED 2019, PT II},
  title                      = {Developing a Deep Learning-Based Affect Recognition System for Young Children},
  year                       = {2019},
  address                    = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
  editor                     = {Isotani, S and Millan, E and Ogan, A and Hastings, P and McLaren, B and Luckin, R},
  note                       = {20th International Conference on Artificial Intelligence in Education (AIED), Chicago, IL, JUN 25-29, 2019},
  pages                      = {73-78},
  publisher                  = {SPRINGER INTERNATIONAL PUBLISHING AG},
  series                     = {Lecture Notes in Artificial Intelligence},
  volume                     = {11626},
  status                     = {Aceito},
  abstract                   = {Affective interaction in tutoring environments has been of great
   interest among several researchers in this community, which has spurred
   the development of various systems to capture learners' emotional
   states. Young children are one of the biggest learner groups in digital
   learning environments, but these studies have rarely targeted them. Our
   current study leverages computer vision and deep learning to analyze
   young childrens' learning-related affective states. We developed an
   effective recognition system to compute the probability for a child to
   present neutral or positive affective state. Our results showed that the
   prototype was able to achieve an average affective state prediction
   accuracy of 93.05\%.},
  affiliation                = {Farzaneh, AH (Corresponding Author), Utah State Univ, Dept Comp Sci, Logan, UT 84322 USA. Farzaneh, Amir Hossein; Qi, Xiaojun, Utah State Univ, Dept Comp Sci, Logan, UT 84322 USA. Kim, Yanghee; Zhou, Mengxi, Northern Illinois Univ, Dept Educ Technol Res \& Assessment, De Kalb, IL 60115 USA.},
  author-email               = {farzaneh@aggiemail.usu.edu ykim9@niu.edu z1841378@students.niu.edu xiaojun.qi@usu.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {BO1FX},
  doi                        = {10.1007/978-3-030-23207-8\_14},
  eissn                      = {1611-3349},
  isbn                       = {978-3-030-23207-8; 978-3-030-23206-1},
  issn                       = {0302-9743},
  keywords                   = {Emotion recognition; Deep learning; Computer vision; Young children; Learner affect},
  keywords-plus              = {FACIAL EXPRESSION RECOGNITION},
  language                   = {English},
  number-of-cited-references = {20},
  priority                   = {prio2},
  research-areas             = {Computer Science; Education \& Educational Research},
  times-cited                = {2},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000495603500014},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {5},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Education \& Educational Research},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S); Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)},
}

@Article{WOS:000711808500036,
  author                     = {Zhang, Tao and Liu, Minjie and Yuan, Tian and Al-Nabhan, Najla},
  journal                    = {IEEE INTERNET OF THINGS JOURNAL},
  title                      = {Emotion-Aware and Intelligent Internet of Medical Things Toward Emotion Recognition During COVID-19 Pandemic},
  year                       = {2021},
  issn                       = {2327-4662},
  month                      = {NOV 1},
  number                     = {21},
  pages                      = {16002-16013},
  volume                     = {8},
  status                     = {Aceito},
  abstract                   = {The Internet of Medical Things (IoMT) is a brand new technology of
   combining medical devices and other wireless devices to access to the
   healthcare management systems. This article has sought the possibilities
   of aiding the current Corona Virus Disease 2019 (COVID-19) pandemic by
   implementing machine learning algorithms while offering emotional
   treatment suggestion to the doctors and patients. The cognitive model
   with respect to IoMT is best suited to this pandemic as every person is
   to be connected and monitored through a cognitive network. However, this
   COVID-19 pandemic still remain some challenges about emotional
   solicitude for infants and young children, elderly, and mentally ill
   persons during pandemic. Confronting these challenges, this article
   proposes an emotion-aware and intelligent IoMT system, which contains
   information sharing, information supervision, patients tracking, data
   gathering and analysis, healthcare, etc. Intelligent IoMT devices are
   connected to collect multimodal data of patients in a surveillance
   environments. The latest data and inputs from official websites and
   reports are tested for further investigation and analysis of the emotion
   analysis. The proposed novel IoMT platform enables remote health
   monitoring and decision-making about the emotion, therefore greatly
   contribute convenient and continuous emotion-aware healthcare services
   during COVID-19 pandemic. Experimental results on some emotion data
   indicate that the proposed framework achieves significant advantage when
   compared with the some mainstream models. The proposed cognition-based
   dynamic technology is an effective solution way for accommodating a big
   number of devices and this COVID-19 pandemic application. The
   controversy and future development trend are also discussed.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Zhang, T (Corresponding Author), Jiangnan Univ, Sch Artificial Intelligence \& Comp Sci, Wuxi 214122, Jiangsu, Peoples R China. Zhang, Tao, Jiangnan Univ, Sch Artificial Intelligence \& Comp Sci, Wuxi 214122, Jiangsu, Peoples R China. Liu, Minjie, Taihu Univ Wuxi, Sch Nursing, Wuxi 214064, Jiangsu, Peoples R China. Yuan, Tian, Nanjing Inst Technol, Sch Comp Engn, Nanjing 210000, Peoples R China. Al-Nabhan, Najla, King Saud Univ, Dept Comp Sci, Riyadh 11682, Saudi Arabia.},
  author-email               = {taozhang@jiangnan.edu.cn minjieliu@163.com ytian@njit.edu.cn nalnabhan@ksu.edu.sa},
  da                         = {2022-09-28},
  doc-delivery-number        = {WN5LI},
  doi                        = {10.1109/JIOT.2020.3038631},
  funding-acknowledgement    = {National Science Foundation, China {[}61702226]; Natural Science Foundation of Jiangsu Province {[}BK20170200]; China Postdoctoral Science Foundation {[}2019M661722]; Fundamental Research Funds for the Central Universities {[}JUSRP11854]; Deanship of Scientific Research at King Saud University {[}RG-1441-331]},
  funding-text               = {This work was supported in part by the National Science Foundation, China under Grant 61702226; in part by the Natural Science Foundation of Jiangsu Province under Grant BK20170200; in part by the China Postdoctoral Science Foundation under Grant 2019M661722; in part by the Fundamental Research Funds for the Central Universities under Grant JUSRP11854; and in part by the Deanship of Scientific Research at King Saud University through Research Group under Grant RG-1441-331.},
  journal-iso                = {IEEE Internet Things J.},
  keywords                   = {Emotion recognition; COVID-19; Medical services; Internet of Things; Pandemics; Wireless communication; Convolution; Cognitive model; Corona Virus Disease 2019 (COVID-19); emotion-aware; healthcare management systems; internet of medical things (IoMT)},
  keywords-plus              = {NEURAL-NETWORK},
  language                   = {English},
  number-of-cited-references = {57},
  oa                         = {Bronze, Green Published},
  priority                   = {prio1},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Computer Science; Engineering; Telecommunications},
  researcherid-numbers       = {Tian, Yuan/AFV-8924-2022},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000711808500036},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {9},
  web-of-science-categories  = {Computer Science, Information Systems; Engineering, Electrical \& Electronic; Telecommunications},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000805848000003,
  author                     = {Yang, Jingwen and Chen, Zelin and Qiu, Guoxin and Li, Xiangyu and Li, Caixia and Yang, Kexin and Chen, Zhuanggui and Gao, Leyan and Lu, Shuo},
  journal                    = {COMPUTER SPEECH AND LANGUAGE},
  title                      = {Exploring the relationship between children's facial emotion processing characteristics and speech communication ability using deep learning on eye tracking and speech performance measures},
  year                       = {2022},
  issn                       = {0885-2308},
  month                      = {NOV},
  volume                     = {76},
  status                     = {Aceito},
  abstract                   = {The ability of efficient facial emotion recognition (FER) plays a
   significant role in successful human communication and is closely
   associated with multiple speech communication disorders (SCD) in
   children. Despite the relevance, little is known about how speech
   communication abilities (SCA) and FER are correlated or of their
   underlying mechanism. To address this, we monitored eye movements of 115
   children while watching human faces with different emotions and designed
   a machine-learning based SCD prediction model to explore the underlying
   pattern of eye movements during the FER task as well as their
   correlation with SCA. Strong and detailed correlations were found
   between different dimensions of SCA and various eye-movement features. A
   group of FER gazing patterns was found to be highly sensitive to the
   possibility of children's SCD. The SCD prediction model reached an
   accuracy as high as 88.9\%, which offers a possible technique to fast
   screen SCD for children.},
  address                    = {24-28 OVAL RD, LONDON NW1 7DX, ENGLAND},
  affiliation                = {Lu, S (Corresponding Author), Shenzhen Univ, Sch Foreign Languages, Shenzhen 518060, Peoples R China. Lu, S (Corresponding Author), Sen Univ, Affiliated Hosp Sun Yat 3, Mental \& Neurol Dis Res Ctr, Dept Clin Neurolinguist Res, Guangzhou 510630, Peoples R China. Lu, Shuo, Shenzhen Univ, Sch Foreign Languages, Shenzhen 518060, Peoples R China. Yang, Jingwen, Sen Univ, Affiliated Hosp Sun Yat 3, Dept Neurol, Guangzhou 510630, Peoples R China. Yang, Jingwen; Qiu, Guoxin; Lu, Shuo, Sen Univ, Affiliated Hosp Sun Yat 3, Mental \& Neurol Dis Res Ctr, Dept Clin Neurolinguist Res, Guangzhou 510630, Peoples R China. Chen, Zelin, Sun Yat sen Univ, Sch Comp Sci \& Engn, Guangzhou 510275, Peoples R China. Li, Xiangyu, Chinese Univ Hong Kong, Dept Linguist \& Modern Languages, Hongkong, Peoples R China. Li, Caixia, Sen Univ, Affiliated Hosp Sun Yat 3, Dept Pediat, Guangzhou 510630, Peoples R China. Gao, Leyan, Sun Yat sen Univ, Dept Chinese Language \& Literature, Teaching Lab Neurolinguist, Guangzhou 510275, Peoples R China.},
  article-number             = {101389},
  author-email               = {Lushuo@szu.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {1V1GV},
  doi                        = {10.1016/j.csl.2022.101389},
  eissn                      = {1095-8363},
  funding-acknowledgement    = {Guangzhou City scientific research Project ``Aphasia Rehabilitation Training and Neurolinguistics Study based on Chinese Language{''} {[}201904010208]; Sun Yat-sen University Key Incubation Project for Young Teachers ``Multi-model database construction for Children's developmental language disorders study based on big data deep learning{''} {[}19wkzd26]},
  funding-text               = {This work was supported by the Guangzhou City scientific research Project ``Aphasia Rehabilitation Training and Neurolinguistics Study based on Chinese Language{''} {[}201904010208]; and Sun Yat-sen University Key Incubation Project for Young Teachers ``Multi-model database construction for Children's developmental language disorders study based on big data deep learning{''} {[}19wkzd26].},
  journal-iso                = {Comput. Speech Lang.},
  keywords                   = {Facial emotion recognition; Speech communication disorder; Eye tracking; Linguistic aspects; Machine-learning},
  keywords-plus              = {AUTISM SPECTRUM DISORDERS; HIGH-FUNCTIONING AUTISM; SOCIAL COMPETENCE; PRAGMATIC LANGUAGE; VISUAL SCANPATH; GAZE PATTERNS; RECOGNITION; EXPRESSIONS; IMPAIRMENTS; HEARING},
  language                   = {English},
  number-of-cited-references = {65},
  priority                   = {prio2},
  publisher                  = {ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD},
  research-areas             = {Computer Science},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000805848000003},
  usage-count-last-180-days  = {4},
  usage-count-since-2013     = {4},
  web-of-science-categories  = {Computer Science, Artificial Intelligence},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@InProceedings{WOS:000546032800016,
  author                     = {Rouhi, Amirreza and Spitale, Micol and Catania, Fabio and Cosentino, Giulia and Gelsomini, Mirko and Garzotto, Franca},
  booktitle                  = {PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES: COMPANION (IUI 2019)},
  title                      = {Emotify: Emotional Game for Children with Autism Spectrum Disorder based-on Machine Learning},
  year                       = {2019},
  address                    = {1515 BROADWAY, NEW YORK, NY 10036-9998 USA},
  note                       = {24th International Conference on Intelligent User Interfaces (IUI), Marina Del Rey, CA, MAR 17-20, 2019},
  organization               = {Assoc Comp Machinery},
  pages                      = {31-32},
  publisher                  = {ASSOC COMPUTING MACHINERY},
  status                     = {Aceito},
  abstract                   = {Children with Autism Spectrum Disorder (ASD) often face the challenge of
   detecting and expressing emotions. I.e., it's hard for them to recognize
   happiness, sadness and anger in other people and to express their own
   feelings. This difficulty produces severe impairments in communication
   and social functioning. The paper proposes a spoken educational game,
   exploiting Machine Learning techniques, to help children with ASD to
   understand how to correctly identify and express emotions. The game
   focuses on four emotional states (happiness, sadness, anger and
   neutrality) and is divided in two levels with increasingly difficulty:
   the first step is to learn how to recognize and express feelings and in
   the second phase emotional skills by the user are examined and
   evaluated. The application integrates a multilingual emotion recognizer
   from the pitch of the voice.},
  affiliation                = {Rouhi, A (Corresponding Author), Politecn Milan, Milan, Italy. Rouhi, Amirreza; Spitale, Micol; Catania, Fabio; Cosentino, Giulia; Gelsomini, Mirko; Garzotto, Franca, Politecn Milan, Milan, Italy.},
  author-email               = {amirreza.rouhi@polimi.it micol.spitale@polimi.it fabio.catania@polimi.it giulia.cosentino@polimi.it mirko.gelsomini@polimi.it franca.garzotto@polimi.it},
  book-group-author          = {ACM},
  da                         = {2022-09-28},
  doc-delivery-number        = {BP3BB},
  doi                        = {10.1145/3308557.3308688},
  isbn                       = {978-1-4503-6673-1},
  keywords                   = {ASD children; machine learning game; classification; random forest classifier; children's speech; speech recognition},
  language                   = {English},
  number-of-cited-references = {6},
  orcid-numbers              = {Gelsomini, Mirko/0000-0001-8421-6850 CATANIA, FABIO/0000-0002-5403-9002},
  priority                   = {prio1},
  research-areas             = {Computer Science},
  researcherid-numbers       = {Gelsomini, Mirko/AAG-9874-2019},
  times-cited                = {3},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000546032800016},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Software Engineering},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{ WOS:000458071800001,
Author = {Song, Tengfei and Zheng, Wenming and Lu, Cheng and Zong, Yuan and Zhang,
   Xilei and Cui, Zhen},
Title = {MPED: A Multi-Model Physiological Emotion Database for Discrete Emotion
   Recongnition},
Journal = {IEEE ACCESS},
Year = {2019},
Volume = {7},
Pages = {12177-12191},
Abstract = {To explore human emotions, in this paper, we design and build a
   multi-modal physiological emotion database, which collects four modal
   physiological signals, i.e., electroencephalogram (EEG), galvanic skin
   response, respiration, and electrocardiogram (ECG). To alleviate the
   influence of culture dependent elicitation materials and evoke desired
   human emotions, we specifically collect an emotion elicitation material
   database selected from more than 1500 video clips. By the considerable
   amount of strict man-made labeling, we elaborately choose 28 videos as
   standardized elicitation samples, which are assessed by psychological
   methods. The physiological signals of participants were synchronously
   recorded when they watched these standardized video clips that described
   six discrete emotions and neutral emotion. With three types of
   classification protocols, different feature extraction methods and
   classifiers (support vector machine and k-NearestNeighbor) were used to
   recognize the physiological responses of different emotions, which
   presented the baseline results. Simultaneously, we present a novel
   attention-long short-term memory (A-LSTM), which strengthens the
   effectiveness of useful sequences to extract more discriminative
   features. In addition, correlations between the EEG signals and the
   participants' ratings are investigated. The database has been made
   publicly available to encourage other researchers to use it to evaluate
   their own emotion estimation methods.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Zheng, WM (Corresponding Author), Southeast Univ, Key Lab Child Dev \& Learning Sci, Minist Educ, Nanjing 210096, Jiangsu, Peoples R China.
   Song, Tengfei; Zheng, Wenming; Lu, Cheng; Zong, Yuan; Zhang, Xilei, Southeast Univ, Key Lab Child Dev \& Learning Sci, Minist Educ, Nanjing 210096, Jiangsu, Peoples R China.
   Song, Tengfei; Lu, Cheng, Southeast Univ, Sch Informat Sci \& Engn, Nanjing 210096, Jiangsu, Peoples R China.
   Cui, Zhen, Nanjing Univ Sci \& Technol, Sch Comp Sci \& Engn, Nanjing 210094, Jiangsu, Peoples R China.},
DOI = {10.1109/ACCESS.2019.2891579},
ISSN = {2169-3536},
Keywords = {Discrete emotion recognition; physiological signals; EEG; affective
   computing; machine learning; video-induced emotion; LSTM},
Keywords-Plus = {RECOGNITION; EEG; MUSIC; FEAR; EXPRESSION},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {wenming\_zheng@seu.edu.cn},
ORCID-Numbers = {Song, Tengfei/0000-0003-0874-2845},
Funding-Acknowledgement = {National Basic Research Program of China {[}2015CB351704]; National
   Natural Science Foundation of China {[}61572009, 61772276]; Key Research
   and Development Program of Jiangsu Province, China {[}BE2016616];
   Fundamental Research Funds for the Central Universities
   {[}2242018K3DN01]},
Funding-Text = {This work was supported in part by the National Basic Research Program
   of China under Grant 2015CB351704, in part by the National Natural
   Science Foundation of China under Grant 61572009 and Grant 61772276, in
   part by the Key Research and Development Program of Jiangsu Province,
   China, under Grant BE2016616, and in part by the Fundamental Research
   Funds for the Central Universities under Grant 2242018K3DN01.},
Number-of-Cited-References = {57},
Times-Cited = {67},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {44},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {HK6GC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000458071800001},
OA = {gold},
DA = {2022-09-28},
}

@InProceedings{WOS:000657957000059,
  author                     = {Asif, Mohammad Ahmed and Al Wadhahi, Firas and Rehman, Muhammad Hassan and Al Kalban, Ismail and Achuthan, Geetha},
  booktitle                  = {INNOVATIVE DATA COMMUNICATION TECHNOLOGIES AND APPLICATION},
  title                      = {Intelligent Educational System for Autistic Children Using Augmented Reality and Machine Learning},
  year                       = {2020},
  address                    = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
  editor                     = {Raj, JS and Bashar, A and Ramson, SRJ},
  note                       = {International Conference on Innovative Data Communication Technologies and Application (ICIDCA), Coimbatore, INDIA, OCT 17-18, 2019},
  pages                      = {524-534},
  publisher                  = {SPRINGER INTERNATIONAL PUBLISHING AG},
  series                     = {Lecture Notes on Data Engineering and Communications Technologies},
  volume                     = {46},
  status                     = {Aceito},
  abstract                   = {Autism is a severe disorder affecting 1 in 160 children globally. Autism
   comprises of several development disabilities such as social,
   communicational and behavioural challenges. Children being diagnosed by
   the autism mainly face a hard time studying curriculum in inclusive
   classrooms based on their IQ level and the autism levels. Although,
   different strategies and learning teaching tools are available to
   support autistic children, only few systems aid them in learning
   efficiently, and are not highly interactive. Thus, the proposed
   Intelligent Education System primarily focuses on providing interactive
   learning experience to the autistic children with IQ level >50\% and
   efficient teaching assistance to their tutors using augmented reality
   and machine learning in both English and Arabic The capability of the
   education system to perform an action, allows the autistic child to
   interact with the playable sand and gain interest. In learning stage,
   once the child scribbles on the sandbox, Kinect 3D camera captures and
   recognizes the drawn image. After the refinement and recognition of the
   image using OpenCV and classification model, the stored set of real
   world object are projected on the canvas. Besides, a webcam captures the
   facial expression of the child, and emotion detection algorithm
   determines the reaction of the child. Based on the child's emotion, the
   current object is projected and pronounced three times to enforce better
   learning. Once the instructor chooses the language and character to be
   taught using the developed mobile application, the system displays it
   over the sandbox and further three objects that starts with the
   particular character are pronounced and projected. The system is tested
   rigorously with large set of users, and the results prove the efficiency
   of the system and happiness of the autistic children in better learning.},
  affiliation                = {Asif, MA (Corresponding Author), Natl Univ Sci \& Technol, Coll Engn, Muscat, Oman. Asif, Mohammad Ahmed; Al Wadhahi, Firas; Rehman, Muhammad Hassan; Al Kalban, Ismail; Achuthan, Geetha, Natl Univ Sci \& Technol, Coll Engn, Muscat, Oman.},
  author-email               = {ahmed\_asif96@hotmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {BR5WO},
  doi                        = {10.1007/978-3-030-38040-3\_59},
  funding-acknowledgement    = {The Research Council (TRC) of Oman},
  funding-text               = {We would like to thank The Research Council (TRC) of Oman for funding to carry out this research as FURAP project. Our deepest gratitude towards the National University of Science and Technology for providing all the essential assets and facilities. All author states that there is no conflict of interest.},
  isbn                       = {978-3-030-38040-3; 978-3-030-38039-7},
  issn                       = {2367-4512},
  keywords                   = {Autism spectrum disorder; Autistic children; Education; Machine learning; Augmented reality; Image processing},
  language                   = {English},
  number-of-cited-references = {13},
  priority                   = {prio1},
  research-areas             = {Telecommunications},
  times-cited                = {0},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000657957000059},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {7},
  web-of-science-categories  = {Telecommunications},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000544034800009,
  author                     = {Kalantarian, Haik and Jedoui, Khaled and Washington, Peter and Wall, Dennis P.},
  journal                    = {IEEE TRANSACTIONS ON GAMES},
  title                      = {A Mobile Game for Automatic Emotion-Labeling of Images},
  year                       = {2020},
  issn                       = {2475-1502},
  month                      = {JUN},
  number                     = {2},
  pages                      = {213-218},
  volume                     = {12},
  status                     = {Aceito},
  abstract                   = {In this short paper, we describe challenges in the development of a
   mobile charades-style game for delivery of social training to children
   with autism spectrum disorder (ASD). Providing real-time feedback and
   adapting game difficulty in response to the child's performance
   necessitates the integration of emotion classifiers into the system. Due
   to the limited performance of existing emotion recognition platforms for
   children with ASD, we propose a novel technique to automatically extract
   emotion-labeled frames from video acquired from game sessions, which we
   hypothesize can be used to train new emotion classifiers to overcome
   these limitations. Our technique, which uses probability scores from
   three different classifiers and meta information from game sessions,
   correctly identified 83\% of frames compared to a baseline of 51.6\%
   from the best emotion classification API evaluated in this paper.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Wall, DP (Corresponding Author), Stanford Univ, Sch Med, Stanford, CA 94305 USA. Wall, DP (Corresponding Author), Stanford Univ, Dept Pediat \& Biomed Data Sci, Stanford, CA 94305 USA. Kalantarian, Haik; Jedoui, Khaled; Washington, Peter; Wall, Dennis P., Stanford Univ, Sch Med, Stanford, CA 94305 USA. Kalantarian, Haik; Wall, Dennis P., Stanford Univ, Dept Pediat \& Biomed Data Sci, Stanford, CA 94305 USA. Jedoui, Khaled, Stanford Univ, Dept Math, Stanford, CA 94305 USA. Washington, Peter, Stanford Univ, Dept Bioengn, Stanford, CA 94305 USA.},
  author-email               = {haik@stanford.edu thekej@stanford.edu peter100@stanford.edu dpwall@stanford.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {MD5TK},
  doi                        = {10.1109/TG.2018.2877325},
  eissn                      = {2475-1510},
  funding-acknowledgement    = {National Institutes of Health {[}1R21HD091500-01, 1R01EB025025-01]; Dekeyser and Friends Foundation; Mosbacher Family Fund for Autism Research; Hartwell Foundation; David and Lucile Packard Foundation Special Projects Grant; Beckman Center for Molecular and Genetic Medicine; Coulter Endowment Translational Research Grant; Berry Fellowship; Child Health Research Institute; Spectrum Pilot Program; Stanford's Precision Health; Integrated Diagnostics Center (PHIND); Stanford's Human Centered Artificial Intelligence Program; Thrasher Research Fund; Stanford NLM Clinical Data Science program {[}T-15LM007033-35]},
  funding-text               = {This work was supported in part by awards to D. P. Wall by the National Institutes of Health (1R21HD091500-01 and 1R01EB025025-01) and in part by the Dekeyser and Friends Foundation, in part by the Mosbacher Family Fund for Autism Research, and in part by Peter Sullivan, in part by the Hartwell Foundation, in part by theDavid and Lucile Packard Foundation Special Projects Grant, in part by the Beckman Center for Molecular and Genetic Medicine, in part by the Coulter Endowment Translational Research Grant, in part by the Berry Fellowship, in part by the Child Health Research Institute, in part by the Spectrum Pilot Program, in part by the Stanford's Precision Health, in part by the Integrated Diagnostics Center (PHIND), and in part by the Stanford's Human Centered Artificial Intelligence Program. The work of H. Kalantarian was supported in part by the Thrasher Research Fund and in part by the Stanford NLM Clinical Data Science program (T-15LM007033-35) (Haik Kalantarian and Khaled Jedoui contributed equally to this work.)},
  journal-iso                = {IEEE Trans. Gamres},
  keywords                   = {Games; Autism; Emotion recognition; Medical treatment; Training; Databases; Pediatrics; Autism; crowdsourcing; emotion; mobile; domain adaptation; machine learning; deep learning},
  keywords-plus              = {AUTISM; RECOGNITION},
  language                   = {English},
  number-of-cited-references = {30},
  oa                         = {Bronze, Green Accepted},
  orcid-numbers              = {washington, peter/0000-0003-3276-4411 Wall, Dennis/0000-0002-7889-9146 JEDOUI, Khaled/0000-0001-9838-6658},
  priority                   = {prio1},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Computer Science},
  times-cited                = {11},
  type                       = {Article},
  unique-id                  = {WOS:000544034800009},
  usage-count-last-180-days  = {6},
  usage-count-since-2013     = {12},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Software Engineering},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000766268600030,
  author                     = {Zhang, Tong and Wang, Xuehan and Xu, Xiangmin and Chen, C. L. Philip},
  journal                    = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title                      = {GCB-Net: Graph Convolutional Broad Network and Its Application in Emotion Recognition},
  year                       = {2022},
  issn                       = {1949-3045},
  month                      = {JAN},
  number                     = {1},
  pages                      = {379-388},
  volume                     = {13},
  status                     = {Aceito},
  abstract                   = {In recent years, emotion recognition has become a research focus in the
   area of artificial intelligence. Due to its irregular structure, EEG
   data can be analyzed by applying graphical based algorithms or models
   much more efficiently. In this work, a Graph Convolutional Broad Network
   (GCB-net) was designed for exploring the deeper-level information of
   graph-structured data. It used the graph convolutional layer to extract
   features of graph-structured input and stacks multiple regular
   convolutional layers to extract relatively abstract features. The final
   concatenation utilized the broad concept, which preserves the outputs of
   all hierarchical layers, allowing the model to search features in broad
   spaces. To improve the performance of the proposed GCB-net, the broad
   learning system (BLS) was applied to enhance its features. For
   comparison, two individual experiments were conducted to examine the
   efficiency of the proposed GCB-net based on the SJTU emotion EEG dataset
   (SEED) and DREAMER dataset respectively. In SEED, compared with other
   state-of-art methods, the GCB-net could better promote the accuracy
   (reaching 94.24 percent) on the DE feature of the all-frequency band. In
   DREAMER dataset, GCB-net performed better than other models with the
   same setting. Furthermore, the GCB-net reached high accuracies of 86.99,
   89.32 and 89.20 percent on dimensions of Valence, Arousal and Dominance
   respectively. The experimental results showed the robust classifying
   ability of the GCB-net and BLS in EEG emotion recognition.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Zhang, T (Corresponding Author), South China Univ Technol, Sch Comp Sci \& Engn, Guangzhou 510006, Peoples R China. Zhang, Tong; Wang, Xuehan; Chen, C. L. Philip, South China Univ Technol, Sch Comp Sci \& Engn, Guangzhou 510006, Peoples R China. Xu, Xiangmin, South China Univ Technol, Sch Elect \& Informat, Guangzhou 510006, Peoples R China.},
  author-email               = {tony@scut.edu.cn 373289477@qq.com xmxu@scut.edu.cn Philip.Chen@ieee.org},
  da                         = {2022-09-28},
  doc-delivery-number        = {ZP2QE},
  doi                        = {10.1109/TAFFC.2019.2937768},
  esi-highly-cited-paper     = {Y},
  esi-hot-paper              = {Y},
  funding-acknowledgement    = {Key Laboratory of Child Development and Learning Science (Southeast University); Brain-like Computing and Machine Intelligence (BCMI) Laboratory; University of the West of Scotland, School of Engineering and Computing; National Natural Science Foundation of China {[}61702195, 61751202, U1813203, 61572540, U1801262]},
  funding-text               = {The authors would like to thank the support of Key Laboratory of Child Development and Learning Science (Southeast University), Brain-like Computing and Machine Intelligence (BCMI) Laboratory and University of the West of Scotland, School of Engineering and Computing. This work was supported in part by the National Natural Science Foundation of China grant under number 61702195, 61751202, U1813203, 61572540, U1801262.},
  journal-iso                = {IEEE Trans. Affect. Comput.},
  keywords                   = {Emotion recognition; graph convolutional neural networks; broad learning system; graph convolutional broad network},
  keywords-plus              = {EEG; SYSTEM; APPROXIMATION; ENTROPY; SIGNALS; GESTURE},
  language                   = {English},
  number-of-cited-references = {47},
  priority                   = {prio3},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Computer Science},
  times-cited                = {109},
  type                       = {Article},
  unique-id                  = {WOS:000766268600030},
  usage-count-last-180-days  = {93},
  usage-count-since-2013     = {98},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Cybernetics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000562697400005,
  author                     = {Jarraya, Salma Kammoun and Masmoudi, Marwa and Hammami, Mohamed},
  journal                    = {MULTIMEDIA TOOLS AND APPLICATIONS},
  title                      = {A comparative study of Autistic Children Emotion recognition based on patio-Temporal and Deep analysis of facial expressions features during a Meltdown Crisis},
  year                       = {2021},
  issn                       = {1380-7501},
  month                      = {JAN},
  number                     = {1},
  pages                      = {83-125},
  volume                     = {80},
  status                     = {Aceito},
  abstract                   = {The recognition of human emotion is a significant contribution to many
   computer vision appli-cations. Despite its importance, this work is the
   first one towards an automatic Autistic Children emotion recognition
   system to ensure their security during meltdown crisis. The current
   solutions to handle a meltdown crisis are based on a preventive
   approach. Indeed, Meltdown symptoms are determined by abnormal facial
   expressions related to compound emotions. To provide for this
   correspondence, we experimentally evaluate, in this paper, hand-crafted
   Geometric Spatio-Temporal and Deep features of realistic autistic
   children facial expressions. Towards this end, we compared the Compound
   Emotion Recognition (CER) performance for different combinations of
   these features, and we determined the features that best distinguish a
   Compound Emotion (CE) of autistic children during a meltdown crisis from
   the normal state. We used ``Meltdown crisis{''}(1)dataset to conduct our
   experiments on realistic Meltdown / Normal scenarios of autistic
   children. In this evaluation, we show that the gathered features can
   lead to very encouraging performances through the use ofRandom
   Forestclassifier (91.27\%) with hand-crafted features. Moreover,
   classifiers trained on deep features fromInceptionResnetV2show higher
   performance (97.5\%) with supervised learning techniques.},
  address                    = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  affiliation                = {Masmoudi, M (Corresponding Author), Univ Sfax, Mir Cl Lab, Sfax, Tunisia. Jarraya, Salma Kammoun; Masmoudi, Marwa; Hammami, Mohamed, Univ Sfax, Mir Cl Lab, Sfax, Tunisia. Jarraya, Salma Kammoun, King Abdulaziz Univ, Fac Comp \& Informat Technol, CS Dept, Jeddah, Saudi Arabia. Hammami, Mohamed, Sfax Univ, Fac Sci, Dept Comp Sci, Sfax, Tunisia.},
  author-email               = {smohamad1@kau.edu.sa marwa.masmoudi19@gmail.com Mohamed.Hammami@fss.rnu.tn},
  da                         = {2022-09-28},
  doc-delivery-number        = {PR6IB},
  doi                        = {10.1007/s11042-020-09451-y},
  earlyaccessdate            = {AUG 2020},
  eissn                      = {1573-7721},
  journal-iso                = {Multimed. Tools Appl.},
  keywords                   = {Autism; Meltdown crisis; Facial expressions; Compound emotions; Spatio-temporal Features; CNN},
  keywords-plus              = {SELECTION TECHNIQUES},
  language                   = {English},
  number-of-cited-references = {58},
  orcid-numbers              = {Masmoudi, Marwa/0000-0002-5248-8525},
  priority                   = {prio3},
  publisher                  = {SPRINGER},
  research-areas             = {Computer Science; Engineering},
  times-cited                = {8},
  type                       = {Article},
  unique-id                  = {WOS:000562697400005},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {10},
  web-of-science-categories  = {Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory \& Methods; Engineering, Electrical \& Electronic},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000475910800001,
  author                     = {Barrett, Lisa Feldman and Adolphs, Ralph and Marsella, Stacy and Martinez, Aleix M. and Pollak, Seth D.},
  journal                    = {PSYCHOLOGICAL SCIENCE IN THE PUBLIC INTEREST},
  title                      = {Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements},
  year                       = {2019},
  issn                       = {1529-1006},
  month                      = {JUL},
  number                     = {1},
  pages                      = {1-68},
  volume                     = {20},
  status                     = {Aceito},
  abstract                   = {It is commonly assumed that a person's emotional state can be readily
   inferred from his or her facial movements, typically called emotional
   expressions or facial expressions. This assumption influences legal
   judgments, policy decisions, national security protocols, and
   educational practices; guides the diagnosis and treatment of psychiatric
   illness, as well as the development of commercial applications; and
   pervades everyday social interactions as well as research in other
   scientific fields such as artificial intelligence, neuroscience, and
   computer vision. In this article, we survey examples of this widespread
   assumption, which we refer to as the common view, and we then examine
   the scientific evidence that tests this view, focusing on the six most
   popular emotion categories used by consumers of emotion research: anger,
   disgust, fear, happiness, sadness, and surprise. The available
   scientific evidence suggests that people do sometimes smile when happy,
   frown when sad, scowl when angry, and so on, as proposed by the common
   view, more than what would be expected by chance. Yet how people
   communicate anger, disgust, fear, happiness, sadness, and surprise
   varies substantially across cultures, situations, and even across people
   within a single situation. Furthermore, similar configurations of facial
   movements variably express instances of more than one emotion category.
   In fact, a given configuration of facial movements, such as a scowl,
   often communicates something other than an emotional state. Scientists
   agree that facial movements convey a range of information and are
   important for social communication, emotional or otherwise. But our
   review suggests an urgent need for research that examines how people
   actually move their faces to express emotions and other social
   information in the variety of contexts that make up everyday life, as
   well as careful study of the mechanisms by which people perceive
   instances of emotion in one another. We make specific research
   recommendations that will yield a more valid picture of how people move
   their faces to express emotions and how they infer emotional meaning
   from facial movements in situations of everyday life. This research is
   crucial to provide consumers of emotion research with the translational
   information they require.},
  address                    = {1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND},
  affiliation                = {Barrett, LF (Corresponding Author), Northeastern Univ, 125 Nightingale Hall, Boston, MA 02115 USA. Barrett, Lisa Feldman; Marsella, Stacy, Northeastern Univ, Dept Biol, Boston, MA 02115 USA. Barrett, Lisa Feldman, Massachusetts Gen Hosp, Dept Psychiat, Boston, MA 02114 USA. Barrett, Lisa Feldman, Massachusetts Gen Hosp, Athinoula A Martinos Ctr Biomed Imaging, Boston, MA 02114 USA. Adolphs, Ralph, CALTECH, Div Humanities \& Social Sci, Pasadena, CA 91125 USA. Marsella, Stacy, Northeastern Univ, Coll Comp \& Informat Sci, Boston, MA 02115 USA. Marsella, Stacy, Univ Glasgow, Inst Neurosci \& Psychol, Glasgow, Lanark, Scotland. Martinez, Aleix M., Ohio State Univ, Dept Elect \& Comp Engn, Columbus, OH 43210 USA. Martinez, Aleix M., Ohio State Univ, Ctr Cognit \& Brain Sci, Columbus, OH 43210 USA. Pollak, Seth D., Univ Wisconsin, Dept Psychol, Madison, WI 53706 USA.},
  author-email               = {l.barrett@northeastern.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {IJ4YU},
  doi                        = {10.1177/1529100619832930},
  eissn                      = {1539-6053},
  esi-highly-cited-paper     = {Y},
  esi-hot-paper              = {N},
  funding-acknowledgement    = {U.S. Army Research Institute for the Behavioral and Social Sciences {[}W911NF-16-1-019]; National Cancer Institute {[}U01-CA193632]; National Institute of Mental Health {[}R01-MH113234, R01-MH109464, 2P50-MH094258, R01-MH61285]; National Science Foundation Civil, Mechanical and Manufacturing Innovation Grant {[}1638234]; National Institute on Deafness and Other Communication Disorders {[}R01-DC014498]; National Eye Institute {[}R01-EY020834]; Human Frontier Science Program {[}RGP0036/2016]; Eunice Kennedy Shriver National Institute of Child Health and Human Development {[}U54-HD090256]; James McKeen Cattell Fund Fellowship; Air Force Office of Scientific Research Grant {[}FA9550-14-1-0364]; EUNICE KENNEDY SHRIVER NATIONAL INSTITUTE OF CHILD HEALTH \& HUMAN DEVELOPMENT {[}U54HD090256] Funding Source: NIH RePORTER; NATIONAL CANCER INSTITUTE {[}U01CA193632] Funding Source: NIH RePORTER; NATIONAL EYE INSTITUTE {[}R01EY020834] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF MENTAL HEALTH {[}R01MH061285, P50MH094258, R01MH113234, R01MH109464] Funding Source: NIH RePORTER; NATIONAL INSTITUTE ON DEAFNESS AND OTHER COMMUNICATION DISORDERS {[}R01DC014498] Funding Source: NIH RePORTER},
  funding-text               = {This work was supported by U.S. Army Research Institute for the Behavioral and Social Sciences Grant W911NF-16-1-019 (to L. F. Barrett); National Cancer Institute Grant U01-CA193632 (to L. F. Barrett); National Institute of Mental Health Grants R01-MH113234 and R01-MH109464 (to L. F. Barrett), 2P50-MH094258 (to R. Adolphs), and R01-MH61285 (to S. D. Pollak); National Science Foundation Civil, Mechanical and Manufacturing Innovation Grant 1638234 (to L. F. Barrett and S. Marsella); National Institute on Deafness and Other Communication Disorders Grant R01-DC014498 (to A. M. Martinez); National Eye Institute Grant R01-EY020834 (to A. M. Martinez); Human Frontier Science Program Grant RGP0036/2016 (to A. M. Martinez); Eunice Kennedy Shriver National Institute of Child Health and Human Development Grant U54-HD090256 (to S. D. Pollak); a James McKeen Cattell Fund Fellowship (to S. D. Pollak); and Air Force Office of Scientific Research Grant FA9550-14-1-0364 (to S. Marsella).},
  journal-iso                = {Psychol. Sci. Public Interest},
  keywords                   = {emotion perception; emotional expression; emotion recognition},
  keywords-plus              = {CONGENITALLY BLIND; DEVELOPMENTAL-CHANGES; BASIC EMOTIONS; FORCED-CHOICE; CATEGORICAL PERCEPTION; CHILDRENS RECOGNITION; CULTURAL-DIFFERENCES; INFANTS PERCEPTION; EARLY EXPERIENCE; DYNAMIC ASPECTS},
  language                   = {English},
  number-of-cited-references = {389},
  oa                         = {Green Accepted, Bronze},
  orcid-numbers              = {Barrett, Lisa/0000-0003-4478-2051},
  priority                   = {prio2},
  publisher                  = {SAGE PUBLICATIONS LTD},
  research-areas             = {Psychology},
  researcherid-numbers       = {Barrett, Lisa Feldman/ABC-8157-2020},
  times-cited                = {388},
  type                       = {Article},
  unique-id                  = {WOS:000475910800001},
  usage-count-last-180-days  = {41},
  usage-count-since-2013     = {259},
  web-of-science-categories  = {Psychology, Multidisciplinary},
  web-of-science-index       = {Social Science Citation Index (SSCI)},
}

@Article{WOS:000548230800001,
  author                     = {Zampella, Casey J. and Bennetto, Loisa and Herrington, John D.},
  journal                    = {AUTISM RESEARCH},
  title                      = {Computer Vision Analysis of Reduced Interpersonal Affect Coordination in Youth With Autism Spectrum Disorder},
  year                       = {2020},
  issn                       = {1939-3792},
  month                      = {DEC},
  number                     = {12},
  pages                      = {2133-2142},
  volume                     = {13},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Atypical social-emotional reciprocity is a core feature of autism
   spectrum disorder (ASD) but can be difficult to operationalize. One
   measurable manifestation of reciprocity may be interpersonal
   coordination, the tendency to align the form and timing of one's
   behaviors (including facial affect) with others. Interpersonal affect
   coordination facilitates sharing and understanding of emotional cues,
   and there is evidence that it is reduced in ASD. However, most research
   has not measured this process in true social contexts, due in part to a
   lack of tools for measuring dynamic facial expressions over the course
   of an interaction. Automated facial analysis via computer vision
   provides an efficient, granular, objective method for measuring
   naturally occurring facial affect and coordination. Youth with ASD and
   matched typically developing youth participated in cooperative
   conversations with their mothers and unfamiliar adults.
   Time-synchronized videos were analyzed with an open-source computer
   vision toolkit for automated facial analysis, for the presence and
   intensity of facial movements associated with positive affect. Both
   youth and adult conversation partners exhibited less positive affect
   during conversations when the youth partner had ASD. Youth with ASD also
   engaged in less affect coordination over the course of conversations.
   When considered dimensionally across youth with and without ASD, affect
   coordination significantly predicted scores on rating scales of
   autism-related social atypicality, adaptive social skills, and empathy.
   Findings suggest that affect coordination is an important interpersonal
   process with implications for broader social-emotional functioning. This
   preliminary study introduces a promising novel method for quantifying
   moment-to-moment facial expression and emotional reciprocity during
   natural interactions. Lay Summary This study introduces a novel,
   automated method for measuring social-emotional reciprocity during
   natural conversations, which may improve assessment of this core autism
   diagnostic behavior. We used computerized methods to measure facial
   affect and the degree of affect coordination between conversation
   partners. Youth with autism displayed reduced affect coordination, and
   reduced affect coordination predicted lower scores on measures of
   broader social-emotional skills.},
  address                    = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
  affiliation                = {Zampella, CJ (Corresponding Author), Childrens Hosp Philadelphia, Ctr Autism Res, Philadelphia, PA 19104 USA. Zampella, Casey J.; Herrington, John D., Childrens Hosp Philadelphia, Ctr Autism Res, Philadelphia, PA 19104 USA. Bennetto, Loisa, Univ Rochester, Dept Psychol, Meliora Hall, Rochester, NY 14627 USA. Herrington, John D., Univ Penn, Perelman Sch Med, Dept Psychiat, Philadelphia, PA 19104 USA.},
  author-email               = {zampellac@email.chop.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {PG3IQ},
  doi                        = {10.1002/aur.2334},
  earlyaccessdate            = {JUL 2020},
  eissn                      = {1939-3806},
  funding-acknowledgement    = {McMorris Family Foundation; National Institute on Deafness and Other Communication Disorders {[}R01 DC009439]},
  funding-text               = {We are grateful to the families that participated in this research. This research was supported by grants from the McMorris Family Foundation and the National Institute on Deafness and Other Communication Disorders (R01 DC009439). We would also like to thank Laura Soskey Cubit, Jessica Keith, Kim Schauder, Kelsey Csumitta, Paul Allen, and our undergraduate research assistants for their invaluable contributions.},
  journal-iso                = {Autism Res.},
  keywords                   = {affect; emotion; social-emotional reciprocity; computer vision; interpersonal coordination; facial expression; synchrony},
  keywords-plus              = {EMOTIONAL FACIAL EXPRESSIONS; FACE-TO-FACE; MIMICRY; SYNCHRONY; RECOGNITION; MECHANISMS; INFORMATION; VOLUNTARY; CHILDREN; EMPATHY},
  language                   = {English},
  number-of-cited-references = {59},
  oa                         = {Green Accepted},
  orcid-numbers              = {Zampella, Casey/0000-0002-7973-8520},
  publisher                  = {WILEY},
  research-areas             = {Behavioral Sciences; Psychology},
  times-cited                = {5},
  type                       = {Article},
  unique-id                  = {WOS:000548230800001},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {14},
  web-of-science-categories  = {Behavioral Sciences; Psychology, Developmental},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@InProceedings{WOS:000470221500006,
  author                     = {Irani, Atefeh and Moradi, Hadi and Vahid, Leila Kashani},
  booktitle                  = {2018 2ND NATIONAL AND 1ST INTERNATIONAL DIGITAL GAMES RESEARCH CONFERENCE: TRENDS, TECHNOLOGIES, AND APPLICATIONS (DGRC)},
  title                      = {Autism Screening Using a Video Game Based on Emotions},
  year                       = {2018},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {2nd National and 1st International Digital Games Research Conference - Trends, Technologies, and Applications (DGRC), Tehran, IRAN, NOV 29-30, 2018},
  organization               = {DIREC; Iran Comp \& Video Games Fdn; Univ Islam Republ Iran Broadcasting; Digital Games Res Ctr},
  pages                      = {40-45},
  publisher                  = {IEEE},
  status                     = {Aceito},
  abstract                   = {Autism spectrum disorders (ASDs) is referred to as a range of mental and
   physical. Usually these children deal with disorders such as social
   problems, repeated patterns of behaviors and inability to understand
   abstract concepts. If at the very childhood the occupational therapy
   exercises are done on a person with autism, he can return to normal
   life. Given the importance of early diagnosis of autism, this study
   presents the structure of a serious smart game for screening and
   rehabilitation of autism. This computer game is based on training of
   three important skills in understanding emotions i. e. the recognition
   of emotions, the emergence of emotions, and emotion regulation. The game
   was performed on children with autism and normal children for designing
   pattern-recognition algorithms for screening children with autism. Then
   the screening precision reached 93\% using a support vector machine with
   Gaussian kernel.},
  affiliation                = {Irani, A (Corresponding Author), Univ Tehran, Sch ECE, Adv Robot \& Intelligent Syst Lab, Tehran, Iran. Irani, Atefeh, Univ Tehran, Sch ECE, Adv Robot \& Intelligent Syst Lab, Tehran, Iran. Moradi, Hadi, SKKU, Adjunct Res Prof Intelligent Syst Res Inst, Tehran, Iran. Vahid, Leila Kashani, Azad Univ, Sci \& Res Branch, Dept Psychol \& Special Educ, Tehran, Iran.},
  author-email               = {a.irani@utae.ir moradih@utae.ir lkashani@ut.ac.ir},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BM9AE},
  isbn                       = {978-1-7281-1114-8},
  keywords                   = {autism; Serious Game; emotion expression; emotion recognition; emotion regulation; autism screening},
  language                   = {English},
  number-of-cited-references = {11},
  priority                   = {prio1},
  research-areas             = {Computer Science; Engineering},
  researcherid-numbers       = {Vahid, Leila Kashani/AAT-9473-2021 Kashani Vahid, Leila/GSJ-3704-2022},
  times-cited                = {1},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000470221500006},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {6},
  web-of-science-categories  = {Computer Science, Software Engineering; Computer Science, Theory \& Methods; Engineering, Electrical \& Electronic},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000474504900002,
  author                     = {Samad, Manar D. and Diawara, Norou and Bobzien, Jonna L. and Taylor, Cora M. and Harrington, John W. and Iftekharuddin, Khan M.},
  journal                    = {RESEARCH IN AUTISM SPECTRUM DISORDERS},
  title                      = {A pilot study to identify autism related traits in spontaneous facial actions using computer vision},
  year                       = {2019},
  issn                       = {1750-9467},
  month                      = {SEP},
  pages                      = {14-24},
  volume                     = {65},
  status                     = {Aceito},
  abstract                   = {Background: Individuals with autism spectrum disorders (ASD) may be
   differentiated from typically developing controls (TDC) based on
   phenotypic features in spontaneous facial expressions. Computer vision
   technology can automatically track subtle facial actions to gain
   quantitative insights into ASD related behavioral abnormalities.
   Method: This study proposes a novel psychovisual human-study to elicit
   spontaneous facial expressions in response to a variety of social and
   emotional contexts. We introduce a markerless facial motion capture and
   computer vision methods to track spontaneous and subtle activations of
   facial muscles. The facial muscle activations are encoded into ten
   representative facial action units (FAU) to gain quantitative, granular,
   and contextual insights into the psychophysical development of the
   participating individuals. Statistical tests are performed to identify
   differential traits in individuals with ASD after comparing those in a
   cohort of age-matched TDC individuals.
   Results: The proposed framework has revealed significant difference (p <
   0.001) in the activation of ten FAU and contrasting activations of FAU
   between the group with ASD and the TDC group. Unlike the TDC group, the
   group with ASD has shown unusual prevalence of mouth frown (FAU 15) and
   low correlations in temporal activations of several FAU pairs: 6-12,
   10-12, and 10-20. The interpretation of different FAU activations
   suggests quantitative evidence of expression bluntness, lack of
   expression mimicry, incongruent reaction to negative emotions in the
   group with ASD.
   Conclusion: Our generalized framework may be used to quantify
   psychophysical traits in individuals with ASD and replicate in similar
   studies that require quantitative measurements of behavioral responses.},
  address                    = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
  affiliation                = {Samad, MD (Corresponding Author), Tennessee State Univ, Dept Comp Sci, 3500 John A Merritt Blvd,POB 9604, Nashville, TN 37203 USA. Samad, Manar D.; Iftekharuddin, Khan M., Old Dominion Univ, Dept Elect \& Comp Engn, Norfolk, VA USA. Diawara, Norou, Old Dominion Univ, Dept Math \& Stat, Norfolk, VA 23529 USA. Bobzien, Jonna L., Old Dominion Univ, Dept Commun Disorder \& Special Educ, Norfolk, VA USA. Taylor, Cora M., Geisinger, Dept Clin Psychol, Lewisburg, PA USA. Harrington, John W., Childrens Hosp Kings Daughters, Dept Pediat, Norfolk, VA USA.},
  author-email               = {msama005@odu.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {IH5AZ},
  doi                        = {10.1016/j.rasd.2019.05.001},
  eissn                      = {1878-0237},
  funding-acknowledgement    = {Old Dominion University Office of Research Multidisciplinary Funding Program {[}300261-010]; NSF {[}ECCS 1310353]},
  funding-text               = {This work has been partially funded by a grant from Old Dominion University Office of Research Multidisciplinary Funding Program (\#300261-010) and a grant from NSF (ECCS 1310353).},
  journal-iso                = {Res. Autism Spectr. Disord.},
  keywords                   = {ASD; Behavioral marker; Differential traits; Facial action units; Computer vision; Spontaneous expressions},
  keywords-plus              = {SPECTRUM; EXPRESSIONS; CHILDREN; INDIVIDUALS; DEFICITS; RECOGNITION; FRAMEWORK; RESPONSES; VOLUNTARY; MIMICRY},
  language                   = {English},
  number-of-cited-references = {37},
  orcid-numbers              = {Iftekharuddin, Khan/0000-0001-8316-4163 Samad, Manar/0000-0002-6263-6261 Diawara, Norou/0000-0002-8403-6793},
  priority                   = {prio3},
  publisher                  = {ELSEVIER SCI LTD},
  research-areas             = {Education \& Educational Research; Psychology; Psychiatry; Rehabilitation},
  researcherid-numbers       = {Iftekharuddin, Khan/AAT-5217-2020},
  times-cited                = {7},
  type                       = {Article},
  unique-id                  = {WOS:000474504900002},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {18},
  web-of-science-categories  = {Education, Special; Psychology, Developmental; Psychiatry; Rehabilitation},
  web-of-science-index       = {Social Science Citation Index (SSCI)},
}

@InProceedings{WOS:000457843602030,
  author                     = {Marinoiu, Elisabeta and Zanfir, Mihai and Olaru, Vlad and Sminchisescu, Cristian},
  booktitle                  = {2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
  title                      = {3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children with Autism},
  year                       = {2018},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {31st IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, JUN 18-23, 2018},
  organization               = {IEEE; CVF; IEEE Comp Soc},
  pages                      = {2158-2167},
  publisher                  = {IEEE},
  series                     = {IEEE Conference on Computer Vision and Pattern Recognition},
  status                     = {Aceito},
  abstract                   = {We introduce new, fine-grained action and emotion recognition tasks
   defined on non-staged videos, recorded during robot-assisted therapy
   sessions of children with autism. The tasks present several challenges:
   a large dataset with long videos, a large number of highly variable
   actions, children that are only partially visible, have different ages
   and may show unpredictable behaviour, as well as non-standard camera
   viewpoints. We investigate how state-of-the-art 3d human pose
   reconstruction methods perform on the newly introduced tasks and propose
   extensions to adapt them to deal with these challenges. We also analyze
   multiple approaches in action and emotion recognition from 3d human pose
   data, establish several baselines, and discuss results and their
   implications in the context of child-robot interaction.},
  affiliation                = {Marinoiu, E (Corresponding Author), Romanian Acad, Inst Math, Bucharest, Romania. Sminchisescu, Cristian, Lund Univ, Fac Engn, Dept Math, Lund, Sweden. Marinoiu, Elisabeta; Zanfir, Mihai; Olaru, Vlad; Sminchisescu, Cristian, Romanian Acad, Inst Math, Bucharest, Romania.},
  author-email               = {elisabeta.marinoiu@imar.ro mihai.zanfir@imar.ro vlad.olaru@imar.ro cristian.sminchisescu@math.lth.se},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BL9NZ},
  doi                        = {10.1109/CVPR.2018.00230},
  funding-acknowledgement    = {EU {[}688835 DE-ENIGMA]; European Research Council Consolidator grant SEED, CNCS-UEFISCDI {[}PN-III-P4-ID-PCE-2016-0535]; SSF},
  funding-text               = {This work was supported in part by the EU Horizon 2020 Grant No. 688835 DE-ENIGMA, European Research Council Consolidator grant SEED, CNCS-UEFISCDI PN-III-P4-ID-PCE-2016-0535, and SSF. Corresponding authors: Vlad Olaru and Cristian Sminchisescu.},
  isbn                       = {978-1-5386-6420-9},
  issn                       = {1063-6919},
  keywords-plus              = {COLLABORATIVE PLAY; KASPAR},
  language                   = {English},
  number-of-cited-references = {33},
  priority                   = {prio2},
  research-areas             = {Computer Science},
  researcherid-numbers       = {Zanfir, Mihai/AAG-7817-2021},
  times-cited                = {33},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000457843602030},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {10},
  web-of-science-categories  = {Computer Science, Artificial Intelligence},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000719224600003,
  author                     = {Uluer, Pinar and Kose, Hatice and Gumuslu, Elif and Barkana, Duygun Erol},
  journal                    = {INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS},
  title                      = {Experience with an Affective Robot Assistant for Children with Hearing Disabilities},
  issn                       = {1875-4791},
  abstract                   = {This study presents an assistive robotic system enhanced with emotion
   recognition capabilities for children with hearing disabilities. The
   system is designed and developed for the audiometry tests and
   rehabilitation of children in a clinical setting and includes a social
   humanoid robot (Pepper), an interactive interface, gamified audiometry
   tests, sensory setup and a machine/deep learning based emotion
   recognition module. Three scenarios involving conventional setup, tablet
   setup and setup with the robot+tablet are evaluated with 16 children
   having cochlear implant or hearing aid. Several machine learning
   techniques and deep learning models are used for the classification of
   the three test setups and for the classification of the emotions
   (pleasant, neutral, unpleasant) of children using the recorded
   physiological signals by E4 wristband. The results show that the
   collected signals during the tests can be separated successfully and the
   positive and negative emotions of children can be better distinguished
   when they interact with the robot than in the other two setups. In
   addition, the children's objective and subjective evaluations as well as
   their impressions about the robot and its emotional behaviors are
   analyzed and discussed extensively.},
  address                    = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  affiliation                = {Uluer, P (Corresponding Author), Galatasaray Univ, Dept Comp Engn, Istanbul, Turkey. Uluer, P (Corresponding Author), Istanbul Tech Univ, Dept AI \& Data Engn, Istanbul, Turkey. Uluer, Pinar, Galatasaray Univ, Dept Comp Engn, Istanbul, Turkey. Uluer, Pinar; Kose, Hatice, Istanbul Tech Univ, Dept AI \& Data Engn, Istanbul, Turkey. Gumuslu, Elif; Barkana, Duygun Erol, Yeditepe Univ, Dept Elect \& Elect Engn, Istanbul, Turkey.},
  author-email               = {puluer@gsu.edu.tr hatice.kose@itu.edu.tr elif.gumuslu@std.yeditepe.edu.tr duygunerol@yeditepe.edu.tr},
  da                         = {2022-09-28},
  doc-delivery-number        = {WY4BT},
  doi                        = {10.1007/s12369-021-00830-5},
  earlyaccessdate            = {NOV 2021},
  eissn                      = {1875-4805},
  funding-acknowledgement    = {Scientific and Technological Research Council of Turkey (TUBITAK) {[}118E214]},
  funding-text               = {Research supported by The Scientific and Technological Research Council of Turkey (TUB.ITAK) under the grant number 118E214.},
  journal-iso                = {Int. J. Soc. Robot.},
  keywords                   = {Social robots; Human-robot interaction; Machine learning; Deep learning; Emotion recognition; Physiological signals},
  keywords-plus              = {EMOTION; RECOGNITION; SIGNALS; AUTISM},
  language                   = {English},
  number-of-cited-references = {82},
  oa                         = {Green Published, Bronze},
  orcid-numbers              = {Barkana, Duygun Erol/0000-0002-8929-0459},
  publisher                  = {SPRINGER},
  research-areas             = {Robotics},
  researcherid-numbers       = {Barkana, Duygun Erol/N-2285-2018},
  times-cited                = {2},
  type                       = {Article; Early Access},
  unique-id                  = {WOS:000719224600003},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {5},
  web-of-science-categories  = {Robotics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@InProceedings{WOS:000483076402203,
  author                     = {Aslam, Abdul Rehman and Bin Altaf, Muhammad Awais},
  booktitle                  = {2019 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)},
  title                      = {An 8 Channel Patient Specific Neuromorphic Processor for the early screening of Autistic Children through Emotion Detection},
  year                       = {2019},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {IEEE International Symposium on Circuits and Systems (IEEE ISCAS), Sapporo, JAPAN, MAY 26-29, 2019},
  organization               = {IEEE; IEEE Circuits \& Syst Soc; Hokkaido Univ, Global Inst Collaborat Res \& Educ, Big Data \& Cybersecur; Springer Nature; River Publishers; Sci Council Japan; IEEE Circuits \& Syst Soc Japan Joint Chapter; IEEE Circuits \& Syst Soc Fukuoka Chapter; IEEE Circuits \& Syst Soc Kansai Chapter; IEEE Circuits \& Syst Soc Shikoku Chapter},
  publisher                  = {IEEE},
  series                     = {IEEE International Symposium on Circuits and Systems},
  status                     = {Aceito},
  abstract                   = {Autism Spectrum Disorder (ASD) is a neurodevelopment disorder that
   affects children's development and can lead to handicap life if remain
   untreated. Scalp Electroencephalography (EEG) data can be used as a
   biomarker to characterize the human emotions on the valence-arousal
   scale. This work presents a machine learning patient-specific emotion
   detection (PSED) classification processor based on an eight-channel EEG
   signal. The proposed PSED classification processor integrates a
   hardware-efficient feature extraction engine and patient-specific
   support vector machine (SVM) classifier to discriminate the emotions in
   real-time. To utilize minimal hardware resources a hardware realizable
   feature set comprising of power spectral density (PSD), an absolute
   difference of interhemispheric power asymmetry (IHPD), and the scaled
   interhemispheric power asymmetry ratio (SIHPR) of eight electrode pairs
   are evaluated. To avoid high overhead of area and power consumption for
   an integer divider for SIHPR; simple LUT based divider is proposed that
   calculates the approximated value of SIHPR with a minimal overhead of 64
   Bytes. The classification is performed using a Linear SVM and resulted
   in an accuracy of 63\% and 60\% for valence and arousal, respectively,
   based on the database for emotion analysis using physiological signals
   (DEAP). The PSED processor is synthesized using a 65nm CMOS technology
   with an overall energy efficiency of 10uJ/classification.},
  affiliation                = {Aslam, AR (Corresponding Author), Lahore Univ Management Sci, Lahore, Pakistan. Aslam, Abdul Rehman; Bin Altaf, Muhammad Awais, Lahore Univ Management Sci, Lahore, Pakistan.},
  author-email               = {17060056@lums.edu.pk awais.altaf@lums.edu.pk},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BN5FJ},
  funding-acknowledgement    = {Lahore University of Management Sciences (LUMS), Lahore, Pakistan Faculty Initiative Fund {[}FIF441-1819-EED]},
  funding-text               = {This work was funded by the Lahore University of Management Sciences (LUMS), Lahore, Pakistan Faculty Initiative Fund (FIF441-1819-EED).},
  isbn                       = {978-1-7281-0397-6},
  issn                       = {0271-4302},
  keywords                   = {arousal; autism spectrum disorder (ASD); electroencephalogram (EEG); emotion; a look-up table (LUT); support vector machine (SVM); valence},
  keywords-plus              = {RECOGNITION; VOLTAGE},
  language                   = {English},
  number-of-cited-references = {24},
  orcid-numbers              = {Bin Altaf, Muhammad Awais/0000-0003-3615-3546 ASLAM, ABDUL REHMAN/0000-0001-7767-2575},
  priority                   = {prio2},
  research-areas             = {Engineering},
  researcherid-numbers       = {Bin Altaf, Muhammad Awais/T-5271-2019 ASLAM, ABDUL REHMAN/AAH-6364-2021},
  times-cited                = {13},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000483076402203},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {2},
  web-of-science-categories  = {Engineering, Electrical \& Electronic},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@InProceedings{WOS:000461314200089,
  author                     = {Ul Haque, Md Inzamam and Valles, Damian},
  booktitle                  = {2018 IEEE 9TH ANNUAL INFORMATION TECHNOLOGY, ELECTRONICS AND MOBILE COMMUNICATION CONFERENCE (IEMCON)},
  title                      = {A Facial Expression Recognition Approach Using DCNN for Autistic Children to Identify Emotions},
  year                       = {2018},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  editor                     = {Chakrabarti, S and Saha, HN},
  note                       = {9th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), Univ British Columbia, Vancouver, CANADA, NOV 01-03, 2018},
  organization               = {Inst Engn \& Management; IEEE Vancouver Sect; UBC; Univ Engn \& Management},
  pages                      = {546-551},
  publisher                  = {IEEE},
  status                     = {Aceito},
  abstract                   = {In this paper, an initial work of a research is discussed which is to
   teach young autistic children recognizing human facial expression with
   the help of computer vision and image processing. This paper mostly
   discusses the initial work of facial expression recognition using a deep
   convolutional neural network. The Kaggle's FER2013 dataset has been used
   to train and experiment with a deep convolutional neural network model.
   Once a satisfactory result is achieved, the dataset is modified with
   pictures of four different lighting conditions and each of these
   datasets is again trained with the same model. This is necessary for the
   end goal of the research which is to recognize facial expression in any
   possible environment. Finally, the comparison between results with
   different datasets is discussed and future work of the project is
   outlined.},
  affiliation                = {Ul Haque, MI (Corresponding Author), Texas State Univ, Ingram Sch Engn, San Marcos, TX 78666 USA. Ul Haque, Md Inzamam; Valles, Damian, Texas State Univ, Ingram Sch Engn, San Marcos, TX 78666 USA.},
  author-email               = {m\_h536@txstate.edu dvalles@txstate.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {BM2QM},
  isbn                       = {978-1-5386-7266-2},
  keywords                   = {Facial Expression Recognition; Autistic Children; DCNN; Loss; Accuracy},
  language                   = {English},
  number-of-cited-references = {22},
  orcid-numbers              = {Valles, Damian/0000-0002-5684-9498},
  priority                   = {prio1},
  research-areas             = {Engineering; Telecommunications},
  researcherid-numbers       = {Valles, Damian/AAV-2603-2021},
  times-cited                = {13},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000461314200089},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {2},
  web-of-science-categories  = {Engineering, Electrical \& Electronic; Telecommunications},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{ WOS:000460064700015,
Author = {Camacho, M. Catalina and Karim, Helmet T. and Perlman, Susan B.},
Title = {Neural architecture supporting active emotion processing in children: A
   multivariate approach},
Journal = {NEUROIMAGE},
Year = {2019},
Volume = {188},
Pages = {171-180},
Month = {MAR},
Abstract = {Background: Adaptive emotion processing is critical for nearly all
   aspects of social and emotional functioning. There are distinct
   developmental trajectories associated with improved emotion processing,
   with a protracted developmental course for negative or complex emotions.
   The specific changes in neural circuitry that underlie this development,
   however are still scarcely understood. We employed a multivariate
   approach in order to elucidate distinctions in complex, naturalistic
   emotion processing between childhood and adulthood.
   Method: Twenty-one adults (M +/- SD age = 26.57 +/- 5.08 years) and
   thirty children (age = 7.75 +/- 1.80 years) completed a free-viewing
   movie task during BOLD fMRI scanning. This task was designed to assess
   naturalistic processing of movie clips portraying positive, negative,
   and neutral emotions. Multivariate support vector machines (SVM) were
   trained to classify age groups based on neural activation during the
   task.
   Results: SVMs were able to successfully classify condition (positive,
   negative, and neutral) across all participants with high accuracy
   (61.44\%). SVMs could successfully distinguish adults and children
   within each condition (ps < 0.05). Regions that informed the age group
   SVMs were associated with sensory and socio-emotional processing
   (inferior parietal lobule), emotion regulation (inferior frontal gyrus),
   and sensory regions of the temporal and occipital lobes.
   Conclusions: These results point to distributed differences in
   activation between childhood and adulthood unique to each emotional
   condition. In the negative condition specifically, there is evidence for
   a shift in engagement from regions of sensory and socio-emotional
   integration to emotion regulation regions between children and adults.
   These results provide insight into circuitry contributing to maturation
   of emotional processing across development.},
Publisher = {ACADEMIC PRESS INC ELSEVIER SCIENCE},
Address = {525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA},
Type = {Article},
Language = {English},
Affiliation = {Camacho, MC (Corresponding Author), Univ Pittsburgh, Dept Psychiat, 121 Meyran Ave, Pittsburgh, PA 15213 USA.
   Camacho, M. Catalina; Perlman, Susan B., Univ Pittsburgh, Ctr Neurosci, Pittsburgh, PA 15213 USA.
   Camacho, M. Catalina; Perlman, Susan B., Univ Pittsburgh, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA.
   Karim, Helmet T.; Perlman, Susan B., Univ Pittsburgh, Dept Psychiat, 121 Meyran Ave, Pittsburgh, PA 15213 USA.},
DOI = {10.1016/j.neuroimage.2018.12.013},
ISSN = {1053-8119},
EISSN = {1095-9572},
Keywords = {Emotion processing; Development; Machine learning; Naturalistic viewing;
   Children},
Keywords-Plus = {FACIAL EXPRESSIONS; AMYGDALA ACTIVATION; PATTERN-ANALYSIS;
   VISUAL-CORTEX; BRAIN; RISK; FMRI; RECOGNITION; REACTIVITY; CHILDHOOD},
Research-Areas = {Neurosciences \& Neurology; Radiology, Nuclear Medicine \& Medical
   Imaging},
Web-of-Science-Categories  = {Neurosciences; Neuroimaging; Radiology, Nuclear Medicine \& Medical
   Imaging},
Author-Email = {mccamacho@pitt.edu},
ResearcherID-Numbers = {Camacho, M. Catalina/AAB-5985-2021
   Karim, Helmet/I-6516-2019},
ORCID-Numbers = {Camacho, M. Catalina/0000-0003-0457-5410
   Karim, Helmet/0000-0002-9286-0694},
Funding-Acknowledgement = {National Institute of Health {[}K01 MH094467, R01 MH107540, T32
   MH019986]; National Science Foundation {[}174745]; NATIONAL INSTITUTE OF
   MENTAL HEALTH {[}R01MH107540] Funding Source: NIH RePORTER},
Funding-Text = {We thank the many families who contributed their time to this study. We
   thank Lisa Bemis, Brianna Jones, Caroline Macgillivray, Meghan Murphy,
   and Amiee Willet for their help with data collection, stimuli
   development, and subject recruitment. This work was supported by the
   National Institute of Health {[}K01 MH094467 (Perlman), R01 MH107540
   (Perlman), T32 MH019986 (Karim)] and the National Science Foundation
   Graduate Research Fellowship {[}174745 (Camacho)].},
Number-of-Cited-References = {64},
Times-Cited = {16},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {13},
Journal-ISO = {Neuroimage},
Doc-Delivery-Number = {HN3DT},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000460064700015},
OA = {Green Accepted},
DA = {2022-09-28},
}

@InProceedings{WOS:000557295306110,
  author                     = {Jiang, Ming and Francis, Sunday M. and Srishyla, Diksha and Conelea, Christine and Zhao, Qi and Jacob, Suma},
  booktitle                  = {2019 41ST ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY (EMBC)},
  title                      = {Classifying Individuals with ASD Through Facial Emotion Recognition and Eye-Tracking},
  year                       = {2019},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Berlin, GERMANY, JUL 23-27, 2019},
  pages                      = {6063-6068},
  publisher                  = {IEEE},
  series                     = {IEEE Engineering in Medicine and Biology Society Conference Proceedings},
  status                     = {Aceito},
  abstract                   = {Individuals with Autism Spectrum Disorder (ASD) have been shown to have
   atypical scanning patterns during face and emotion perception. While
   previous studies characterized ASD using eye-tracking data, this study
   examined whether the use of eye movements combined with task performance
   in facial emotion recognition could be helpful to identify individuals
   with ASD. We tested 23 subjects with ASD and 35 controls using a Dynamic
   Affect Recognition Evaluation (DARE) task that requires an individual to
   recognize one of six emotions (i.e., anger, disgust, fear, happiness,
   sadness, and surprise) while observing a slowly transitioning face
   video. We observed differences in response time and eye movements, but
   not in the recognition accuracy. Based on these observations, we
   proposed a machine learning method to distinguish between individuals
   with ASD and typically developing (TD) controls. The proposed method
   classifies eye fixations based on a comprehensive set of features that
   integrate task performance, gaze information, and face features
   extracted using a deep neural network. It achieved an 86\%
   classification accuracy that is comparable with the standardized
   diagnostic scales, with advantages of efficiency and objectiveness.
   Feature visualization and interpretations were further carried out to
   reveal distinguishing features between the two subject groups and to
   understand the social and attentional deficits in ASD.},
  affiliation                = {Jiang, M (Corresponding Author), Univ Minnesota, Dept Comp Sci \& Engn, Minneapolis, MN 55455 USA. Jiang, Ming; Zhao, Qi, Univ Minnesota, Dept Comp Sci \& Engn, Minneapolis, MN 55455 USA. Francis, Sunday M.; Srishyla, Diksha; Conelea, Christine; Jacob, Suma, Univ Minnesota, Dept Psychiat \& Behav Sci, Minneapolis, MN USA.},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BP5OL},
  eissn                      = {1558-4615},
  funding-acknowledgement    = {Minnesota Clinical \& Translational Research Funding; University of Minnesota Foundation Equipment Grant; Clinical and Translational Science Institute; Center for Neurobehavioral Development; NIMH T32 training grant; Leadership Education in Neurodevelopmental and Related Disorders Training Program; University of Minnesota Department of Computer Science and Engineering Start-up Fund; NSF {[}1763761]},
  funding-text               = {This work was supported by Minnesota Clinical \& Translational Research Funding, University of Minnesota Foundation Equipment Grant, Clinical and Translational Science Institute, The Center for Neurobehavioral Development, NIMH T32 training grant, Leadership Education in Neurodevelopmental and Related Disorders Training Program, a University of Minnesota Department of Computer Science and Engineering Start-up Fund to Q.Z. and an NSF grant 1763761. We thank Stephen Porges for sharing the DARE task. Jaclyn Gunderson provided assistance with project organization, for which we are grateful. We appreciate the support of Jed Elison and the Elison lab in helping us set up initial eye-tracking data collection. To the parents and children who participated in the study, we express our sincere gratitude.},
  isbn                       = {978-1-5386-1311-5},
  issn                       = {1557-170X},
  keywords-plus              = {AUTISM SPECTRUM DISORDERS; FACE; GAZE; FIXATION},
  language                   = {English},
  number-of-cited-references = {26},
  orcid-numbers              = {Francis, S/0000-0003-4369-0331 Jiang, Ming/0000-0001-6439-5476 Conelea, Christine/0000-0003-2791-9802 Jacob, Suma/0000-0001-7434-7398},
  priority                   = {prio2},
  research-areas             = {Engineering},
  researcherid-numbers       = {Jiang, Ming/I-1536-2016 Jacob, Suma/J-7941-2013},
  times-cited                = {19},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000557295306110},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {11},
  web-of-science-categories  = {Engineering, Biomedical; Engineering, Electrical \& Electronic},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{ WOS:000816333900001,
Author = {Ramirez-Melendez, Rafael and Matamoros, Elisabet and Hernandez, Davinia
   and Mirabel, Julia and Sanchez, Elisabet and Escude, Nuria},
Title = {Music-Enhanced Emotion Identification of Facial Emotions in Autistic
   Spectrum Disorder Children: A Pilot EEG Study},
Journal = {BRAIN SCIENCES},
Year = {2022},
Volume = {12},
Number = {6},
Month = {JUN},
Abstract = {The Autistic Spectrum Disorder (ASD) is characterized by a difficulty in
   expressing and interpreting others' emotions. In particular, people with
   ASD have difficulties when interpreting emotions encoded in facial
   expressions. In the past, music interventions have been shown to improve
   autistic individuals' emotional and social skills. The present study
   describes a pilot study to explore the usefulness of music as a tool for
   improving autistic children's emotion recognition in facial expressions.
   Twenty-five children (mean age = 8.8 y, SD = 1.24) with high-functioning
   ASD and normal hearing participated in the study consisting of four
   weekly sessions of 15 min each. Twenty-five participants were randomly
   divided into an experimental group (N = 14) and a control group (N =
   11). During each session, participants in the experimental group were
   exposed to images of facial expressions for four emotions (happy, sad,
   angry, and fear). Images were shown in three conditions, with the second
   condition consisting of music of congruent emotion with the shown
   images. Participants in the control group were shown only images in all
   three conditions. For six participants in each group, EEG data were
   acquired during the sessions, and instantaneous emotional responses
   (arousal and valence values) were extracted from the EEG data. Inter-
   and intra-session emotion identification improvement was measured in
   terms of verbal response accuracy, and EEG response differences were
   analyzed. A comparison of the verbal responses of the experimental group
   pre- and post-intervention showed a significant (p = 0.001) average
   improvement in emotion identification accuracy responses of 26\% (SD =
   3.4). Furthermore, emotional responses of the experimental group at the
   end of the study showed a higher correlation with the emotional stimuli
   being presented, compared with their emotional responses at the
   beginning of the study. No similar verbal responses improvement or
   EEG-stimuli correlation was found in the control group. These results
   seem to indicate that music can be used to improve both emotion
   identification in facial expressions and emotion induction through
   facial stimuli in children with high-functioning ASD.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Ramirez-Melendez, R (Corresponding Author), Univ Pompeu Fabra, Mus \& Machine Learning Lab, DTIC, Barcelona 08018, Spain.
   Ramirez-Melendez, Rafael, Univ Pompeu Fabra, Mus \& Machine Learning Lab, DTIC, Barcelona 08018, Spain.
   Matamoros, Elisabet; Hernandez, Davinia, Univ Pompeu Fabra, Dept Informat \& Commun Technol, Barcelona 08018, Spain.
   Mirabel, Julia; Sanchez, Elisabet, Ctr Carrilet, Barcelona 08031, Spain.
   Escude, Nuria, Inst Catala Musicoterapia, Barcelona 08021, Spain.},
DOI = {10.3390/brainsci12060704},
Article-Number = {704},
EISSN = {2076-3425},
Keywords = {autistic spectrum disorder (ASD); emotions; affective facial
   expressions; music; brain activity; EEG},
Keywords-Plus = {AFFECTIVE STYLE; RECOGNITION; FACE; COMMUNICATION; INDIVIDUALS;
   EXPRESSIONS; DEFICITS; BRAIN; VOICE; DISCRIMINATION},
Research-Areas = {Neurosciences \& Neurology},
Web-of-Science-Categories  = {Neurosciences},
Author-Email = {rafael.ramirez@upF.edu
   elisabet.mlle@gmail.com
   davinia.hernandez@upF.edu
   juliamiralbell@gmail.com
   esanchez@carrilet.org
   nuriescude@ub.edu},
ResearcherID-Numbers = {Hernández-Leo, Davinia/C-2929-2011},
ORCID-Numbers = {Hernández-Leo, Davinia/0000-0003-0548-7455},
Funding-Acknowledgement = {European Union {[}688269]; Spanish Ministerio de Ciencia, Innovacion y
   Universidades (MCIU); Agencia Estatal de Investigacion (AEI) {[}Musical
   AI-PID2019-111403GB-I00/AEI/10.13039/501100011033]},
Funding-Text = {This work was partly sponsored by the European Union Horizon 2020
   research and innovation program under grant agreement No. 688269 (TELMI
   project) and by the Spanish Ministerio de Ciencia, Innovacion y
   Universidades (MCIU) and the Agencia Estatal de Investigacion (AEI)
   project Musical AI-PID2019-111403GB-I00/AEI/10.13039/501100011033.},
Number-of-Cited-References = {68},
Times-Cited = {0},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Brain Sci.},
Doc-Delivery-Number = {2K4UW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000816333900001},
OA = {gold, Green Published},
DA = {2022-09-28},
}

@InProceedings{WOS:000462636300077,
  author                     = {Allouch, Merav and Azaria, Amos and Azoulay, Rina and Ben-Izchak, Ester and Zwilling, Moti and Zachor, Ditza A.},
  booktitle                  = {2018 IEEE INTERNATIONAL CONFERENCE ON THE SCIENCE OF ELECTRICAL ENGINEERING IN ISRAEL (ICSEE)},
  title                      = {Automatic Detection of Insulting Sentences in Conversation},
  year                       = {2018},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {IEEE International Conference on the Science of Electrical Engineering in Israel (ICSEE), Eilat, ISRAEL, DEC 12-14, 2018},
  organization               = {IEEE},
  publisher                  = {IEEE},
  status                     = {Rejeitado - Escopo},
  abstract                   = {An overall goal of our work is to use machine-learning based solutions
   to assist children with communication difficulties in their
   communication task. In this paper, we concentrate on the problem of
   recognizing insulting sentences the child says, or insulting sentences
   that are told to him. An automated agent that is able to recognize such
   sentences can alert the child in real time situations, and can suggest
   how to respond to the resulting social situation. We composed a dataset
   of 1241 non-insulting and 1255 insulting sentences. We trained different
   machine learning methods on 90\% randomly chosen sentences from the
   dataset and tested it on the remaining. We used the following machine
   learning methods: Multi-Layer Neural Network, SVM, Naive Bayes, Decision
   Tree, and Tree Bagger for the task. We found that the best predictors of
   the insulting sentences, were the SVM method, with 80\% recall and over
   75\% precision, and the Multi-Layer Neural Network and the Tree Bagger,
   with precision and recall exceeding 75\%, We also found that adding
   additional data to the learning process, such as 9500 labeled sentences
   from twitter, or adding the word ``positive{''} and the word
   ``negative{''} to sentences including positive or negative words,
   respectively, slightly improves the results in most of the cases. Our
   results provide the cornerstones for an automated system that would
   enable on-line assistance and consultation for children with
   communication disabilities, and also for other persons with
   communication problems, in a way that will enable them to function
   better in society through this assistance.},
  affiliation                = {Allouch, M (Corresponding Author), Ariel Univ, Comp Sci Dept, Ariel, Israel. Allouch, Merav; Azaria, Amos, Ariel Univ, Comp Sci Dept, Ariel, Israel. Azoulay, Rina, Jerusalem Coll Technol, Dept Comp Sci, Jerusalem, Israel. Ben-Izchak, Ester, Ariel Univ, Dept Commun Disorders, Ariel, Israel. Zwilling, Moti, Ariel Univ, Dept Econ \& Business Management, Ariel, Israel. Zachor, Ditza A., Tel Aviv Univ, Sackler Fac Med, Assaf Harofeh Med Ctr, Dept Pediat, Tel Aviv, Israel.},
  author-email               = {merav@g.jct.ac.il amos.azaria@gmail.com rrinaa@gmail.com benitze@ariel.ac.il motiz@ariel.ac.il dzachor@bezeqint.net},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BM3TC},
  isbn                       = {978-1-5386-6378-3},
  keywords                   = {Autism Spectrum Disorder; Machine Learning; Text Emotion Recognition},
  language                   = {English},
  number-of-cited-references = {16},
  orcid-numbers              = {Azaria, Amos/0000-0002-5057-1309 Zwilling, Moti/0000-0001-7628-8889 Ben-Itzchak, Esther/0000-0002-3167-5006},
  research-areas             = {Engineering},
  researcherid-numbers       = {Azaria, Amos/Y-6302-2019 Zwilling, Moti/AAD-3965-2020 Ben-Itzchak, Esther/AAF-7833-2019},
  times-cited                = {3},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000462636300077},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {4},
  web-of-science-categories  = {Engineering, Electrical \& Electronic},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{ WOS:000605354700001,
Author = {Li, Russell and Liu, Zhandong},
Title = {Stress detection using deep neural networks},
Journal = {BMC MEDICAL INFORMATICS AND DECISION MAKING},
Year = {2020},
Volume = {20},
Number = {11, SI},
Month = {DEC 30},
Note = {Annual International Conference on Intelligent Biology and Medicine
   (ICIBM) - Scalable Techniques and Algorithms for Computational Genomics,
   Univ Philadelphia, Int Assoc Intelligent Biol \& Med, ELECTR NETWORK,
   AUG 09-10, 2020},
Organization = {Temple Univ},
Abstract = {BackgroundOver 70\% of Americans regularly experience stress. Chronic
   stress results in cancer, cardiovascular disease, depression, and
   diabetes, and thus is deeply detrimental to physiological health and
   psychological wellbeing. Developing robust methods for the rapid and
   accurate detection of human stress is of paramount importance.
   MethodsPrior research has shown that analyzing physiological signals is
   a reliable predictor of stress. Such signals are collected from sensors
   that are attached to the human body. Researchers have attempted to
   detect stress by using traditional machine learning methods to analyze
   physiological signals. Results, ranging between 50 and 90\% accuracy,
   have been mixed. A limitation of traditional machine learning algorithms
   is the requirement for hand-crafted features. Accuracy decreases if
   features are misidentified. To address this deficiency, we developed two
   deep neural networks: a 1-dimensional (1D) convolutional neural network
   and a multilayer perceptron neural network. Deep neural networks do not
   require hand-crafted features but instead extract features from raw data
   through the layers of the neural networks. The deep neural networks
   analyzed physiological data collected from chest-worn and wrist-worn
   sensors to perform two tasks. We tailored each neural network to analyze
   data from either the chest-worn (1D convolutional neural network) or
   wrist-worn (multilayer perceptron neural network) sensors. The first
   task was binary classification for stress detection, in which the
   networks differentiated between stressed and non-stressed states. The
   second task was 3-class classification for emotion classification, in
   which the networks differentiated between baseline, stressed, and amused
   states. The networks were trained and tested on publicly available data
   collected in previous studies.ResultsThe deep convolutional neural
   network achieved 99.80\% and 99.55\% accuracy rates for binary and
   3-class classification, respectively. The deep multilayer perceptron
   neural network achieved 99.65\% and 98.38\% accuracy rates for binary
   and 3-class classification, respectively. The networks' performance
   exhibited a significant improvement over past methods that analyzed
   physiological signals for both binary stress detection and 3-class
   emotion classification.ConclusionsWe demonstrated the potential of deep
   neural networks for developing robust, continuous, and noninvasive
   methods for stress detection and emotion classification, with the end
   goal of improving the quality of life.},
Publisher = {BMC},
Address = {CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
Type = {Article; Proceedings Paper},
Language = {English},
Affiliation = {Liu, ZD (Corresponding Author), Baylor Coll Med, Dept Pediat, Houston, TX 77030 USA.
   Liu, ZD (Corresponding Author), Texas Childrens Hosp, Jan \& Dan Duncan Neurol Res Inst, Houston, TX 77030 USA.
   Li, Russell, St Johns Sch, Houston, TX USA.
   Liu, Zhandong, Baylor Coll Med, Dept Pediat, Houston, TX 77030 USA.
   Liu, Zhandong, Texas Childrens Hosp, Jan \& Dan Duncan Neurol Res Inst, Houston, TX 77030 USA.},
DOI = {10.1186/s12911-020-01299-4},
Article-Number = {285},
EISSN = {1472-6947},
Keywords = {Convolutional neural network; Emotion classification; Multilayer
   perceptron; Stress detection},
Keywords-Plus = {PSYCHOLOGICAL STRESS; SYSTEM},
Research-Areas = {Medical Informatics},
Web-of-Science-Categories  = {Medical Informatics},
Author-Email = {zhandong.liu@bcm.edu},
Funding-Acknowledgement = {Baylor College of Medicine Seed Fund; Chao Family Foundation},
Funding-Text = {Publication costs are funded by the Baylor College of Medicine Seed Fund
   and Chao Family Foundation.},
Number-of-Cited-References = {25},
Times-Cited = {7},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {10},
Journal-ISO = {BMC Med. Inform. Decis. Mak.},
Doc-Delivery-Number = {PO7NF},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000605354700001},
OA = {gold, Green Published},
DA = {2022-09-28},
}

@Article{WOS:000640391700057,
  author                     = {Washington, Peter and Tariq, Qandeel and Leblanc, Emilie and Chrisman, Brianna and Dunlap, Kaitlyn and Kline, Aaron and Kalantarian, Haik and Penev, Yordan and Paskov, Kelley and Voss, Catalin and Stockham, Nathaniel and Varma, Maya and Husic, Arman and Kent, Jack and Haber, Nick and Winograd, Terry and Wall, Dennis P.},
  journal                    = {SCIENTIFIC REPORTS},
  title                      = {Crowdsourced privacy-preserved feature tagging of short home videos for machine learning ASD detection},
  year                       = {2021},
  issn                       = {2045-2322},
  month                      = {APR 7},
  number                     = {1},
  volume                     = {11},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Standard medical diagnosis of mental health conditions requires licensed
   experts who are increasingly outnumbered by those at risk, limiting
   reach. We test the hypothesis that a trustworthy crowd of non-experts
   can efficiently annotate behavioral features needed for accurate machine
   learning detection of the common childhood developmental disorder Autism
   Spectrum Disorder (ASD) for children under 8 years old. We implement a
   novel process for identifying and certifying a trustworthy distributed
   workforce for video feature extraction, selecting a workforce of 102
   workers from a pool of 1,107. Two previously validated ASD logistic
   regression classifiers, evaluated against parent-reported diagnoses,
   were used to assess the accuracy of the trusted crowd's ratings of
   unstructured home videos. A representative balanced sample (N=50 videos)
   of videos were evaluated with and without face box and pitch shift
   privacy alterations, with AUROC and AUPRC scores>0.98. With both
   privacy-preserving modifications, sensitivity is preserved (96.0\%)
   while maintaining specificity (80.0\%) and accuracy (88.0\%) at levels
   comparable to prior classification methods without alterations. We find
   that machine learning classification from features extracted by a
   certified nonexpert crowd achieves high performance for ASD detection
   from natural home videos of the child at risk and maintains high
   sensitivity when privacy-preserving mechanisms are applied. These
   results suggest that privacy-safeguarded crowdsourced analysis of short
   home videos can help enable rapid and mobile machine-learning detection
   of developmental delays in children.},
  address                    = {HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY},
  affiliation                = {Wall, DP (Corresponding Author), Stanford Univ, Dept Pediat Syst Med, Stanford, CA 94305 USA. Wall, DP (Corresponding Author), Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA. Wall, DP (Corresponding Author), Stanford Univ, Dept Psychiat \& Behav Sci Courtesy, Stanford, CA 94305 USA. Washington, Peter; Chrisman, Brianna, Stanford Univ, Dept Bioengn, Stanford, CA 94305 USA. Tariq, Qandeel, Amazon, Seattle, WA USA. Leblanc, Emilie; Dunlap, Kaitlyn; Kline, Aaron; Kalantarian, Haik; Penev, Yordan; Husic, Arman; Kent, Jack; Wall, Dennis P., Stanford Univ, Dept Pediat Syst Med, Stanford, CA 94305 USA. Paskov, Kelley; Wall, Dennis P., Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA. Voss, Catalin; Varma, Maya; Winograd, Terry, Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA. Stockham, Nathaniel, Stanford Univ, Dept Neurosci, Stanford, CA 94305 USA. Haber, Nick, Stanford Univ, Grad Sch Educ, Stanford, CA 94305 USA. Wall, Dennis P., Stanford Univ, Dept Psychiat \& Behav Sci Courtesy, Stanford, CA 94305 USA.},
  article-number             = {7620},
  author-email               = {dpwall@stanford.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {RN5LD},
  doi                        = {10.1038/s41598-021-87059-4},
  funding-acknowledgement    = {National Institutes of Health {[}1R01LM013083, 1R21HD091500-01, 1R01EB025025-01]; National Science Foundation {[}2014232]; Hartwell Foundation; David and Lucile Packard Foundation; Beckman Center for Molecular and Genetic Medicine; Coulter Endowment Translational Research Grant; Berry Fellowship; Spectrum Pilot Program; Stanford's Precision Health and Integrated Diagnostics Center (PHIND); Wu Tsai Neurosciences Institute Neuroscience: Translate Program; Stanford's Institute of Human Centered Artificial Intelligence; Thrasher Research Fund; Stanford NLM Clinical Data Science program {[}T-15LM007033-35]},
  funding-text               = {The study was in part supported by awards to D.P.W. by the National Institutes of Health (1R01LM013083, 1R21HD091500-01 and 1R01EB025025-01) and by the the National Science Foundation (Award 2014232). Additionally, we acknowledge the support of grants to D.P.W. from The Hartwell Foundation, the David and Lucile Packard Foundation Special Projects Grant, Beckman Center for Molecular and Genetic Medicine, Coulter Endowment Translational Research Grant, Berry Fellowship, Spectrum Pilot Program, Stanford's Precision Health and Integrated Diagnostics Center (PHIND), Wu Tsai Neurosciences Institute Neuroscience: Translate Program, and Stanford's Institute of Human Centered Artificial Intelligence as well as philanthropic support from Mr. Peter Sullivan. HK would like to acknowledge support from the Thrasher Research Fund and Stanford NLM Clinical Data Science program (T-15LM007033-35).},
  journal-iso                = {Sci Rep},
  keywords-plus              = {AUTISM SPECTRUM DISORDER; MOBILE-HEALTH; CHILDREN; DIAGNOSIS; BRAIN; SECURITY; BEHAVIOR; EMOTION; MODELS; TIME},
  language                   = {English},
  number-of-cited-references = {59},
  oa                         = {gold, Green Published},
  orcid-numbers              = {washington, peter/0000-0003-3276-4411 Leblanc, Emilie/0000-0002-3492-3554 Chrisman, Brianna/0000-0002-7157-607X},
  publisher                  = {NATURE PORTFOLIO},
  research-areas             = {Science \& Technology - Other Topics},
  times-cited                = {2},
  type                       = {Article},
  unique-id                  = {WOS:000640391700057},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {4},
  web-of-science-categories  = {Multidisciplinary Sciences},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@inproceedings{ WOS:000760910501152,
Author = {Duville, Mathilde M. and Alonso-Valerdi, Luz M. and Ibarra-Zarate, David
   I.},
Book-Group-Author = {IEEE},
Title = {The Mexican Emotional Speech Database (MESD): elaboration and assessment
   based on machine learning},
Booktitle = {2021 43RD ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN
   MEDICINE \& BIOLOGY SOCIETY (EMBC)},
Series = {IEEE Engineering in Medicine and Biology Society Conference Proceedings},
Year = {2021},
Pages = {1644-1647},
Note = {43rd Annual International Conference of the
   IEEE-Engineering-in-Medicine-and-Biology-Society (IEEE EMBC), ELECTR
   NETWORK, NOV 01-05, 2021},
Organization = {IEEE Engn Med \& Biol Soc; IEEE; Elsevier; Inst Engn \& Technol},
Abstract = {The Mexican Emotional Speech Database is presented along with the
   evaluation of its reliability based on machine learning analysis. The
   database contains 864 voice recordings with six different prosodies:
   anger, disgust, fear, happiness, neutral, and sadness. Furthermore,
   three voice categories are included: female adult, male adult, and
   child. The following emotion recognition was reached for each category:
   89.4\% 93.9\% and 83.3\% accuracy on female, male and child voices,
   respectively.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Duville, MM (Corresponding Author), Tecnol Monterrey, Escuela Ingn \& Ciencias, Ave Eugenio Garza Sada 2501, Monterrey 64849, NL, Mexico.
   Duville, Mathilde M.; Alonso-Valerdi, Luz M.; Ibarra-Zarate, David I., Tecnol Monterrey, Escuela Ingn \& Ciencias, Ave Eugenio Garza Sada 2501, Monterrey 64849, NL, Mexico.
   R Fdn Stat Comp, Vienna, Austria.},
DOI = {10.1109/EMBC46164.2021.9629934},
ISSN = {1557-170X},
EISSN = {1558-4615},
ISBN = {978-1-7281-1179-7},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Biomedical; Engineering, Electrical \& Electronic},
Author-Email = {A00829725@itesm.mx
   lm.aloval@tec.mx
   david.ibarra@tec.mx},
Funding-Acknowledgement = {Mexican National Council of Science and Technology {[}1061809]},
Funding-Text = {Research supported by the Mexican National Council of Science and
   Technology (grant reference number: 1061809).},
Number-of-Cited-References = {16},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BS7HB},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000760910501152},
DA = {2022-09-28},
}

@Article{WOS:000466223600017,
  author                     = {Zhang, Zixing and Han, Jing and Coutinho, Eduardo and Schuller, Bjorn},
  journal                    = {IEEE TRANSACTIONS ON MULTIMEDIA},
  title                      = {Dynamic Difficulty Awareness Training for Continuous Emotion Prediction},
  year                       = {2019},
  issn                       = {1520-9210},
  month                      = {MAY},
  number                     = {5},
  pages                      = {1289-1301},
  volume                     = {21},
  status                     = {Aceito},
  abstract                   = {Time-continuous emotion prediction has become an increasingly compelling
   task in machine learning. Considerable efforts have been made to advance
   the performance of these systems. Nonetheless, the main focus has been
   the development of more sophisticated models and the incorporation of
   different expressive modalities (e.g., speech, face, and physiology). In
   this paper, motivated by the benefit of difficulty awareness in a human
   learning procedure, we propose a novel machine learning framework,
   namely, dynamic difficulty awareness training (DDAT), which sheds fresh
   light on the research-directly exploiting the difficulties in learning
   to boost the machine learning process. The DDAT framework consists of
   two stages: information retrieval and information exploitation. In the
   first stage, we make use of the reconstruction error of input features
   or the annotation uncertainty to estimate the difficulty of learning
   specific information. The obtained difficulty level is then used in
   tandem with original features to update the model input in a second
   learning stage with the expectation that the model can learn to focus on
   high difficulty regions of the learning process. We perform extensive
   experiments on a benchmark database REmote COLlaborative and affective
   to evaluate the effectiveness of the proposed framework. The
   experimental results show that our approach outperforms related
   baselines as well as other well-established time-continuous emotion
   prediction systems, which suggests that dynamically integrating the
   difficulty information for neural networks can help enhance the learning
   process.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Han, J (Corresponding Author), Univ Augsburg, ZD B Chair Embedded Intelligence Hlth Care \& Well, D-86159 Augsburg, Germany. Zhang, Zixing; Schuller, Bjorn, Imperial Coll London, Grp Language Audio \& Mus, London SW7 2AZ, England. Han, Jing; Coutinho, Eduardo; Schuller, Bjorn, Univ Augsburg, ZD B Chair Embedded Intelligence Hlth Care \& Well, D-86159 Augsburg, Germany. Coutinho, Eduardo, Univ Liverpool, Dept Mus, Liverpool L69 3BX1, Merseyside, England.},
  author-email               = {zixing.zhang@imperial.ac.uk jing.han@informatik.uni-augsburg.de e.coutinho@liverpool.ac.uk bjoern.schuller@imperial.ac.uk},
  da                         = {2022-09-28},
  doc-delivery-number        = {HV8HY},
  doi                        = {10.1109/TMM.2018.2871949},
  eissn                      = {1941-0077},
  funding-acknowledgement    = {UK's Economic and Social Research Council {[}HJ-253479]; European Union {[}645094, 645378]; TransAtlantic Platform ``Digging into Data{''} collaboration Grant (ACLEW: Analysing Child Language Experiences Around TheWorld); ESRC {[}ES/R00398X/1] Funding Source: UKRI},
  funding-text               = {This work was supported in part by the TransAtlantic Platform ``Digging into Data{''} collaboration Grant (ACLEW: Analysing Child Language Experiences Around TheWorld), in part by the UK's Economic and Social Research Council through the research Grant HJ-253479 (ACLEW), and in part by the European Union's Horizon 2020 Programme through Research and Innovation Action 645094 (SEWA) and 645378 (ARIA-VALUSPA). The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Chuan Wu.},
  journal-iso                = {IEEE Trans. Multimedia},
  keywords                   = {Emotion prediction; difficulty awareness learning; dynamic learning},
  keywords-plus              = {RECOGNITION; ATTENTION},
  language                   = {English},
  number-of-cited-references = {68},
  oa                         = {Green Submitted},
  orcid-numbers              = {Coutinho, Eduardo/0000-0001-5234-1497 Han, Jing/0000-0001-5776-6849 Schuller, Bjorn/0000-0002-6478-8699},
  priority                   = {prio3},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Computer Science; Telecommunications},
  researcherid-numbers       = {Coutinho, Eduardo/K-1391-2019 Han, Jing/AAB-3944-2020},
  times-cited                = {15},
  type                       = {Article},
  unique-id                  = {WOS:000466223600017},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {9},
  web-of-science-categories  = {Computer Science, Information Systems; Computer Science, Software Engineering; Telecommunications},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@InProceedings{WOS:000556121100052,
  author                     = {Bryant, De'Aira and Howard, Ayanna},
  booktitle                  = {AIES `19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY},
  title                      = {A Comparative Analysis of Emotion-Detecting Al Systems with Respect to Algorithm Performance and Dataset Diversity},
  year                       = {2019},
  address                    = {1515 BROADWAY, NEW YORK, NY 10036-9998 USA},
  note                       = {2nd AAAI/ACM Conference on AI, Ethics, and Society (AIES), Honolulu, HI, JAN 27-28, 2019},
  organization               = {AAAI; Assoc Comp Machinery; ACM SIGAI; Berkeley Existential Risk Initiat; DeepMind Eth \& Soc; Google; Natl Sci Fdn; IBM Res; Facebook; Amazon; PricewaterhouseCoopers; Future Life Inst; Partnership AI},
  pages                      = {377-382},
  publisher                  = {ASSOC COMPUTING MACHINERY},
  status                     = {Aceito},
  abstract                   = {In recent news, organizations have been considering the use of facial
   and emotion recognition for applications involving youth such as
   tackling surveillance and security in schools. However, the majority of
   efforts on facial emotion recognition research have focused on adults.
   Children, particularly in their early years, have been shown to express
   emotions quite differently than adults. Thus, before such algorithms are
   deployed in environments that impact the wellbeing and circumstance of
   youth, a careful examination should be made on their accuracy with
   respect to appropriateness for this target demographic. In this work, we
   utilize several datasets that contain facial expressions of children
   linked to their emotional state to evaluate eight different commercial
   emotion classification systems. We compare the ground truth labels
   provided by the respective datasets to the labels given with the highest
   confidence by the classification systems and assess the results in terms
   of matching score (TPR), positive predictive value, and failure to
   compute rate. Overall results show that the emotion recognition systems
   displayed subpar performance on the datasets of children `s expressions
   compared to prior work with adult datasets and initial human ratings. We
   then identify limitations associated with automated recognition of
   emotions in children and provide suggestions on directions with
   enhancing recognition accuracy through data diversification, dataset
   accountability, and algorithmic regulation.},
  affiliation                = {Bryant, D (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Bryant, De'Aira; Howard, Ayanna, Georgia Inst Technol, Atlanta, GA 30332 USA.},
  author-email               = {dbryant@gatech.edu ayanna.howard@gatech.edu},
  book-group-author          = {Assoc Comp Machinery},
  da                         = {2022-09-28},
  doc-delivery-number        = {BP5KP},
  doi                        = {10.1145/3306618.3314284},
  funding-acknowledgement    = {Linda J. and Mark C. Smith Endowed Chair; NSF-GRFP {[}DGE-1650044]; NSF {[}1849101]},
  funding-text               = {This research is based upon work partially supported by funding from the Linda J. and Mark C. Smith Endowed Chair, the NSF-GRFP under Grant No. DGE-1650044 and NSF Award No. 1849101.},
  isbn                       = {978-1-4503-6324-2},
  keywords-plus              = {RECOGNITION},
  language                   = {English},
  number-of-cited-references = {30},
  oa                         = {Bronze},
  priority                   = {prio3},
  research-areas             = {Computer Science},
  times-cited                = {0},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000556121100052},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {0},
  web-of-science-categories  = {Computer Science, Artificial Intelligence},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@InProceedings{WOS:000458534800090,
  author                     = {Althobaiti, Turke and Katsigiannis, Stamos and West, Daune and Bronte-Stewart, Malcolm and Ramzan, Naeem},
  booktitle                  = {2018 21ST SAUDI COMPUTER SOCIETY NATIONAL COMPUTER CONFERENCE (NCC)},
  title                      = {Affect Detection for Human-Horse Interaction},
  year                       = {2018},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {21st Saudi-Computer-Society National Computer Conference (NCC), Riyadh, SAUDI ARABIA, APR 25-26, 2018},
  organization               = {Saudi Comp Soc; IEEE Saudi Sect; IEEE},
  publisher                  = {IEEE},
  status                     = {Rejeitado - Escopo},
  abstract                   = {In this work, we aim to study the potential use of affect recognition
   techniques for examining the interaction between humans and horses using
   qualitative and quantitative methods. To this end, we propose a
   multi-modal portable system for physiological signal acquisition such as
   the electrocardiogram (ECG), electromyogram (EMG), and
   electroencephalogram (EEG). The proposed system is used to acquire
   signals while users are interacting with horses. The captured signals
   will then be used in order to quantitatively evaluate human and equine
   interaction by mapping the signals to the emotional state of the
   subjects using machine learning techniques. In this preliminary study,
   ECG based features were utilised in order to create a supervised
   classification model that can identify emotions elicited during
   human-horse interaction. Experimental results provide evidence about the
   efficiency of the proposed approach in distinguishing between negative
   and positive emotions, reaching a classification accuracy of 74.21\%.},
  affiliation                = {Althobaiti, T (Corresponding Author), Univ West Scotland, Sch Engn \& Comp, Paisley, Renfrew, Scotland. Althobaiti, Turke; Katsigiannis, Stamos; West, Daune; Bronte-Stewart, Malcolm; Ramzan, Naeem, Univ West Scotland, Sch Engn \& Comp, Paisley, Renfrew, Scotland.},
  author-email               = {Turke.Althobaiti@uws.ac.uk Stamos.Katsigiannis@uws.ac.uk Daune.West@uws.ac.uk Malcolm.Bronte-Stewart@uws.ac.uk Naeem.Ramzan@uws.ac.uk},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BL9ZY},
  isbn                       = {978-1-5386-4110-1},
  keywords                   = {Emotion recognition; physiological signals; human/horse interaction; EEG; ECG; EMG},
  keywords-plus              = {EQUINE-ASSISTED THERAPY; DATABASE; CHILDREN; EEG},
  language                   = {English},
  number-of-cited-references = {36},
  orcid-numbers              = {Ramzan, Naeem/0000-0002-5088-1462 West, Daune/0000-0002-7245-8825},
  research-areas             = {Computer Science},
  times-cited                = {0},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000458534800090},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {5},
  web-of-science-categories  = {Computer Science, Theory \& Methods},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000473119900013,
  author                     = {Capriola-Hall, Nicole N. and Wieckowski, Andrea Trubanova and Swain, Deanna and Tech, Virginia and Aly, Sherin and Youssef, Amira and Abbott, A. Lynn and White, Susan W.},
  journal                    = {BEHAVIOR THERAPY},
  title                      = {Group Differences in Facial Emotion Expression in Autism: Evidence for the Utility of Machine Classification},
  year                       = {2019},
  issn                       = {0005-7894},
  month                      = {JUL},
  number                     = {4},
  pages                      = {828-838},
  volume                     = {50},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Effective social communication relies, in part, on accurate nonverbal
   expression of emotion. To evaluate the nature of facial emotion
   expression (FEE) deficits in children with autism spectrum disorder
   (ASD), we compared 20 youths with ASD to a sample of typically
   developing (TD) youth (n = 20) using a machine-based classifier of FEE.
   Results indicate group differences in FEE for overall accuracy across
   emotions. In particular, a significant group difference in accuracy of
   FEE was observed when participants were prompted by a video of a human
   expressing an emotion, F(2, 36) = 4.99, p = .032, eta(2) = .12.
   Specifically, youth with ASD made significantly more errors in FEE
   relative to TD youth. Findings support continued refinement of
   machine-based approaches to assess and potentially remediate FEE
   impairment in youth with ASD.},
  address                    = {525 B STREET, STE 1900, SAN DIEGO, CA 92101-4495 USA},
  affiliation                = {Capriola-Hall, NN (Corresponding Author), Univ Alabama, Dept Psychol, Tuscaloosa, AL 35487 USA. Capriola-Hall, Nicole N.; White, Susan W., Univ Alabama, Tuscaloosa, AL USA. Wieckowski, Andrea Trubanova; Swain, Deanna; Tech, Virginia; Aly, Sherin, Alexandria Univ, Alexandria, Egypt. Youssef, Amira; Abbott, A. Lynn, Virginia Tech, Blacksburg, VA USA.},
  author-email               = {nncapriola@crimson.ua.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {IF5KO},
  doi                        = {10.1016/j.beth.2018.12.004},
  eissn                      = {1878-1888},
  funding-acknowledgement    = {NICHD {[}R03HD081070]; EUNICE KENNEDY SHRIVER NATIONAL INSTITUTE OF CHILD HEALTH \& HUMAN DEVELOPMENT {[}R03HD081070] Funding Source: NIH RePORTER},
  funding-text               = {This study was funded by NICHD (R03HD081070; Co-PIs Abbott \& White).},
  journal-iso                = {Behav. Therapy},
  keywords                   = {autism spectrum disorder; facial emotion expression; machine learning},
  keywords-plus              = {CHILDREN; RECOGNITION; ADOLESCENTS; ADULTS; INDIVIDUALS; JUDGMENTS; FIXATION; INFANTS; PROSODY},
  language                   = {English},
  number-of-cited-references = {48},
  publisher                  = {ELSEVIER INC},
  research-areas             = {Psychology; Psychiatry},
  times-cited                = {5},
  type                       = {Article},
  unique-id                  = {WOS:000473119900013},
  usage-count-last-180-days  = {5},
  usage-count-since-2013     = {25},
  web-of-science-categories  = {Psychology, Clinical; Psychiatry},
  web-of-science-index       = {Social Science Citation Index (SSCI)},
}

@Article{WOS:000675020500001,
  author                     = {Banire, Bilikis and Al Thani, Dena and Qaraqe, Marwa and Mansoor, Bilal},
  journal                    = {JOURNAL OF HEALTHCARE INFORMATICS RESEARCH},
  title                      = {Face-Based Attention Recognition Model for Children with Autism Spectrum Disorder},
  year                       = {2021},
  issn                       = {2509-4971},
  month                      = {DEC},
  number                     = {4},
  pages                      = {420-445},
  volume                     = {5},
  status                     = {Aceito},
  abstract                   = {Attention recognition plays a vital role in providing learning support
   for children with autism spectrum disorders (ASD). The unobtrusiveness
   of face-tracking techniques makes it possible to build automatic systems
   to detect and classify attentional behaviors. However, constructing such
   systems is a challenging task due to the complexity of attentional
   behavior in ASD. This paper proposes a face-based attention recognition
   model using two methods. The first is based on geometric feature
   transformation using a support vector machine (SVM) classifier, and the
   second is based on the transformation of time-domain spatial features to
   2D spatial images using a convolutional neural network (CNN) approach.
   We conducted an experimental study on different attentional tasks for 46
   children (ASD n=20, typically developing children n=26) and explored the
   limits of the face-based attention recognition model for participant and
   task differences. Our results show that the geometric feature
   transformation using an SVM classifier outperforms the CNN approach.
   Also, attention detection is more generalizable within typically
   developing children than within ASD groups and within low-attention
   tasks than within high-attention tasks. This paper highlights the basis
   for future face-based attentional recognition for real-time learning and
   clinical attention interventions.},
  address                    = {CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND},
  affiliation                = {Banire, B; Al Thani, D; Qaraqe, M (Corresponding Author), Hamad Bin Khalifa Univ, Qatar Fdn, Coll Sci \& Engn, Div Informat \& Comp Technol, Doha, Qatar. Banire, Bilikis; Al Thani, Dena; Qaraqe, Marwa, Hamad Bin Khalifa Univ, Qatar Fdn, Coll Sci \& Engn, Div Informat \& Comp Technol, Doha, Qatar. Mansoor, Bilal, Texas A\&M Univ Doha, Mech Engn Program, Doha, Qatar.},
  author-email               = {banire.bilikis.o@gmail.com dalthani@hbku.edu.qa mqaraqe@hbku.edu.qa},
  da                         = {2022-09-28},
  doc-delivery-number        = {WP2NX},
  doi                        = {10.1007/s41666-021-00101-y},
  earlyaccessdate            = {JUL 2021},
  eissn                      = {2509-498X},
  funding-acknowledgement    = {Qatar National Library; Qatar Foundation; Hamad Bin Khalifa University},
  funding-text               = {Open Access funding provided by the Qatar National Library. We also appreciate Qatar Foundation and Hamad Bin Khalifa University for their support.},
  journal-iso                = {J. Healthc. Inform. Res.},
  keywords                   = {Facial landmarks; Geometric features; Attention recognition; ASD; Machine learning},
  keywords-plus              = {FACIAL EXPRESSION RECOGNITION; EMOTION RECOGNITION; STUDENT ENGAGEMENT; FEATURES; GAZE},
  language                   = {English},
  number-of-cited-references = {79},
  oa                         = {Green Published, hybrid},
  orcid-numbers              = {Mansoor, Bilal/0000-0002-4571-6366},
  priority                   = {prio1},
  publisher                  = {SPRINGERNATURE},
  research-areas             = {Computer Science; Health Care Sciences \& Services; Medical Informatics},
  researcherid-numbers       = {Mansoor, Bilal/E-8189-2016},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000675020500001},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {5},
  web-of-science-categories  = {Computer Science, Information Systems; Health Care Sciences \& Services; Medical Informatics},
  web-of-science-index       = {Emerging Sources Citation Index (ESCI)},
}

@InProceedings{WOS:000621592201050,
  author                     = {Jiang, Ming and Francis, Sunday M. and Tseng, Angela and Srishyla, Diksha and DuBois, Megan and Beard, Katie and Conelea, Christine and Zhao, Qi and Jacob, Suma},
  booktitle                  = {42ND ANNUAL INTERNATIONAL CONFERENCES OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY: ENABLING INNOVATIVE TECHNOLOGIES FOR GLOBAL HEALTHCARE EMBC'20},
  title                      = {Predicting Core Characteristics of ASD Through Facial Emotion Recognition and Eye Tracking in Youth},
  year                       = {2020},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {42nd Annual International Conference of the IEEE-Engineering-in-Medicine-and-Biology-Society (EMBC), Montreal, CANADA, JUL 20-24, 2020},
  organization               = {IEEE Engn Med \& Biol Soc},
  pages                      = {871-875},
  publisher                  = {IEEE},
  series                     = {IEEE Engineering in Medicine and Biology Society Conference Proceedings},
  abstract                   = {Autism Spectrum Disorder (ASD) is a heterogeneous neurodevelopmental
   disorder (NDD) with a high rate of comorbidity. The implementation of
   eye-tracking methodologies has informed behavioral and
   neurophysiological patterns of visual processing across ASD and comorbid
   NDDs. In this study, we propose a machine learning method to predict
   measures of two core ASD characteristics: impaired social interactions
   and communication, and restricted, repetitive, and stereotyped behaviors
   and interests. Our method extracts behavioral features from task
   performance and eye-tracking data collected during a facial emotion
   recognition paradigm. We achieved high regression accuracy using a
   Random Forest regressor trained to predict scores on the SRS-2 and RBS-R
   assessments; this approach may serve as a classifier for ASD diagnosis.},
  affiliation                = {Jiang, M (Corresponding Author), Univ Minnesota, Dept Comp Sci \& Engn, Minneapolis, MN 55455 USA. Jiang, Ming; Zhao, Qi, Univ Minnesota, Dept Comp Sci \& Engn, Minneapolis, MN 55455 USA. Francis, Sunday M.; Tseng, Angela; DuBois, Megan; Beard, Katie; Conelea, Christine; Jacob, Suma, Univ Minnesota, Dept Psychiat \& Behav Sci, Minneapolis, MN USA. Srishyla, Diksha, Univ Minnesota, Div Epidemiol \& Community Hlth, Minneapolis, MN USA.},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BQ8TK},
  eissn                      = {1558-4615},
  funding-acknowledgement    = {NIMH {[}T32MH115886]; NSF {[}1908711, 1763761]; Center for Neurobehavioral Development; Clinical and Translational Science Institute; Minnesota Clinical \& Translational Research Funding; University of Minnesota Foundation Equipment Grant; Leadership Education in Neurodevelopmental and Related Disorders (LEND) Training Program {[}T73MC12835]},
  funding-text               = {This work was supported by Minnesota Clinical \& Translational Research Funding, University of Minnesota Foundation Equipment Grant, Leadership Education in Neurodevelopmental and Related Disorders (LEND) Training Program T73MC12835 (SF, SJ), Clinical and Translational Science Institute (SF, SJ), NIMH T32MH115886 training grant (SF), NSF grants 1908711 and 1763761 (QZ), and the Center for Neurobehavioral Development.},
  isbn                       = {978-1-7281-1990-8},
  issn                       = {1557-170X},
  keywords-plus              = {AUTISM SPECTRUM DISORDER; PSYCHIATRIC-DISORDERS; ATTENTION; CHILDREN; COMORBIDITY; FACES},
  language                   = {English},
  number-of-cited-references = {36},
  orcid-numbers              = {Francis, S/0000-0003-4369-0331 Tseng, Angela/0000-0002-4838-9452 Jiang, Ming/0000-0001-6439-5476 Conelea, Christine/0000-0003-2791-9802},
  research-areas             = {Engineering},
  researcherid-numbers       = {Jiang, Ming/I-1536-2016},
  times-cited                = {3},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000621592201050},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {10},
  web-of-science-categories  = {Engineering, Biomedical; Engineering, Electrical \& Electronic},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000818943600004,
  author                     = {Yasin, Sana and Draz, Umar and Ali, Tariq and Shahid, Kashaf and Abid, Amna and Bibi, Rukhsana and Irfan, Muhammad and Huneif, Mohammed A. and Almedhesh, Sultan A. and Alqahtani, Seham M. and Abdulwahab, Alqahtani and Alzahrani, Mohammed Jamaan and Alshehri, Dhafer Batti and Abdullah, Alshehri Ali and Rahman, Saifur},
  journal                    = {CMC-COMPUTERS MATERIALS \& CONTINUA},
  title                      = {Automated Speech Recognition System to Detect Babies' Feelings through Feature Analysis},
  year                       = {2022},
  issn                       = {1546-2218},
  number                     = {2},
  pages                      = {4349-4367},
  volume                     = {73},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Diagnosing a baby???s feelings poses a challenge for both doctors and
   parents because babies cannot explain their feelings through expression
   or speech. Understanding the emotions of babies and their associated
   expressions during different sensations such as hunger, pain, etc., is a
   complicated task. In infancy, all communication and feelings are
   propagated through cry speech, which is a natural phenomenon. Several
   clinical methods can be used to diagnose a baby???s diseases, but
   nonclinical methods of diagnosing a baby???s feelings are lacking. As
   such, in this study, we aimed to identify babies??? feelings and
   emotions through their cry using a nonclinical method. Changes in the
   cry sound can be identified using our method and used to assess the
   baby???s feelings. We considered the frequency of the cries from the
   energy of the sound. The feelings represented by the infant???s cry are
   judged to represent certain sensations expressed by the child using the
   optimal frequency of the recognition of a real-world audio sound. We
   used machine learning and artificial intelligence to distinguish cry
   tones in real time through feature analysis. The experimental group
   consisted of 50\% each male and female babies, and we determined the
   relevancy of the results against different parameters. This application
   produced real-time results after recognizing a child???s cry sounds. The
   novelty of our work is that we, for the first time, successfully derived
   the feelings of young children through the cry-speech of the child,
   showing promise for end-user applications.},
  address                    = {871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA},
  affiliation                = {Draz, U (Corresponding Author), Univ Sahiwal, Dept Comp Sci, Sahiwal 57000, Punjab, Pakistan. Draz, U (Corresponding Author), COMSATS Univ Islamabad, Comp Sci Dept, Lahore Campus, Lahore 57000, Pakistan. Yasin, Sana; Shahid, Kashaf; Abid, Amna; Bibi, Rukhsana, Univ Okara, Fac Comp, Okara 57000, Pakistan. Draz, Umar, Univ Sahiwal, Dept Comp Sci, Sahiwal 57000, Punjab, Pakistan. Draz, Umar, COMSATS Univ Islamabad, Comp Sci Dept, Lahore Campus, Lahore 57000, Pakistan. Ali, Tariq, CUI, Dept Comp Sci, Sahiwal Campus, Sahiwal 57000, Pakistan. Irfan, Muhammad; Rahman, Saifur, Najran Univ Saudi Arabia, Coll Engn, Elect Engn Dept, Najran 61441, Saudi Arabia. Huneif, Mohammed A.; Almedhesh, Sultan A.; Alqahtani, Seham M.; Abdulwahab, Alqahtani; Alzahrani, Mohammed Jamaan; Alshehri, Dhafer Batti, Najran Univ Saudi Arabia, Med Coll, Pediat Dept, Najran 61441, Saudi Arabia. Abdullah, Alshehri Ali, Najran Univ Saudi Arabia, Coll Med, ORL \& HNs \& Facial Plast Surg, Najran 61441, Saudi Arabia.},
  author-email               = {sheikhumar520@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {2O3DP},
  doi                        = {10.32604/cmc.2022.028251},
  eissn                      = {1546-2226},
  funding-acknowledgement    = {Deanship of Scientific Research, Najran University, Kingdom of Saudi Arabia {[}NU/RC/SERC/11/5]},
  funding-text               = {This research was funded by the Deanship of Scientific Research, Najran University, Kingdom of Saudi Arabia, grant number NU/RC/SERC/11/5.},
  journal-iso                = {CMC-Comput. Mat. Contin.},
  keywords                   = {Cry-to-speak; machine learning; artificial intelligence; cry speech detection; babies},
  keywords-plus              = {INFANT CRY; CLASSIFICATION; DYNAMICS; CEPSTRUM},
  language                   = {English},
  number-of-cited-references = {38},
  oa                         = {gold},
  orcid-numbers              = {Rahman, Saifur/0000-0002-7262-183X Alshehri, Dhafer/0000-0002-7382-3337},
  publisher                  = {TECH SCIENCE PRESS},
  research-areas             = {Computer Science; Materials Science},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000818943600004},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Computer Science, Information Systems; Materials Science, Multidisciplinary},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000753776900011,
  author                     = {Saranya, A. and Anandan, R.},
  journal                    = {INTELLIGENT AUTOMATION AND SOFT COMPUTING},
  title                      = {Facial Action Coding and Hybrid Deep Learning Architectures for Autism Detection},
  year                       = {2022},
  issn                       = {1079-8587},
  number                     = {2},
  pages                      = {1167-1182},
  volume                     = {33},
  status                     = {Aceito},
  abstract                   = {Hereditary Autism Spectrum Disorder (ASD) is a neuron disorder that
   affects a person's ability for communication, interaction, and also
   behaviors. Diagnostics of autism are available throughout all stages of
   life, from infancy through adolescence and adulthood. Facial Emotions
   detection is considered to be the most parameter for the detection of
   Autismdisorders among the different categories of people. Propelled with
   a machine and deep learning algorithms, detection of autism disorder
   using facial emotions has reached a new dimension and has even been
   considered as the precautionary warning system for caregivers. Since
   Facial emotions are limited to only seven expressions, detection of ASD
   using facial emotions needs improvisation in terms of accurate detection
   and diagnosis. In this paper, we empirically relate the facial emotions
   to the ASD using the Facial Action Coding Systems (FACS) in which the
   different features are extracted by the FACS systems. For feature
   extraction, DEEPFACENET uses the FACS integrated Convolutional Neural
   Network (FACS-CNN) and hybrid Deep Learning of LSTM (Long Short-Term
   Memory) for the classification and detection of autism spectrum
   disorders (ASD). The experimentation is carried out using AFFECTNET
   databases and validated using Kaggle Autistic facial datasets
   (KAFD-2020). The Multi-Layer Perceptron (48.67\%), Convolutional neural
   networks (67.75\%), and Long ShortTerm Memory (71.56), the suggested
   model showed a considerable increase in recognition rate (92\%), from
   this proposed model prove its superiority in detecting autistic facial
   emotions among children effectively.},
  address                    = {871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA},
  affiliation                = {Saranya, A (Corresponding Author), SRM Inst Sci \& Technol, Dept Computat Intelligence, Chennai 603203, Tamil Nadu, India. Saranya, A., SRM Inst Sci \& Technol, Dept Computat Intelligence, Chennai 603203, Tamil Nadu, India. Anandan, R., Vels Inst Sci Technol \& Adv Studies VISTAS, Dept CSE, Chennai 603203, Tamil Nadu, India.},
  author-email               = {saranyajournalssaro1@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {YX0CP},
  doi                        = {10.32604/iasc.2022.023445},
  eissn                      = {2326-005X},
  journal-iso                = {Intell. Autom. Soft Comput.},
  keywords                   = {Autism spectrum disorder; facial emotions; facial action coding; systems; convolutional neural networks; LSTM},
  keywords-plus              = {EMOTION RECOGNITION; DISORDER},
  language                   = {English},
  number-of-cited-references = {23},
  oa                         = {hybrid},
  priority                   = {prio2},
  publisher                  = {TECH SCIENCE PRESS},
  research-areas             = {Automation \& Control Systems; Computer Science},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000753776900011},
  usage-count-last-180-days  = {10},
  usage-count-since-2013     = {10},
  web-of-science-categories  = {Automation \& Control Systems; Computer Science, Artificial Intelligence},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@article{ WOS:000700975300001,
Author = {Washington, Peter and Kalantarian, Haik and Kent, Jack and Husic, Arman
   and Kline, Aaron and Leblanc, Emilie and Hou, Cathy and Mutlu, Cezmi and
   Dunlap, Kaitlyn and Penev, Yordan and Stockham, Nate and Chrisman,
   Brianna and Paskov, Kelley and Jung, Jae-Yoon and Voss, Catalin and
   Haber, Nick and Wall, Dennis P.},
Title = {Training Affective Computer Vision Models by Crowdsourcing Soft-Target
   Labels},
Journal = {COGNITIVE COMPUTATION},
Year = {2021},
Volume = {13},
Number = {5},
Pages = {1363-1373},
Month = {SEP},
Abstract = {Emotion detection classifiers traditionally predict discrete emotions.
   However, emotion expressions are often subjective, thus requiring a
   method to handle compound and ambiguous labels. We explore the
   feasibility of using crowdsourcing to acquire reliable soft-target
   labels and evaluate an emotion detection classifier trained with these
   labels. We hypothesize that training with labels that are representative
   of the diversity of human interpretation of an image will result in
   predictions that are similarly representative on a disjoint test set. We
   also hypothesize that crowdsourcing can generate distributions which
   mirror those generated in a lab setting. We center our study on the
   Child Affective Facial Expression (CAFE) dataset, a gold standard
   collection of images depicting pediatric facial expressions along with
   100 human labels per image. To test the feasibility of crowdsourcing to
   generate these labels, we used Microworkers to acquire labels for 207
   CAFE images. We evaluate both unfiltered workers and workers selected
   through a short crowd filtration process. We then train two versions of
   a ResNet-152 neural network on soft-target CAFE labels using the
   original 100 annotations provided with the dataset: (1) a classifier
   trained with traditional one-hot encoded labels and (2) a classifier
   trained with vector labels representing the distribution of CAFE
   annotator responses. We compare the resulting softmax output
   distributions of the two classifiers with a 2-sample independent t-test
   of L1 distances between the classifier's output probability distribution
   and the distribution of human labels. While agreement with CAFE is weak
   for unfiltered crowd workers, the filtered crowd agree with the CAFE
   labels 100\% of the time for happy, neutral, sad, and ``fear +
   surprise{''} and 88.8\% for ``anger + disgust.{''} While the F1-score
   for a one-hot encoded classifier is much higher (94.33\% vs. 78.68\%)
   with respect to the ground truth CAFE labels, the output probability
   vector of the crowd-trained classifier more closely resembles the
   distribution of human labels (t = 3.2827, p = 0.0014). For many
   applications of affective computing, reporting an emotion probability
   distribution that accounts for the subjectivity of human interpretation
   can be more useful than an absolute label. Crowdsourcing, including a
   sufficient filtering mechanism for selecting reliable crowd workers, is
   a feasible solution for acquiring soft-target labels.},
Publisher = {SPRINGER},
Address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
Type = {Article},
Language = {English},
Affiliation = {Washington, P (Corresponding Author), Stanford Univ, Dept Bioengn, Stanford, CA 94305 USA.
   Wall, DP (Corresponding Author), Stanford Univ, Dept Pediat Syst Med, Stanford, CA 94305 USA.
   Wall, DP (Corresponding Author), Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA.
   Wall, DP (Corresponding Author), Stanford Univ, Dept Psychiat \& Behav Sci, Stanford, CA 94305 USA.
   Washington, Peter; Chrisman, Brianna, Stanford Univ, Dept Bioengn, Stanford, CA 94305 USA.
   Kalantarian, Haik; Kent, Jack; Husic, Arman; Kline, Aaron; Leblanc, Emilie; Dunlap, Kaitlyn; Penev, Yordan; Jung, Jae-Yoon, Stanford Univ, Dept Pediat Syst Med, Stanford, CA 94305 USA.
   Hou, Cathy; Voss, Catalin, Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   Mutlu, Cezmi, Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   Stockham, Nate, Stanford Univ, Dept Neurosci, Stanford, CA 94305 USA.
   Paskov, Kelley, Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA.
   Haber, Nick, Stanford Univ, Grad Sch Educ, Stanford, CA 94305 USA.
   Wall, Dennis P., Stanford Univ, Dept Psychiat \& Behav Sci, Stanford, CA 94305 USA.},
DOI = {10.1007/s12559-021-09936-4},
EarlyAccessDate = {SEP 2021},
ISSN = {1866-9956},
EISSN = {1866-9964},
Keywords-Plus = {EMOTION RECOGNITION; FACIAL EXPRESSIONS; CLASSIFICATION; DOMINANCE;
   CHILDREN; AUTISM},
Research-Areas = {Computer Science; Neurosciences \& Neurology},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Neurosciences},
Author-Email = {peterwashington@stanford.edu
   haik.kalantarian@gmail.com
   jackkent@stanford.edu
   ahusic@stanford.edu
   akline@stanford.edu
   emilie.leblanc@stanford.edu
   cathyhou@stanford.edu
   cezmi@stanford.edu
   kaiti.dunlap@stanford.edu
   ypenev@stanford.edu
   stockham@stanford.edu
   briannac@stanford.edu
   kpaskov@stanford.edu
   jaeyjung@stanford.edu
   catalin@cs.stanford.edu
   nhaber@stanford.edu
   dpwall@stanford.edu},
ORCID-Numbers = {Stockham, Nathaniel/0000-0002-0752-6801
   Chrisman, Brianna/0000-0002-7157-607X
   washington, peter/0000-0003-3276-4411},
Funding-Acknowledgement = {National Institutes of Health {[}1R01EB025025-01, 1R21HD091500-01,
   1R01LM013083, 1R01LM013364]; National Science Foundation {[}2014232];
   Hartwell Foundation; Bill and Melinda Gates Foundation; Coulter
   Foundation; Weston Havens Foundation; Stanford's Human Centered
   Artificial Intelligence Program; Lucile Packard Foundation},
Funding-Text = {This work was supported in part by funds to DPW from the National
   Institutes of Health (1R01EB025025-01, 1R21HD091500-01, 1R01LM013083,
   1R01LM013364), the National Science Foundation (Award 2014232), The
   Hartwell Foundation, Bill and Melinda Gates Foundation, Coulter
   Foundation, Lucile Packard Foundation, the Weston Havens Foundation, and
   program grants from Stanford's Human Centered Artificial Intelligence
   Program, Stanford's Precision Health and Integrated Diagnostics Center
   (PHIND), Stanford's Beckman Center, Stanford's Bio-X Center, Predictives
   and Diagnostics Accelerator (SPADA) Spectrum, Stanford's Spark Program
   in Translational Research, Stanford mediaX, and Stanford's Wu Tsai
   Neurosciences Institute's Neuroscience: Translate Program. We also
   acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer
   and Peter Sullivan. P.W. would like to acknowledge support from Mr.
   Schroeder and the Stanford Interdisciplinary Graduate Fellowship (SIGF)
   as the Schroeder Family Goldman Sachs Graduate Fellow.},
Number-of-Cited-References = {105},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {3},
Journal-ISO = {Cogn. Comput.},
Doc-Delivery-Number = {WH0NK},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000700975300001},
OA = {Green Submitted},
DA = {2022-09-28},
}

@Article{WOS:000431374500001,
  author                     = {Movahedi, Faezeh and Coyle, James L. and Sejdic, Ervin},
  journal                    = {IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS},
  title                      = {Deep Belief Networks for Electroencephalography: A Review of Recent Contributions and Future Outlooks},
  year                       = {2018},
  issn                       = {2168-2194},
  month                      = {MAY},
  number                     = {3},
  pages                      = {642-652},
  volume                     = {22},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Deep learning, a relatively new branch of machine learning, has been
   investigated for use in a variety of biomedical applications. Deep
   learning algorithms have been used to analyze different physiological
   signals and gain a better understanding of human physiology for
   automated diagnosis of abnormal conditions. In this paper, we provide an
   overview of deep learning approaches with a focus on deep belief
   networks in electroencephalography applications. We investigate the
   state-of-the-art algorithms for deep belief networks and then cover the
   application of these algorithms and their performances in
   electroencephalographic applications. We covered various applications of
   electroencephalography in medicine, including emotion recognition, sleep
   stage classification, and seizure detection, in order to understand how
   deep learning algorithms could be modified to better suit the tasks
   desired. This review is intended to provide researchers with a broad
   overview of the currently existing deep belief network methodology for
   electroencephalography signals, as well as to highlight potential
   challenges for future research.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Sejdic, E (Corresponding Author), Univ Pittsburgh, Swanson Sch Engn, Dept Elect \& Comp Engn, Pittsburgh, PA 15260 USA. Movahedi, Faezeh; Sejdic, Ervin, Univ Pittsburgh, Swanson Sch Engn, Dept Elect \& Comp Engn, Pittsburgh, PA 15260 USA. Coyle, James L., Univ Pittsburgh, Sch Hlth \& Rehabil Sci, Dept Commun Sci \& Disorders, Pittsburgh, PA 15260 USA.},
  author-email               = {fam32@pitt.edu jcoyle@pitt.edu esejdic@ieee.org},
  da                         = {2022-09-28},
  doc-delivery-number        = {GE6XN},
  doi                        = {10.1109/JBHI.2017.2727218},
  funding-acknowledgement    = {Eunice Kennedy Shriver National Institute Of Child Health \& Human Development of the National Institutes of Health {[}R01HD074819]; EUNICE KENNEDY SHRIVER NATIONAL INSTITUTE OF CHILD HEALTH \& HUMAN DEVELOPMENT {[}R01HD074819] Funding Source: NIH RePORTER},
  funding-text               = {This work was supported by the Eunice Kennedy Shriver National Institute Of Child Health \& Human Development of the National Institutes of Health under Award Number R01HD074819.},
  journal-iso                = {IEEE J. Biomed. Health Inform.},
  keywords                   = {Classification; deep learning; electroencephalography; machine learning},
  keywords-plus              = {TIME-SERIES ANALYSIS; EMOTION RECOGNITION; NEURAL-NETWORK; INTERNATIONAL-BUREAU; EPILEPTIC SEIZURES; EEG SIGNAL; CLASSIFICATION; DIAGNOSIS; MACHINE; DISEASE},
  language                   = {English},
  number-of-cited-references = {97},
  oa                         = {Green Accepted},
  orcid-numbers              = {Sejdic, Ervin/0000-0003-4987-8298},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Computer Science; Mathematical \& Computational Biology; Medical Informatics},
  times-cited                = {46},
  type                       = {Review},
  unique-id                  = {WOS:000431374500001},
  usage-count-last-180-days  = {11},
  usage-count-since-2013     = {113},
  web-of-science-categories  = {Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Mathematical \& Computational Biology; Medical Informatics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{ WOS:000523305700001,
Author = {Kalantarian, Haik and Jedoui, Khaled and Dunlap, Kaitlyn and Schwartz,
   Jessey and Washington, Peter and Husic, Arman and Tariq, Qandeel and
   Ning, Michael and Kline, Aaron and Wall, Dennis Paul},
Title = {The Performance of Emotion Classifiers for Children With Parent-Reported
   Autism: Quantitative Feasibility Study},
Journal = {JMIR MENTAL HEALTH},
Year = {2020},
Volume = {7},
Number = {4},
Month = {APR 1},
Abstract = {Background: Autism spectrum disorder (ASD) is a developmental disorder
   characterized by deficits in social communication and interaction, and
   restricted and repetitive behaviors and interests. The incidence of ASD
   has increased in recent years; it is now estimated that approximately 1
   in 40 children in the United States are affected. Due in part to
   increasing prevalence, access to treatment has become constrained. Hope
   lies in mobile solutions that provide therapy through artificial
   intelligence (AI) approaches, including facial and emotion detection AI
   models developed by mainstream cloud providers, available directly to
   consumers. However, these solutions may not be sufficiently trained for
   use in pediatric populations.
   Objective: Emotion classifiers available off-the-shelf to the general
   public through Microsoft, Amazon, Google, and Sighthound are well-suited
   to the pediatric population, and could be used for developing mobile
   therapies targeting aspects of social communication and interaction,
   perhaps accelerating innovation in this space. This study aimed to test
   these classifiers directly with image data from children with
   parent-reported ASD recruited through crowdsourcing.
   Methods: We used a mobile game called Guess What? that challenges a
   child to act out a series of prompts displayed on the screen of the
   smartphone held on the forehead of his or her care provider. The game is
   intended to be a fun and engaging way for the child and parent to
   interact socially, for example, the parent attempting to guess what
   emotion the child is acting out (eg, surprised, scared, or disgusted).
   During a 90-second game session, as many as 50 prompts are shown while
   the child acts, and the video records the actions and expressions of the
   child. Due in part to the fun nature of the game, it is a viable way to
   remotely engage pediatric populations, including the autism population
   through crowdsourcing. We recruited 21 children with ASD to play the
   game and gathered 2602 emotive frames following their game sessions.
   These data were used to evaluate the accuracy and performance of four
   state-of-the-art facial emotion classifiers to develop an understanding
   of the feasibility of these platforms for pediatric research.
   Results: All classifiers performed poorly for every evaluated emotion
   except happy. None of the classifiers correctly labeled over 60.18\%
   (1566/2602) of the evaluated frames. Moreover, none of the classifiers
   correctly identified more than 11\% (6/51) of the angry frames and 14\%
   (10/69) of the disgust frames.
   Conclusions: The findings suggest that commercial emotion classifiers
   may be insufficiently trained for use in digital approaches to autism
   treatment and treatment tracking. Secure, privacy-preserving methods to
   increase labeled training data are needed to boost the models'
   performance before they can be used in AI-enabled approaches to social
   therapy of the kind that is common in autism treatments.},
Publisher = {JMIR PUBLICATIONS, INC},
Address = {130 QUEENS QUAY East, Unit 1100, TORONTO, ON M5A 0P6, CANADA},
Type = {Article},
Language = {English},
Affiliation = {Wall, DP (Corresponding Author), Stanford Univ, Dept Pediat, 450 Serra Mall, Stanford, CA 94305 USA.
   Kalantarian, Haik; Dunlap, Kaitlyn; Schwartz, Jessey; Washington, Peter; Husic, Arman; Tariq, Qandeel; Ning, Michael; Kline, Aaron; Wall, Dennis Paul, Stanford Univ, Dept Pediat, 450 Serra Mall, Stanford, CA 94305 USA.
   Kalantarian, Haik; Dunlap, Kaitlyn; Schwartz, Jessey; Washington, Peter; Husic, Arman; Tariq, Qandeel; Ning, Michael; Kline, Aaron; Wall, Dennis Paul, Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA.
   Jedoui, Khaled, Stanford Univ, Dept Math, Stanford, CA 94305 USA.
   Wall, Dennis Paul, Stanford Univ, Dept Psychiat \& Behav Sci, Stanford, CA 94305 USA.},
DOI = {10.2196/13174},
Article-Number = {e13174},
ISSN = {2368-7959},
Keywords = {mobile phone; emotion; autism; digital data; mobile app; mHealth;
   affect; machine learning; artificial intelligence; digital health},
Keywords-Plus = {EARLY BEHAVIORAL INTERVENTION; FACIAL AFFECT RECOGNITION; EARLY
   IDENTIFICATION; SPECTRUM DISORDERS; DIAGNOSIS},
Research-Areas = {Psychiatry},
Web-of-Science-Categories  = {Psychiatry},
Author-Email = {dpwall@stanford.edu},
ORCID-Numbers = {Schwartz, Jessey/0000-0003-4916-7891
   Wall, Dennis/0000-0002-7889-9146
   husic, arman/0000-0002-9180-5212
   washington, peter/0000-0003-3276-4411
   Ning, Michael/0000-0002-8334-1104
   JEDOUI, Khaled/0000-0001-9838-6658
   Tariq, Qandeel/0000-0002-5217-8332},
Funding-Acknowledgement = {National Institute of Health {[}1R01EB025025-01, 1R21HD091500-01];
   Hartwell Foundation; Bill and Melinda Gates Foundation; Coulter
   Foundation; Stanford's Human Centered Artificial Intelligence Program;
   Precision Health and Integrated Diagnostics Center; Beckman Center;
   Bio-X Center; Predictives and Diagnostics Accelerator Spectrum; Wu Tsai
   Neurosciences Institute Neuroscience: Translate Program; Stanford Spark;
   Weston Havens Foundation; Thrasher Research Fund; Stanford NLM Clinical
   Data Science program {[}T-15LM007033-35]},
Funding-Text = {This work was supported in part by funds to DW from the National
   Institute of Health (1R01EB025025-01 and 1R21HD091500-01), the Hartwell
   Foundation, Bill and Melinda Gates Foundation, Coulter Foundation, and
   program grants from Stanford's Human Centered Artificial Intelligence
   Program, Precision Health and Integrated Diagnostics Center, Beckman
   Center, Bio-X Center, Predictives and Diagnostics Accelerator Spectrum,
   the Wu Tsai Neurosciences Institute Neuroscience: Translate Program,
   Stanford Spark, and the Weston Havens Foundation. We also acknowledge
   generous support from Bobby Dekesyer and Peter Sullivan. Dr Haik
   Kalantarian would like to acknowledge the support from the Thrasher
   Research Fund and Stanford NLM Clinical Data Science program
   (T-15LM007033-35).},
Number-of-Cited-References = {59},
Times-Cited = {11},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {22},
Journal-ISO = {JMIR Ment. Health},
Doc-Delivery-Number = {KZ5MG},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000523305700001},
OA = {Green Published, gold},
DA = {2022-09-28},
}

@Article{WOS:000455331200022,
  author                     = {Ozturk, Mahiye Uluyagmur and Arman, Ayse Rodopman and Bulut, Gresa Carkaxhiu and Findik, Onur Tugce Poyraz and Yilmaz, Sultan Seval and Genc, Herdem Aslan and Yazgan, M. Yanki and Teker, Umut and Cataltepe, Zehra},
  journal                    = {INTELLIGENT AUTOMATION AND SOFT COMPUTING},
  title                      = {Statistical Analysis and Multimodal Classification on Noisy Eye Tracker and Application Log Data of children with Autism and ADHD},
  year                       = {2018},
  issn                       = {1079-8587},
  month                      = {DEC},
  number                     = {4},
  pages                      = {891-906},
  volume                     = {24},
  abstract                   = {Emotion recognition behavior and performance may vary between people
   with major neurodevelopmental disorders such as Autism Spectrum Disorder
   (ASD), Attention Deficit Hyperactivity Disorder (ADHD) and control
   groups. It is crucial to identify these differences for early diagnosis
   and individual treatment purposes. This study represents a methodology
   by using statistical data analysis and machine learning to provide help
   to psychiatrists and therapists on the diagnosis and individualized
   treatment of participants with ASD and ADHD. In this paper we propose an
   emotion recognition experiment environment and collect eye tracker
   fixation data together with the application log data (APL). In order to
   detect the diagnosis of the participant we used classification
   algorithms with the Tomek links noise removing method. The highest
   classification accuracy results were reported as 86.36\% for ASD vs.
   Control, 81.82\% for ADHD vs. Control and 70.83\% for ASD vs. ADHD. This
   study provides evidence that fixation and APL data have distinguishing
   features for the diagnosis of ASD and ADHD.},
  address                    = {871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA},
  affiliation                = {Ozturk, MU (Corresponding Author), Istanbul Tech Univ, Comp Engn Dept, Istanbul, Turkey. Ozturk, Mahiye Uluyagmur; Teker, Umut; Cataltepe, Zehra, Istanbul Tech Univ, Comp Engn Dept, Istanbul, Turkey. Arman, Ayse Rodopman; Findik, Onur Tugce Poyraz; Genc, Herdem Aslan, Marmara Univ, Med Fac, Child \& Adolescent Psychiat, Istanbul, Turkey. Bulut, Gresa Carkaxhiu, Mus State Hosp Child \& Adolescent Psychiat, Mus, Turkey. Yilmaz, Sultan Seval, TC Hlth Municipal Medeniyet Univ Child \& Adolesce, Istanbul, Turkey. Yazgan, M. Yanki, Guzel Gunler Polyclin, Istanbul, Turkey. Yazgan, M. Yanki, Yale Child Study Ctr, New Haven, CT USA.},
  author-email               = {muluyagmur@itu.edu.tr},
  da                         = {2022-09-28},
  doc-delivery-number        = {HG9LY},
  eissn                      = {2326-005X},
  journal-iso                = {Intell. Autom. Soft Comput.},
  keywords                   = {Classification of Medical Diagnosis; Emotion Recognition Ability; Eye Tracking; Noise Removal},
  keywords-plus              = {FACIAL AFFECT RECOGNITION; SPECTRUM DISORDERS; PUPIL SIZE; ATTENTION; FIXATION; IMPAIRMENT; DIAGNOSIS; SCHEDULE; BEHAVIOR},
  language                   = {English},
  number-of-cited-references = {50},
  orcid-numbers              = {Fındık, Onur Tuğçe Poyraz/0000-0002-2376-7592},
  publisher                  = {TECH SCIENCE PRESS},
  research-areas             = {Automation \& Control Systems; Computer Science},
  researcherid-numbers       = {Fındık, Onur Tuğçe Poyraz/Y-4044-2018 Aslan Genç, Herdem/ABI-2040-2020 ARSLAN, Okan/AAA-3232-2020},
  times-cited                = {3},
  type                       = {Article},
  unique-id                  = {WOS:000455331200022},
  usage-count-last-180-days  = {5},
  usage-count-since-2013     = {15},
  web-of-science-categories  = {Automation \& Control Systems; Computer Science, Artificial Intelligence},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@article{ WOS:000684796000015,
Author = {Wu, Xingyu and Jiang, Bingbing and Yu, Kui and Chen, Huanhuan},
Title = {Separation and recovery Markov boundary discovery and its application in
   EEG-based emotion recognition},
Journal = {INFORMATION SCIENCES},
Year = {2021},
Volume = {571},
Pages = {262-278},
Month = {SEP},
Abstract = {In a Bayesian network (BN), the Markov boundary (MB) presents the local
   causal structure around a target. Due to the interpretability and
   robustness, it has been widely applied to feature selection and BN
   structure learning. However, existing MB discovery algorithms might fail
   to identify some true positives, leading to poor performance in
   real-world applications. To tackle this issue, we introduce a
   two-phase-discovery strategy to search more true positives. Based on
   this strategy, we propose a more accurate and data-efficient algorithm,
   separation and recovery MB discovery algorithm (SRMB). SRMB first
   discovers an incomplete parent-child set and spouse set via an MB
   separation process, and then retrieves the ignored true positives via an
   MB recovery process, which further exploits a symmetry test to improve
   accuracy in unfaithful cases. Experiments on standard BN and real-world
   data sets demonstrate the effectiveness and superiority of SRMB in terms
   of MB discovery, BN structure learning, and feature selection. To
   demonstrate the superiority of SRMB in data with distribution shift, we
   further apply SRMB to EEG-based emotion recognition tasks, where
   distribution shift exists in multiple unstable sessions. We prove that
   the most predictive features are from Gamma/Beta frequency bands and are
   distributed at the lateral temporal area. (c) 2021 Elsevier Inc. All
   rights reserved.},
Publisher = {ELSEVIER SCIENCE INC},
Address = {STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA},
Type = {Article},
Language = {English},
Affiliation = {Chen, HH (Corresponding Author), Univ Sci \& Technol China, Sch Comp Sci \& Technol, Hefei 230027, Peoples R China.
   Wu, Xingyu; Chen, Huanhuan, Univ Sci \& Technol China, Sch Comp Sci \& Technol, Hefei 230027, Peoples R China.
   Jiang, Bingbing, Hangzhou Normal Univ, Sch Informat Sci \& Engn, Hangzhou 311121, Peoples R China.
   Yu, Kui, Hefei Univ Technol, Sch Comp Sci \& Informat Engn, Hefei 230601, Peoples R China.},
DOI = {10.1016/j.ins.2021.04.071},
EarlyAccessDate = {MAY 2021},
ISSN = {0020-0255},
EISSN = {1872-6291},
Keywords = {Bayesian network (BN); Markov boundary (MB); Feature selection;
   Electroencephalography (EEG); Causality; Markov blanket},
Keywords-Plus = {FEATURE-SELECTION; EFFICIENT; ALGORITHMS; BLANKETS; SEARCH},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems},
Author-Email = {xingyuwu@mail.ustc.edu.cn
   jiangbb@hznu.edu.cn
   yukui@hfut.edu.cn
   hchen@ustc.edu.cn},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}91746209, 62006065,
   61876206]; Open Project Foundation of Intelligent Information Processing
   Key Laboratory of Shanxi Province {[}CICIP2020003]; Scientific Research
   Foundation of HZNU {[}4115C50220204003]; Fundamental Research Funds for
   the Central Universities},
Funding-Text = {We thank the native speaker Muhammad Usman for help to improve the
   language. This research is supported in part by the National Natural
   Science Foundation of China under Grant No. 91746209, 62006065, and
   61876206, the Open Project Foundation of Intelligent Information
   Processing Key Laboratory of Shanxi Province under Grant No.
   CICIP2020003, the Scientific Research Foundation of HZNU under Grant No.
   4115C50220204003, and the Fundamental Research Funds for the Central
   Universities.},
Number-of-Cited-References = {48},
Times-Cited = {8},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {12},
Journal-ISO = {Inf. Sci.},
Doc-Delivery-Number = {TZ9OB},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000684796000015},
DA = {2022-09-28},
}

@Article{WOS:000608175000013,
  author                     = {Camada, Marcos Y. O. and Cerqueira, Jes J. F. and Lima, Antonio M. N.},
  journal                    = {APPLIED SOFT COMPUTING},
  title                      = {Computational model for identifying stereotyped behaviors and determining the activation level of pseudo-autistic},
  year                       = {2021},
  issn                       = {1568-4946},
  month                      = {FEB},
  volume                     = {99},
  status                     = {Aceito},
  abstract                   = {Affective state recognition of an individual is based on the emotional
   cues, such as the activation level. Body expression is a modal able to
   convey emotions and can be used for autism diagnosis through the
   presence of stereotyped behaviors (SBs). These behaviors are atypical
   and repetitive movements of the body, which can be related to a low
   mental health condition. The development of systems able to both
   recognize SBs and inferring activation level can automatically aid some
   therapeutic approaches. In this paper, a computational model of low
   intrusiveness is proposed to infer activation levels from recognized
   SBs, Machine Learning Algorithms (MLAs) are for identifying the SBs and
   for determining the related activation levels. A metric performance is
   also proposed to evaluate the performance of MLAs considering the time
   for classification of the SBs, accuracy, and precision. For classifying
   the SBs, the Hidden Markov Models and Multilayer Perceptron presented
   the best performance than Support Vector Machine and Convolutional
   Neural Network. The Adaptive Neuro-Fuzzy technique based on the Fuzzy
   C-Means algorithm allowed one to determine and differentiate the
   activation levels of the stereotyped behaviors considered in the present
   study. The experiments were performed with non-autistic participants,
   here referred to as pseudo-autistic. (C) 2020 Elsevier B.V. All rights
   reserved.},
  address                    = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  affiliation                = {Camada, MYO (Corresponding Author), Inst Fed Baiano, Rua Barao Camacari 118, BR-48110000 Catu, BA, Brazil. Camada, Marcos Y. O., Inst Fed Baiano, Rua Barao Camacari 118, BR-48110000 Catu, BA, Brazil. Cerqueira, Jes J. F., Univ Fed Bahia, Dept Elect Engn \& Comp, Rua Prof Aristides Novis,02, BR-40210630 Salvador, BA, Brazil. Lima, Antonio M. N., Univ Fed Campina Grande, Dept Elect Engn, Rua Aprigio Veloso,882, BR-58429900 Campina Grande, PB, Brazil.},
  article-number             = {106877},
  author-email               = {marcos.camada@ifbaiano.edu.br jes@ufba.edu.br amnlima@dee.ufcg.edu.br},
  da                         = {2022-09-28},
  doc-delivery-number        = {PS8LI},
  doi                        = {10.1016/j.asoc.2020.106877},
  earlyaccessdate            = {JAN 2021},
  eissn                      = {1872-9681},
  funding-acknowledgement    = {CAPES, Brazil},
  funding-text               = {The authors would like to thank all volunteer participants for allowing recording and creating samples dataset of the stereotyped behaviors used in this paper. The authors also thank CAPES, Brazil for their financial support for this research.},
  journal-iso                = {Appl. Soft. Comput.},
  keywords                   = {Machine learning algorithm; Performance analysis; Affective state; Stereotyped behavior; Autism},
  keywords-plus              = {EMOTION RECOGNITION; SYSTEM; FRAMEWORK; MOVEMENT; CHILDREN},
  language                   = {English},
  number-of-cited-references = {52},
  orcid-numbers              = {Lima, Antonio Marcus Nogueira/0000-0002-4568-9126 Cerqueira, Jés J. F./0000-0003-4072-0101},
  priority                   = {prio3},
  publisher                  = {ELSEVIER},
  research-areas             = {Computer Science},
  researcherid-numbers       = {Lima, Antonio Marcus Nogueira/F-9201-2014 Cerqueira, Jés J. F./P-5510-2016},
  times-cited                = {3},
  type                       = {Article},
  unique-id                  = {WOS:000608175000013},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {3},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000480354200004,
  author                     = {Filntisis, Panagiotis Paraskevas and Efthymiou, Niki and Koutras, Petros and Potamianos, Gerasimos and Maragos, Petros},
  journal                    = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  title                      = {Fusing Body Posture With Facial Expressions for Joint Recognition of Affect in Child-Robot Interaction},
  year                       = {2019},
  issn                       = {2377-3766},
  month                      = {OCT},
  number                     = {4},
  pages                      = {4011-4018},
  volume                     = {4},
  status                     = {Aceito},
  abstract                   = {In this letter, we address the problem of multi-cue affect recognition
   in challenging scenarios such as child-robot interaction. Toward this
   goal we propose a method for automatic recognition of affect that
   leverages body expressions alongside facial ones, as opposed to
   traditional methods that typically focus only on the latter. Our
   deep-learning based method uses hierarchical multi-label annotations and
   multi-stage losses, can be trained both jointly and separately, and
   offers us computational models for both individual modalities, as well
   as for the whole body emotion. We evaluate our method on a challenging
   child-robot interaction database of emotional expressions collected by
   us, as well as on the GEneva multimodal emotion portrayal public
   database of acted emotions by adults, and show that the proposed method
   achieves significantly better results than facial-only expression
   baselines.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Filntisis, PP (Corresponding Author), Natl Tech Univ Athens, Sch Elect \& Comp Engn, Zografos 15780, Greece. Filntisis, Panagiotis Paraskevas; Efthymiou, Niki; Koutras, Petros; Maragos, Petros, Natl Tech Univ Athens, Sch Elect \& Comp Engn, Zografos 15780, Greece. Filntisis, Panagiotis Paraskevas; Efthymiou, Niki; Koutras, Petros; Potamianos, Gerasimos; Maragos, Petros, Athena Res \& Innovat Ctr, Maroussi 15125, Greece. Potamianos, Gerasimos, Univ Thessaly, Dept Elect \& Comp Engn, Volos 38221, Greece.},
  author-email               = {filby@central.ntua.gr nefthymiou@central.ntua.gr pkoutras@cs.ntua.gr gpotam@ieee.org maragos@cs.ntua.gr},
  da                         = {2022-09-28},
  doc-delivery-number        = {IP9EY},
  doi                        = {10.1109/LRA.2019.2930434},
  funding-acknowledgement    = {EU-funded Project BabyRobot H2020 {[}687831]},
  funding-text               = {This work was supported by the EU-funded Project BabyRobot H2020 under Grant Agreement 687831.},
  journal-iso                = {IEEE Robot. Autom. Lett.},
  keywords                   = {Gesture; posture and facial expressions; computer vision for other robotic applications; social human-robot interaction; deep learning in robotics and automation},
  keywords-plus              = {EMOTION; FACE; GESTURE},
  language                   = {English},
  number-of-cited-references = {49},
  oa                         = {Green Submitted},
  orcid-numbers              = {Koutras, Petros/0000-0003-3183-2726 Filntisis, Panagiotis Paraskevas/0000-0002-2042-245X},
  priority                   = {prio1},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Robotics},
  researcherid-numbers       = {ARSLAN, Okan/AAA-3232-2020 Koutras, Petros/AAR-2802-2021 Potamianos, Gerasimos/AAP-1495-2021},
  times-cited                = {21},
  type                       = {Article},
  unique-id                  = {WOS:000480354200004},
  usage-count-last-180-days  = {5},
  usage-count-since-2013     = {18},
  web-of-science-categories  = {Robotics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000577077800002,
  author                     = {de Belen, Ryan Anthony J. and Bednarz, Tomasz and Sowmya, Arcot and Del Favero, Dennis},
  journal                    = {TRANSLATIONAL PSYCHIATRY},
  title                      = {Computer vision in autism spectrum disorder research: a systematic review of published studies from 2009 to 2019},
  year                       = {2020},
  issn                       = {2158-3188},
  month                      = {SEP 30},
  number                     = {1},
  volume                     = {10},
  status                     = {Aceito},
  abstract                   = {The current state of computer vision methods applied to autism spectrum
   disorder (ASD) research has not been well established. Increasing
   evidence suggests that computer vision techniques have a strong impact
   on autism research. The primary objective of this systematic review is
   to examine how computer vision analysis has been useful in ASD
   diagnosis, therapy and autism research in general. A systematic review
   of publications indexed on PubMed, IEEE Xplore and ACM Digital Library
   was conducted from 2009 to 2019. Search terms included {[}'autis{*}' AND
   ('computer vision' OR `behavio{*} imaging' OR `behavio{*} analysis' OR
   `affective computing')]. Results are reported according to PRISMA
   statement. A total of 94 studies are included in the analysis. Eligible
   papers are categorised based on the potential biological/behavioural
   markers quantified in each study. Then, different computer vision
   approaches that were employed in the included papers are described.
   Different publicly available datasets are also reviewed in order to
   rapidly familiarise researchers with datasets applicable to their field
   and to accelerate both new behavioural and technological work on autism
   research. Finally, future research directions are outlined. The findings
   in this review suggest that computer vision analysis is useful for the
   quantification of behavioural/biological markers which can further lead
   to a more objective analysis in autism research.},
  address                    = {CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND},
  affiliation                = {de Belen, RAJ (Corresponding Author), Univ New South Wales, Sch Art \& Design, Sydney, NSW, Australia. de Belen, Ryan Anthony J.; Bednarz, Tomasz; Del Favero, Dennis, Univ New South Wales, Sch Art \& Design, Sydney, NSW, Australia. Sowmya, Arcot, Univ New South Wales, Sch Comp Sci \& Engn, Sydney, NSW, Australia.},
  article-number             = {333},
  author-email               = {r.debelen@unsw.edu.au},
  da                         = {2022-09-28},
  doc-delivery-number        = {NZ4PQ},
  doi                        = {10.1038/s41398-020-01015-w},
  journal-iso                = {Transl. Psychiatr.},
  keywords-plus              = {FACIAL EMOTION; INFANTS; CHILDREN; RECOGNITION; EXPRESSION; 1ST; CLASSIFICATION; FEASIBILITY; INDIVIDUALS; RELIABILITY},
  language                   = {English},
  number-of-cited-references = {149},
  oa                         = {gold, Green Published},
  orcid-numbers              = {Bednarz, Tomasz/0000-0001-9240-0922 de Belen, Ryan Anthony J./0000-0002-4624-2668},
  publisher                  = {SPRINGERNATURE},
  research-areas             = {Psychiatry},
  researcherid-numbers       = {Bednarz, Tomasz/AAQ-2605-2021},
  times-cited                = {17},
  type                       = {Review},
  unique-id                  = {WOS:000577077800002},
  usage-count-last-180-days  = {6},
  usage-count-since-2013     = {17},
  web-of-science-categories  = {Psychiatry},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@InProceedings{WOS:000546032800017,
  author                     = {Spitale, Micol and Catania, Fabio and Cosentino, Giulia and Gelsomini, Mirko and Garzotto, Franca},
  booktitle                  = {PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES: COMPANION (IUI 2019)},
  title                      = {WIYE: building a corpus of children's audio and video recordings with a story-based app},
  year                       = {2019},
  address                    = {1515 BROADWAY, NEW YORK, NY 10036-9998 USA},
  note                       = {24th International Conference on Intelligent User Interfaces (IUI), Marina Del Rey, CA, MAR 17-20, 2019},
  organization               = {Assoc Comp Machinery},
  pages                      = {33-34},
  publisher                  = {ASSOC COMPUTING MACHINERY},
  abstract                   = {This paper describes the procedure to create an emotional dataset, named
   WIYE (What Is Your Emotion?), composed by semantic contents, audio and
   video recordings of children. Data have been collected using an
   interactive storytelling application, leading children aged 4 years to
   12 years to discover about their emotional sphere and emotion expression
   skills. During the story, every episode is dedicated and focused on a
   specific emotion. The investigated emotional states are sadness, anger,
   fear, surprise and joy. This corpus can be exploited by Conversational
   Technologies: it can be used with Machine Learning classification
   algorithms to train models to recognize emotions expressed by children
   starting from the pitch of their voice, their facial expression and the
   contents of their conversations.},
  affiliation                = {Spitale, M (Corresponding Author), Politecn Milan, Milan, Italy. Spitale, Micol; Catania, Fabio; Cosentino, Giulia; Gelsomini, Mirko; Garzotto, Franca, Politecn Milan, Milan, Italy.},
  author-email               = {micol.spitale@polimi.it fabio.catania@polimi.it giulia.cosentino@polimi.it mirko.gelsomini@polimi.it franca.garzotto@polimi.it},
  book-group-author          = {ACM},
  da                         = {2022-09-28},
  doc-delivery-number        = {BP3BB},
  doi                        = {10.1145/3308557.3308684},
  isbn                       = {978-1-4503-6673-1},
  keywords                   = {Emotional speech and facial corpus; Emotional database; Affective Computing; Emotion Recognition; Emotion classification},
  language                   = {English},
  number-of-cited-references = {8},
  orcid-numbers              = {Gelsomini, Mirko/0000-0001-8421-6850 CATANIA, FABIO/0000-0002-5403-9002},
  research-areas             = {Computer Science},
  researcherid-numbers       = {Gelsomini, Mirko/AAG-9874-2019},
  times-cited                = {2},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000546032800017},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {0},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Software Engineering},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000756540200001,
  author                     = {Hong, Jiuk and Lee, Chaehyeon and Jung, Heechul},
  journal                    = {APPLIED SCIENCES-BASEL},
  title                      = {Late Fusion-Based Video Transformer for Facial Micro-Expression Recognition},
  year                       = {2022},
  month                      = {FEB},
  number                     = {3},
  volume                     = {12},
  status                     = {Aceito},
  abstract                   = {In this article, we propose a novel model for facial micro-expression
   (FME) recognition. The proposed model basically comprises a transformer,
   which is recently used for computer vision and has never been used for
   FME recognition. A transformer requires a huge amount of data compared
   to a convolution neural network. Then, we use motion features, such as
   optical flow and late fusion to complement the lack of FME dataset. The
   proposed method was verified and evaluated using the SMIC and CASME II
   datasets. Our approach achieved state-of-the-art (SOTA) performance of
   0.7447 and 73.17\% in SMIC in terms of unweighted F1 score (UF1) and
   accuracy (Acc.), respectively, which are 0.31 and 1.8\% higher than
   previous SOTA. Furthermore, UF1 of 0.7106 and Acc. of 70.68\% were shown
   in the CASME II experiment, which are comparable with SOTA.},
  address                    = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  affiliation                = {Jung, H (Corresponding Author), Kyungpook Natl Univ, Dept Artificial Intelligence, Daegu 37224, South Korea. Hong, Jiuk; Lee, Chaehyeon; Jung, Heechul, Kyungpook Natl Univ, Dept Artificial Intelligence, Daegu 37224, South Korea.},
  article-number             = {1169},
  author-email               = {hong4497@knu.ac.kr 123456ccdd@knu.ac.kr heechul@knu.ac.kr},
  da                         = {2022-09-28},
  doc-delivery-number        = {ZB0JU},
  doi                        = {10.3390/app12031169},
  eissn                      = {2076-3417},
  funding-acknowledgement    = {Institute of Information \& Communications Technology Planning \& Evaluation (IITP); Korea government(MSIT) {[}2019-0-00330, 2020R1C1C1007423]; National Research Foundation of Korea (NRF); National Research Foundation (NRF), Korea {[}BK21 FOUR]},
  funding-text               = {This work was supported by Ins<STRIKE>t</STRIKE>itute of Information \& Communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government(MSIT) (2019-0-00330, Development of AI Technology for Early Screening of Infant/Child Autism Spectrum Disorders based on Cognition of the Psychological Behavior and Response). Furthermore, this research was also supported in part by a National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. 2020R1C1C1007423). In addition, this work was partly supported by the National Research Foundation (NRF), Korea, under project BK21 FOUR.},
  journal-iso                = {Appl. Sci.-Basel},
  keywords                   = {deep learning; image processing; facial micro-expression; emotion recognition; vision transformer},
  language                   = {English},
  number-of-cited-references = {28},
  oa                         = {gold},
  orcid-numbers              = {Jung, Heechul/0000-0002-3005-2560 Hong, Jiuk/0000-0002-9306-4825 Lee, Chaehyeon/0000-0003-3780-3870},
  priority                   = {prio1},
  publisher                  = {MDPI},
  research-areas             = {Chemistry; Engineering; Materials Science; Physics},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000756540200001},
  usage-count-last-180-days  = {5},
  usage-count-since-2013     = {6},
  web-of-science-categories  = {Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials Science, Multidisciplinary; Physics, Applied},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000428690000006,
  author                     = {Daniels, Jena and Haber, Nick and Voss, Catalin and Schwartz, Jessey and Tamura, Serena and Fazel, Azar and Kline, Aaron and Washington, Peter and Phillips, Jennifer and Winograd, Terry and Feinstein, Carl and Wall, Dennis P.},
  journal                    = {APPLIED CLINICAL INFORMATICS},
  title                      = {Feasibility Testing of a Wearable Behavioral Aid for Social Learning in Children with Autism},
  year                       = {2018},
  issn                       = {1869-0327},
  month                      = {JAN},
  number                     = {1},
  pages                      = {129-140},
  volume                     = {9},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Background Recent advances in computer vision and wearable technology
   have created an opportunity to introduce mobile therapy systems for
   autism spectrum disorders (ASD) that can respond to the increasing
   demand for therapeutic interventions; however, feasibility questions
   must be answered first.
   Objective We studied the feasibility of a prototype therapeutic tool for
   children with ASD using Google Glass, examining whether children with
   ASD would wear such a device, if providing the emotion classification
   will improve emotion recognition, and how emotion recognition differs
   between ASD participants and neurotypical controls (NC).
   Methods We ran a controlled laboratory experiment with 43 children: 23
   with ASD and 20 NC. Children identified static facial images on a
   computer screen with one of 7 emotions in 3 successive batches: the
   first with no information about emotion provided to the child, the
   second with the correct classification from the Glass labeling the
   emotion, and the third again without emotion information. We then
   trained a logistic regression classifier on the emotion confusion
   matrices generated by the two information-free batches to predict ASD
   versus NC.
   Results All 43 children were comfortable wearing the Glass. ASD and NC
   participants who completed the computer task with Glass providing
   audible emotion labeling (n = 33) showed increased accuracies in emotion
   labeling, and the logistic regression classifier achieved an accuracy of
   72.7\%. Further analysis suggests that the ability to recognize
   surprise, fear, and neutrality may distinguish ASD cases from NC.
   Conclusion This feasibility study supports the utility of a wearable
   device for social affective learning in ASD children and demonstrates
   subtle differences in how ASD and NC children perform on an emotion
   recognition task.},
  address                    = {RUDIGERSTR 14, D-70469 STUTTGART, GERMANY},
  affiliation                = {Wall, DP (Corresponding Author), Stanford Univ, Dept Biomed Data Sci, Dept Pediat, Div Syst Med, 1265 Welch Rd,Suite X143, Stanford, CA 94305 USA. Daniels, Jena; Haber, Nick; Voss, Catalin; Schwartz, Jessey; Fazel, Azar; Kline, Aaron; Washington, Peter; Wall, Dennis P., Stanford Univ, Dept Pediat, Div Syst Med, Stanford, CA 94305 USA. Voss, Catalin; Winograd, Terry, Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA. Tamura, Serena; Phillips, Jennifer; Feinstein, Carl, Stanford Univ, Dept Psychiat \& Behav Sci, Stanford, CA 94305 USA. Wall, Dennis P., Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA.},
  author-email               = {dpwall@stanford.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {GA9VT},
  doi                        = {10.1055/s-0038-1626727},
  funding-acknowledgement    = {NIH {[}1R01EB025025-01, 1R21HD091500-01]; Hartwell Foundation; Bill and Melinda Gates Foundation; Coulter Foundation; Lucile Packard Foundation; Stanford's Precision Health and Integrated Diagnostics Center (PHIND); Beckman Center; Bio-X Center; Predictives and Diagnostics Accelerator (SPADA) Spectrum; Child Health Research Institute; EUNICE KENNEDY SHRIVER NATIONAL INSTITUTE OF CHILD HEALTH \& HUMAN DEVELOPMENT {[}R21HD091500] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF BIOMEDICAL IMAGING AND BIOENGINEERING {[}R01EB025025] Funding Source: NIH RePORTER},
  funding-text               = {The work was supported in part by funds to D.P.W. from NIH (1R01EB025025-01 \& 1R21HD091500-01), The Hartwell Foundation, Bill and Melinda Gates Foundation, Coulter Foundation, Lucile Packard Foundation, and program grants from Stanford's Precision Health and Integrated Diagnostics Center (PHIND), Beckman Center, Bio-X Center, Predictives and Diagnostics Accelerator (SPADA) Spectrum, and Child Health Research Institute. We also acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer and Peter Sullivan.},
  journal-iso                = {Appl. Clin. Inform.},
  keywords                   = {autism spectrum disorder; pilot projects; wearable device and body area network; artificial intelligence; emotion recognition; pediatrics; people with disabilities or special needs},
  keywords-plus              = {FACIAL AFFECT RECOGNITION; COMPUTER-ASSISTED-INSTRUCTION; SPECTRUM DISORDERS; EMOTION RECOGNITION; VIRTUAL ENVIRONMENTS; 4-YEAR-OLD CHILDREN; INTERVENTIONS; PREVALENCE; COMMUNICATION; ADOLESCENTS},
  language                   = {English},
  number-of-cited-references = {64},
  oa                         = {Green Published, Bronze},
  orcid-numbers              = {Phillips, Jennifer/0000-0002-6360-2346 washington, peter/0000-0003-3276-4411},
  publisher                  = {GEORG THIEME VERLAG KG},
  research-areas             = {Medical Informatics},
  times-cited                = {23},
  type                       = {Article},
  unique-id                  = {WOS:000428690000006},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {35},
  web-of-science-categories  = {Medical Informatics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000807364500004,
  author                     = {Li, Xiaohong},
  journal                    = {SCIENTIFIC PROGRAMMING},
  title                      = {Expression Recognition of Classroom Children's Game Video Based on Improved Convolutional Neural Network},
  year                       = {2022},
  issn                       = {1058-9244},
  month                      = {APR 8},
  volume                     = {2022},
  status                     = {Aceito},
  abstract                   = {Humans express emotions in many ways, such as gestures, limbs, and
   expressions. Among them, facial expressions are the most intuitive way
   to express human inner emotional activities in human-to-human
   communication. With the rapid development of computer vision, facial
   expression recognition is an important research topic in the field of
   computer vision. It plays a key role in nonverbal communication and can
   be applied to human-computer interaction, social robotics, video games,
   and other fields. Traditional expression recognition algorithms require
   complex manual feature extraction, which takes a long time, and the
   accuracy of expression recognition in complex scenes is not high.
   However, with the development of deep learning, especially the
   convolutional neural network, facial expression recognition technology
   has also developed rapidly, and the recognition accuracy has been
   greatly improved. This paper studies the facial expression recognition
   method of classroom children's game video based on convolutional neural
   network and proposes a convolutional neural network with deeper layers.
   The full connection is modified to 4 layers of convolution, 4 layers of
   pooling, and 2 layers of full connection. Firstly, the facial expression
   image is preprocessed by, for example, key point location, face
   cropping, and image normalization; then, the convolutional layer is used
   to extract the low-dimensional and high-dimensional feature information
   of the face image; and the pooling layer is used to extract the face
   image. The feature information is dimensionally reduced. Finally, the
   softmax classifier is used to classify and recognize the expressions of
   the training sample images. In order to improve the accuracy of
   expression recognition, a self-made set of labeled pictures was added to
   the expression training set. Simulation and comparison experiments show
   that the improved model has higher accuracy and smoother loss curve,
   which verifies the effectiveness of the improved network.},
  address                    = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
  affiliation                = {Li, XH (Corresponding Author), Henan Inst Econ \& Trade, Zhengzhou 450000, Henan, Peoples R China. Li, Xiaohong, Henan Inst Econ \& Trade, Zhengzhou 450000, Henan, Peoples R China.},
  article-number             = {5203022},
  author-email               = {xiaoxiao518@henetc.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {1X3NL},
  doi                        = {10.1155/2022/5203022},
  eissn                      = {1875-919X},
  funding-acknowledgement    = {Research on the Cultivation Path of ``Artisan Spirit{''} for Students in Henan Vocational Colleges under the Background of Quality Improvement and Excellence {[}149]},
  funding-text               = {The author thanks 2022 Henan Province Key Research and Development and Promotion Special Project (Science and Technology Research, Soft Science Research Support Project) and Research on the Cultivation Path of ``Artisan Spirit{''} for Students in Henan Vocational Colleges under the Background of Quality Improvement and Excellence (no. 149).},
  journal-iso                = {Sci. Program.},
  keywords-plus              = {WINNERS CURSE; MISPRONUNCIATION DETECTION; DECISION-MAKING; AMBIGUITY; LANGUAGE},
  language                   = {English},
  number-of-cited-references = {30},
  oa                         = {Green Published},
  priority                   = {prio1},
  publisher                  = {HINDAWI LTD},
  research-areas             = {Computer Science},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000807364500004},
  usage-count-last-180-days  = {4},
  usage-count-since-2013     = {4},
  web-of-science-categories  = {Computer Science, Software Engineering},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000685088900001,
  author                     = {Ni, Tongguang and Ni, Yuyao and Xue, Jing and Wang, Suhong},
  journal                    = {FRONTIERS IN PSYCHOLOGY},
  title                      = {A Domain Adaptation Sparse Representation Classifier for Cross-Domain Electroencephalogram-Based Emotion Classification},
  year                       = {2021},
  issn                       = {1664-1078},
  month                      = {JUL 29},
  volume                     = {12},
  status                     = {Rejeitado - Escopo},
  abstract                   = {The brain-computer interface (BCI) interprets the physiological
   information of the human brain in the process of consciousness activity.
   It builds a direct information transmission channel between the brain
   and the outside world. As the most common non-invasive BCI modality,
   electroencephalogram (EEG) plays an important role in the emotion
   recognition of BCI; however, due to the individual variability and
   non-stationary of EEG signals, the construction of EEG-based emotion
   classifiers for different subjects, different sessions, and different
   devices is an important research direction. Domain adaptation utilizes
   data or knowledge from more than one domain and focuses on transferring
   knowledge from the source domain (SD) to the target domain (TD), in
   which the EEG data may be collected from different subjects, sessions,
   or devices. In this study, a new domain adaptation sparse representation
   classifier (DASRC) is proposed to address the cross-domain EEG-based
   emotion classification. To reduce the differences in domain
   distribution, the local information preserved criterion is exploited to
   project the samples from SD and TD into a shared subspace. A common
   domain-invariant dictionary is learned in the projection subspace so
   that an inherent connection can be built between SD and TD. In addition,
   both principal component analysis (PCA) and Fisher criteria are
   exploited to promote the recognition ability of the learned dictionary.
   Besides, an optimization method is proposed to alternatively update the
   subspace and dictionary learning. The comparison of CSFDDL shows the
   feasibility and competitive performance for cross-subject and
   cross-dataset EEG-based emotion classification problems.},
  address                    = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
  affiliation                = {Xue, J (Corresponding Author), Nanjing Med Univ, Dept Nephrol, Affiliated Wuxi Peoples Hosp, Wuxi, Jiangsu, Peoples R China. Wang, SH (Corresponding Author), Soochow Univ, Dept Clin Psychol, Affiliated Hosp 3, Changzhou, Peoples R China. Ni, Tongguang, Changzhou Univ, Sch Comp Sci \& Artificial Intelligence, Changzhou, Peoples R China. Ni, Yuyao, Xi An Jiao Tong Univ, Sch Elect Engn, Xian, Peoples R China. Xue, Jing, Nanjing Med Univ, Dept Nephrol, Affiliated Wuxi Peoples Hosp, Wuxi, Jiangsu, Peoples R China. Wang, Suhong, Soochow Univ, Dept Clin Psychol, Affiliated Hosp 3, Changzhou, Peoples R China.},
  article-number             = {721266},
  author-email               = {xuejing@njmu.edu.cn yarmine@foxmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {UA3VG},
  doi                        = {10.3389/fpsyg.2021.721266},
  funding-acknowledgement    = {National Natural Science Foundation of China {[}61806026]; research project of maternal and child health in Jiangsu Province {[}F202060]; Natural Science Foundation of Jiangsu Province {[}SBK2021022110]},
  funding-text               = {This work was supported in part by the National Natural Science Foundation of China under Grants 61806026, the research project of maternal and child health in Jiangsu Province under Grants F202060, and the Natural Science Foundation of Jiangsu Province under Grants SBK2021022110.},
  journal-iso                = {Front. Psychol.},
  keywords                   = {electroencephalogram; domain adaptation; emotion classification; cross-subject; cross-dataset},
  keywords-plus              = {K-SVD; RECOGNITION; DICTIONARY; INVARIANT; ALGORITHM; FEATURES},
  language                   = {English},
  number-of-cited-references = {33},
  oa                         = {gold, Green Published},
  publisher                  = {FRONTIERS MEDIA SA},
  research-areas             = {Psychology},
  researcherid-numbers       = {Ni, Tongguang/GPP-6949-2022 Gu, Xiaoqing/GPP-6913-2022},
  times-cited                = {3},
  type                       = {Article},
  unique-id                  = {WOS:000685088900001},
  usage-count-last-180-days  = {13},
  usage-count-since-2013     = {29},
  web-of-science-categories  = {Psychology, Multidisciplinary},
  web-of-science-index       = {Social Science Citation Index (SSCI)},
}

@Article{WOS:000728926000020,
  author                     = {Aslam, Abdul Rehman and Bin Altaf, Muhammad Awais},
  journal                    = {IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS},
  title                      = {A 10.13 mu J/Classification 2-Channel Deep Neural Network Based SoC for Negative Emotion Outburst Detection of Autistic Children},
  year                       = {2021},
  issn                       = {1932-4545},
  month                      = {OCT},
  number                     = {5},
  pages                      = {1039-1052},
  volume                     = {15},
  status                     = {Rejeitado - Escopo},
  abstract                   = {An electroencephalogram (EEG)-based non-invasive 2-channel
   neuro-feedback SoC is presented to predict and report negative emotion
   outbursts (NEOB) of Autistic patients. The SoC incorporates
   area-and-power efficient dual-channel Analog Front-End (AFE), and a deep
   neural network (DNN) emotion classification processor. The
   classification processor utilizes only the two-feature vector per
   channel to minimize the area and overfitting problems. The 4-layers
   customized DNN classification processor is integrated on-sensor to
   predict the NEOB. The AFE comprises two entirely shared EEG channels
   using sampling capacitors to reduce the area by 30\%. Moreover, it
   achieves an overall integrated input-referred noise, NEF, and crosstalk
   of 0.55 mu V-RMS, 2.71, and -79 dB, respectively. The 16 mm(2) SoC is
   implemented in 0.18 um 1P6M, CMOS process and consumes 10.13 mu
   J/classification for 2 channel operation while achieving an average
   accuracy of >85\% on multiple emotion databases and real-time testing.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Bin Altaf, MA (Corresponding Author), Lahore Univ Management Sci, Elect Engn Dept, Lahore 54792, Pakistan. Aslam, Abdul Rehman; Bin Altaf, Muhammad Awais, Lahore Univ Management Sci, Elect Engn Dept, Lahore 54792, Pakistan.},
  author-email               = {17060056@lums.edu.pk awais.altaf@lums.edu.pk},
  da                         = {2022-09-28},
  doc-delivery-number        = {XM6HP},
  doi                        = {10.1109/TBCAS.2021.3113613},
  eissn                      = {1940-9990},
  funding-acknowledgement    = {National Center of Artificial Intelligence (NCAI) Research Fund {[}RF-NCAI-030]; Syed Babar Ali Research Award (SBARA), LUMS, Pakistan},
  funding-text               = {This work was supported in part by the National Center of Artificial Intelligence (NCAI) Research Fund under Grant \#RF-NCAI-030 and in part by Syed Babar Ali Research Award (SBARA), LUMS, Pakistan. This article was recommended by Associate Editor Prof. E. Y Lam.},
  journal-iso                = {IEEE Trans. Biomed. Circuits Syst.},
  keywords                   = {Feature extraction; Electroencephalography; Hardware; Classification algorithms; Deep learning; Standards; Real-time systems; Autism; classification processor; deep neural network (DNN); electroencephalogram (EEG); emotion detection; neurological disorder},
  keywords-plus              = {ACTIVATION FUNCTION; CIRCUMPLEX MODEL; EEG; DISORDERS; 8-CHANNEL; BURDEN; SYSTEM},
  language                   = {English},
  number-of-cited-references = {52},
  orcid-numbers              = {Altaf, Muhammad Awais Bin/0000-0003-3615-3546 ASLAM, ABDUL REHMAN/0000-0001-7767-2575},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Engineering},
  times-cited                = {6},
  type                       = {Article},
  unique-id                  = {WOS:000728926000020},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {4},
  web-of-science-categories  = {Engineering, Biomedical; Engineering, Electrical \& Electronic},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@InProceedings{WOS:000749408300006,
  author                     = {Chauhan, Harshit and Prasad, Anmol and Shukla, Jainendra},
  booktitle                  = {COMPANION PUBLICATON OF THE 2020 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (ICMI `20 COMPANION)},
  title                      = {Engagement Analysis of ADHD Students using Visual Cues from Eye Tracker},
  year                       = {2020},
  address                    = {1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES},
  note                       = {International Conference on Multimodal Interaction (ICMI), ELECTR NETWORK, OCT 25-29, 2020},
  organization               = {Assoc Comp Machinery; ACM SIGCHI},
  pages                      = {27-31},
  publisher                  = {ASSOC COMPUTING MACHINERY},
  status                     = {Aceito},
  abstract                   = {In this paper, we focus on finding the correlation between visual
   attention and engagement of ADHD students in one-on-one sessions with
   specialized educators using visual cues and eye-tracking data. Our goal
   is to investigate the extent to which observations of eye-gaze, posture,
   emotion and other physiological signals can be used to model the
   cognitive state of subjects and to explore the integration of multiple
   sensor modalities to improve the reliability of detection of human
   displays of awareness and emotion in the context of ADHD affected
   children. This is a novel problem since no previous studies have aimed
   to identify markers of attentiveness in the context of students affected
   with ADHD. The experiment has been designed to collect data in a
   controlled environment and later on can be used to generate Machine
   Learning models to assist real-world educators. Additionally, we propose
   a novel approach for AOI (Area of Interest) detection for eye-tracking
   analysis in dynamic scenarios using existing deep learning-based
   saliency prediction and fixation prediction models. We aim to use the
   processed data to extract the features from a subject's eye-movement
   patterns and use Machine Learning models to classify the attention
   levels.},
  affiliation                = {Chauhan, H (Corresponding Author), IIIT Delhi, HMI Lab, New Delhi, India. Chauhan, Harshit; Prasad, Anmol; Shukla, Jainendra, IIIT Delhi, HMI Lab, New Delhi, India.},
  author-email               = {hchauhan012@gmail.com anmol16012@iiitd.ac.in jainendra@iiitd.ac.in},
  book-group-author          = {ACM},
  da                         = {2022-09-28},
  doc-delivery-number        = {BS6LE},
  doi                        = {10.1145/3395035.3425256},
  funding-acknowledgement    = {Infosys Centre of Artificial Intelligence; TCS -Centre for Design and New Media},
  funding-text               = {This research is partly supported through Infosys Centre of Artificial Intelligence and TCS -Centre for Design and New Media.},
  isbn                       = {978-1-4503-8002-7},
  keywords                   = {Eye Tracking; Saliency prediction; Gaze detection; Machine Learning},
  keywords-plus              = {ATTENTION-DEFICIT/HYPERACTIVITY DISORDER; GAZE; MOVEMENTS; ADULTS},
  language                   = {English},
  number-of-cited-references = {41},
  priority                   = {prio1},
  research-areas             = {Computer Science; Engineering},
  times-cited                = {1},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000749408300006},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {2},
  web-of-science-categories  = {Computer Science, Cybernetics; Computer Science, Theory \& Methods; Engineering, Electrical \& Electronic},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{ WOS:000838718500042,
Author = {Moctezuma, Luis Alfredo and Abe, Takashi and Molinas, Marta},
Title = {Two-dimensional CNN-based distinction of human emotions from EEG
   channels selected by multi-objective evolutionary algorithm},
Journal = {SCIENTIFIC REPORTS},
Year = {2022},
Volume = {12},
Number = {1},
Month = {MAR 3},
Abstract = {In this study we explore how different levels of emotional intensity
   (Arousal) and pleasantness (Valence) are reflected in
   electroencephalographic (EEG) signals. We performed the experiments on
   EEG data of 32 subjects from the DEAP public dataset, where the subjects
   were stimulated using 60-s videos to elicitate different levels of
   Arousal/Valence and then self-reported the rating from 1 to 9 using the
   self-assessment Manikin (SAM). The EEG data was pre-processed and used
   as input to a convolutional neural network (CNN). First, the 32 EEG
   channels were used to compute the maximum accuracy level obtainable for
   each subject as well as for creating a single model using data from all
   the subjects. The experiment was repeated using one channel at a time,
   to see if specific channels contain more information to discriminate
   between low vs high arousal/valence. The results indicate than using one
   channel the accuracy is lower compared to using all the 32 channels. An
   optimization process for EEG channel selection is then designed with the
   Non-dominated Sorting Genetic Algorithm II (NSGA-II) with the objective
   to obtain optimal channel combinations with high accuracy recognition.
   The genetic algorithm evaluates all possible combinations using a
   chromosome representation for all the 32 channels, and the EEG data from
   each chromosome in the different populations are tested iteratively
   solving two unconstrained objectives; to maximize classification
   accuracy and to reduce the number of required EEG channels for the
   classification process. Best combinations obtained from a Pareto-front
   suggests that as few as 8-10 channels can fulfill this condition and
   provide the basis for a lighter design of EEG systems for emotion
   recognition. In the best case, the results show accuracies of up to 1.00
   for low vs high arousal using eight EEG channels, and 1.00 for low vs
   high valence using only two EEG channels. These results are encouraging
   for research and healthcare applications that will require automatic
   emotion recognition with wearable EEG.},
Publisher = {NATURE PORTFOLIO},
Address = {HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY},
Type = {Article},
Language = {English},
Affiliation = {Moctezuma, LA (Corresponding Author), Norwegian Univ Sci \& Technol, Dept Engn Cybernet, N-7491 Trondheim, Norway.
   Moctezuma, Luis Alfredo; Molinas, Marta, Norwegian Univ Sci \& Technol, Dept Engn Cybernet, N-7491 Trondheim, Norway.
   Abe, Takashi; Molinas, Marta, Univ Tsukuba, Int Inst Integrat Sleep Med WPI IIIS, Tsukuba, Ibaraki, Japan.},
DOI = {10.1038/s41598-022-07517-5},
Article-Number = {3523},
ISSN = {2045-2322},
Keywords-Plus = {RECOGNITION; UNPLEASANT; PLEASANT; CHILDREN},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {luisalfredomoctezuma@gmail.com},
ResearcherID-Numbers = {Moctezuma, Luis Alfredo/A-7857-2019},
ORCID-Numbers = {Moctezuma, Luis Alfredo/0000-0002-6632-8784},
Funding-Acknowledgement = {UiT The Arctic University of Norway},
Funding-Text = {Open Access funding provided by UiT The Arctic University of Norway.},
Number-of-Cited-References = {73},
Times-Cited = {1},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {8},
Journal-ISO = {Sci Rep},
Doc-Delivery-Number = {3R1ZI},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000838718500042},
OA = {gold, Green Published},
DA = {2022-09-28},
}

@Article{WOS:000562099400019,
  author                     = {Aslam, Abdul Rehman and Bin Altaf, Muhammad Awais},
  journal                    = {IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS},
  title                      = {An On-Chip Processor for Chronic Neurological Disorders Assistance Using Negative Affectivity Classification},
  year                       = {2020},
  issn                       = {1932-4545},
  month                      = {AUG},
  note                       = {IEEE Biomedical Circuits and Systems Conference (BioCAS), Nara, JAPAN, OCT 17-19, 2019},
  number                     = {4},
  pages                      = {838-851},
  volume                     = {14},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Chronic neurological disorders (CND's) are lifelong diseases and cannot
   be eradicated, but their severe effects can be alleviated by early
   preemptive measures. CND's, such as Alzheimer's, Autism Spectrum
   Disorder (ASD), and Amyotrophic Lateral Sclerosis (ALS), are the chronic
   ailment of the central nervous system that causes the degradation of
   emotional and cognitive abilities. Long term continuous monitoring with
   neuro-feedback of human emotions for patients with CND's is crucial in
   mitigating its harmful effect. This paper presents hardware efficient
   and dedicated human emotion classification processor for CND's. Scalp
   EEG is used for the emotion's classification using the valence and
   arousal scales. A linear support vector machine classifier is used with
   power spectral density, logarithmic interhemispheric power spectral
   ratio, and the interhemispheric power spectral difference of eight EEG
   channel locations suitable for a wearable non-invasive classification
   system. A look-up-table based logarithmic division unit (LDU) is to
   represent the division features in machine learning (ML) applications.
   The implemented LDU minimizes the cost of integer division by 34\% for
   ML applications. The implemented emotion's classification processor
   achieved an accuracy of 72.96\% and 73.14\%, respectively, for the
   valence and arousal classification on multiple publicly available
   datasets. The 2 x 3mm(2) processor is fabricated using a 0.18 mu m 1P6M
   CMOS process with power and energy utilization of 2.04 mW and 16 mu
   J/classification, respectively, for 8-channel operation.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Bin Altaf, MA (Corresponding Author), Lahore Univ Management Sci, Elect Engn Dept, Lahore 54792, Pakistan. Aslam, Abdul Rehman; Bin Altaf, Muhammad Awais, Lahore Univ Management Sci, Elect Engn Dept, Lahore 54792, Pakistan.},
  author-email               = {17060056@lums.edu.pk awais.altaf@lums.edu.pk},
  da                         = {2022-09-28},
  doc-delivery-number        = {ND7RC},
  doi                        = {10.1109/TBCAS.2020.3008766},
  eissn                      = {1940-9990},
  funding-acknowledgement    = {Higher Education Commission (HEC), Pakistan {[}7978/Punjab/NRPU/RD/HEC/2017]},
  funding-text               = {This work was funded by the Higher Education Commission (HEC), Pakistan under Grant 7978/Punjab/NRPU/R\&D/HEC/2017.},
  journal-iso                = {IEEE Trans. Biomed. Circuits Syst.},
  keywords                   = {Continuous health monitoring; classification processor; electroencephalogram (EEG); emotion detection; machine learning; neurological disorder; support vector machine},
  keywords-plus              = {EMOTION RECOGNITION; CHILDREN; EEG},
  language                   = {English},
  number-of-cited-references = {61},
  orcid-numbers              = {ASLAM, ABDUL REHMAN/0000-0001-7767-2575 Altaf, Muhammad Awais Bin/0000-0003-3615-3546},
  organization               = {IEEE; IEEE Circuits \& Syst Soc; IEEE Engn Med \& Biol Soc; IEEE Solid State Circuits Soc; Tateisi Sci \& Technol Fdn; Nanolux; Nidek Co Ltd; Hisol; Keysight Technologies; Horiba Adv Techno; Maxwell Biosystems; Sysmex; Kyocera; Shimadzu; Santen; Omron; ThorLabs; Hamamatsu; Mdpi, Micromachines},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Engineering},
  researcherid-numbers       = {ASLAM, ABDUL REHMAN/AAH-6364-2021},
  times-cited                = {21},
  type                       = {Article; Proceedings Paper},
  unique-id                  = {WOS:000562099400019},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {7},
  web-of-science-categories  = {Engineering, Biomedical; Engineering, Electrical \& Electronic},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{ WOS:000524664400038,
Author = {Zheng, Chunjun and Jia, Ning and Sun, Wei},
Book-Group-Author = {IEEE},
Title = {The Extraction Method of Emotional Feature Based on Children's Spoken
   Speech},
Booktitle = {2019 11TH INTERNATIONAL CONFERENCE ON INTELLIGENT HUMAN-MACHINE SYSTEMS
   AND CYBERNETICS (IHMSC 2019), VOL 1},
Series = {International Conference on Intelligent Human-Machine Systems and
   Cybernetics},
Year = {2019},
Pages = {165-168},
Note = {11th International Conference on Intelligent Human-Machine Systems and
   Cybernetics (IHMSC), Zhejiang Univ, Hangzhou, PEOPLES R CHINA, AUG
   24-25, 2019},
Organization = {IEEE Comp Soc; Univ Bristol; Japan Adv Inst Sci \& Technol; IEEE CIS
   Nanjing Chapter; Beihang Univ; IEEE},
Abstract = {Most modern people ignore the importance of reading aloud. However, for
   children aged 5-12, reading aloud is not only an essential skill in the
   learning process, but also an effective means of cultivating sentiment.
   Because there is a nonlinear relationship between the characteristics of
   the spoken speech signal and the evaluation criteria, the emotional
   features suitable for children's reading evaluation are extracted from
   the audio signal, which is very important for the recognition of
   children's reading emotions. However, automatically recognizing emotions
   from speech is a challenging task, and its recognition depends on the
   validity of the speech emotion features and the accuracy of the model.
   In this research, we start with traditional Low Level Descriptors (LLD)
   to learn emotion-related features automatically which were found in
   speech, using High Level Statistics Functions (HSF), and emotion-related
   Short time frame level acoustic features can be learned. These features
   are appropriately aggregated into a compact feature representation in
   conjunction with a spectrogram to form a set of features that
   effectively characterize the emotion signal. The proposed solution is
   evaluated on the children's emotional reading speech library and shows
   more accurate predictions than existing emotion recognition algorithms.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zheng, CJ (Corresponding Author), Dalian Maritime Univ, Informat Sci \& Technol Coll, Dalian, Liaoning, Peoples R China.
   Zheng, CJ (Corresponding Author), Dalian Neusoft Univ Informat, Sch Comp \& Software, Dalian, Liaoning, Peoples R China.
   Zheng, Chunjun, Dalian Maritime Univ, Informat Sci \& Technol Coll, Dalian, Liaoning, Peoples R China.
   Zheng, Chunjun; Jia, Ning; Sun, Wei, Dalian Neusoft Univ Informat, Sch Comp \& Software, Dalian, Liaoning, Peoples R China.},
DOI = {10.1109/IHMSC.2019.00046},
ISSN = {2157-8982},
ISBN = {978-1-7281-1859-8},
Keywords = {Feature extraction; spectral map; low-level descriptor; emotion
   recognition},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Cybernetics},
Author-Email = {zhengchunjun@neusoft.edu.cn
   jianing@neusoft.edu.cn
   sunwei@neusoft.edu.cn},
ResearcherID-Numbers = {jia, ning/ABB-9496-2021},
Funding-Acknowledgement = {Dalian Key Laboratory for the Application of Big Data and Data Science;
   Natural Science Foundation of China {[}61602075]; ``Research on Key
   Technologies of Fast Association Analysis for Big Data{''}},
Funding-Text = {This paper is funded by the ``Dalian Key Laboratory for the Application
   of Big Data and Data Science{''} and the Natural Science Foundation of
   China (61602075), ``Research on Key Technologies of Fast Association
   Analysis for Big Data{''}.},
Number-of-Cited-References = {11},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BO7KC},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000524664400038},
DA = {2022-09-28},
}

@Article{WOS:000455813300049,
  author                     = {McGinnis, Ryan S. and McGinnis, Ellen W. and Hruschak, Jessica and Lopez-Duran, Nestor L. and Fitzgerald, Kate and Rosenblum, Katherine L. and Muzik, Maria},
  journal                    = {PLOS ONE},
  title                      = {Rapid detection of internalizing diagnosis in young children enabled by wearable sensors and machine learning},
  year                       = {2019},
  issn                       = {1932-6203},
  month                      = {JAN 16},
  number                     = {1},
  volume                     = {14},
  abstract                   = {There is a critical need for fast, inexpensive, objective, and accurate
   screening tools for childhood psychopathology. Perhaps most compelling
   is in the case of internalizing disorders, like anxiety and depression,
   where unobservable symptoms cause children to go unassessed-suffering in
   silence because they never exhibiting the disruptive behaviors that
   would lead to a referral for diagnostic assessment. If left untreated
   these disorders are associated with long-term negative outcomes
   including substance abuse and increased risk for suicide. This paper
   presents a new approach for identifying children with internalizing
   disorders using an instrumented 90-second mood induction task.
   Participant motion during the task is monitored using a commercially
   available wearable sensor. We show that machine learning can be used to
   differentiate children with an internalizing diagnosis from controls
   with 81\% accuracy (67\% sensitivity, 88\% specificity). We provide a
   detailed description of the modeling methodology used to arrive at these
   results and explore further the predictive ability of each temporal
   phase of the mood induction task. Kinematical measures most
   discriminative of internalizing diagnosis are analyzed in detail,
   showing affected children exhibit significantly more avoidance of
   ambiguous threat. Performance of the proposed approach is compared to
   clinical thresholds on parent-reported child symptoms which
   differentiate children with an internalizing diagnosis from controls
   with slightly lower accuracy (.68-. 75 vs. .81), slightly higher
   specificity (.88-1.00 vs. .88), and lower sensitivity (.00-. 42 vs. .67)
   than the proposed, instrumented method. These results point toward the
   future use of this approach for screening children for internalizing
   disorders so that interventions can be deployed when they have the
   highest chance for long-term success.},
  address                    = {1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA},
  affiliation                = {McGinnis, EW (Corresponding Author), Univ Vermont, Dept Psychiat, Burlington, VT 05405 USA. McGinnis, EW (Corresponding Author), Univ Michigan, Dept Psychol, Ann Arbor, MI 48109 USA. McGinnis, Ryan S., Univ Vermont, Dept Elect \& Biomed Engn, Burlington, VT USA. McGinnis, Ellen W., Univ Vermont, Dept Psychiat, Burlington, VT 05405 USA. McGinnis, Ellen W.; Lopez-Duran, Nestor L., Univ Michigan, Dept Psychol, Ann Arbor, MI 48109 USA. Hruschak, Jessica; Fitzgerald, Kate; Rosenblum, Katherine L.; Muzik, Maria, Univ Michigan, Dept Psychiat, Ann Arbor, MI 48109 USA.},
  article-number             = {e0210267},
  author-email               = {ryan.mcginnis@uvm.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {HH6BK},
  doi                        = {10.1371/journal.pone.0210267},
  funding-acknowledgement    = {Michigan Institute for Clinical and Health Research {[}UL1TR000433]; Blue Cross Blue Shield of Michigan Foundation {[}1982.SAP]; Brain Behavior Research Foundation; National Institute of Mental Health {[}R03MH102648]; NATIONAL CENTER FOR ADVANCING TRANSLATIONAL SCIENCES {[}UL1TR002240, UL1TR000433] Funding Source: NIH RePORTER},
  funding-text               = {This work received support from the following sources: Michigan Institute for Clinical and Health Research (UL1TR000433, PI: Muzik; Biomedical and Social Sciences Scholar Program, University of Michigan PI: E.W. McGinnis); Blue Cross Blue Shield of Michigan Foundation Grant (1982.SAP, PI: E.W. McGinnis); Brain Behavior Research Foundation (PI: Fitzgerald); National Institute of Mental Health R03MH102648 (PI: Fitzgerald; Rosenblum). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.},
  journal-iso                = {PLoS One},
  keywords-plus              = {PRESCHOOL DEPRESSION; BEHAVIOR CHECKLIST; ANXIETY DISORDERS; EMOTION REGULATION; EARLY-CHILDHOOD; MENTAL-HEALTH; AGES 3; PARENT; PSYCHOPATHOLOGY; ADOLESCENCE},
  language                   = {English},
  number-of-cited-references = {73},
  oa                         = {Green Published, gold, Green Submitted},
  orcid-numbers              = {Lopez-Duran, Nestor/0000-0003-0666-496X McGinnis, Ryan/0000-0001-8396-6967},
  publisher                  = {PUBLIC LIBRARY SCIENCE},
  research-areas             = {Science \& Technology - Other Topics},
  researcherid-numbers       = {Rosenblum, Katherine/AAC-5921-2019 Fitzgerald, Kate D./AAV-7278-2020},
  times-cited                = {20},
  type                       = {Article},
  unique-id                  = {WOS:000455813300049},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {10},
  web-of-science-categories  = {Multidisciplinary Sciences},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@article{ WOS:000475304200070,
Author = {Gao, Zhilin and Cui, Xingran and Wan, Wang and Gu, Zhongze},
Title = {Recognition of Emotional States Using Multiscale Information Analysis of
   High Frequency EEG Oscillations},
Journal = {ENTROPY},
Year = {2019},
Volume = {21},
Number = {6},
Month = {JUN},
Abstract = {Exploring the manifestation of emotion in electroencephalogram (EEG)
   signals is helpful for improving the accuracy of emotion recognition.
   This paper introduced the novel features based on the multiscale
   information analysis (MIA) of EEG signals for distinguishing emotional
   states in four dimensions based on Russell's circumplex model. The
   algorithms were applied to extract features on the DEAP database, which
   included multiscale EEG complexity index in the time domain, and
   ensemble empirical mode decomposition enhanced energy and fuzzy entropy
   in the frequency domain. The support vector machine and cross validation
   method were applied to assess classification accuracy. The
   classification performance of MIA methods (accuracy = 62.01\%, precision
   = 62.03\%, recall/sensitivity = 60.51\%, and specificity = 82.80\%) was
   much higher than classical methods (accuracy = 43.98\%, precision =
   43.81\%, recall/sensitivity = 41.86\%, and specificity = 70.50\%), which
   extracted features contain similar energy based on a discrete wavelet
   transform, fractal dimension, and sample entropy. In this study, we
   found that emotion recognition is more associated with high frequency
   oscillations (51-100Hz) of EEG signals rather than low frequency
   oscillations (0.3-49Hz), and the significance of the frontal and
   temporal regions are higher than other regions. Such information has
   predictive power and may provide more insights into analyzing the
   multiscale information of high frequency oscillations in EEG signals.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Cui, XR; Gu, ZZ (Corresponding Author), Southeast Univ, Sch Biol Sci \& Med Engn, Key Lab Child Dev \& Learning Sci, Minist Educ, Nanjing 210000, Jiangsu, Peoples R China.
   Cui, XR (Corresponding Author), Southeast Univ, Inst Biomed Devices Suzhou, Suzhou 215000, Peoples R China.
   Gao, Zhilin; Cui, Xingran; Wan, Wang; Gu, Zhongze, Southeast Univ, Sch Biol Sci \& Med Engn, Key Lab Child Dev \& Learning Sci, Minist Educ, Nanjing 210000, Jiangsu, Peoples R China.
   Cui, Xingran, Southeast Univ, Inst Biomed Devices Suzhou, Suzhou 215000, Peoples R China.},
DOI = {10.3390/e21060609},
Article-Number = {609},
EISSN = {1099-4300},
Keywords = {emotion recognition; EEG; multiscale information analysis; multiscale
   sample entropy; ensemble empirical mode decomposition; fuzzy entropy;
   support vector machine},
Keywords-Plus = {EMPIRICAL MODE DECOMPOSITION; OCULAR ARTIFACTS; ENTROPY ANALYSIS;
   COMPLEXITY; IDENTIFICATION; EXTRACTION},
Research-Areas = {Physics},
Web-of-Science-Categories  = {Physics, Multidisciplinary},
Author-Email = {gaozhilin\_seu@163.com
   cuixr@seu.edu.cn
   wanwang1996@163.com
   gu@seu.edu.cn},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}61807007]; National Key
   Research and Development Program of China {[}2018YFC2001100];
   Fundamental Research Funds for the Central Universities of China
   {[}2242018K40050, 2242019K40042]},
Funding-Text = {The National Natural Science Foundation of China, grant number 61807007,
   National Key Research and Development Program of China, grant number
   2018YFC2001100, Fundamental Research Funds for the Central Universities
   of China, grant number 2242018K40050, 2242019K40042, funded this
   research.},
Number-of-Cited-References = {55},
Times-Cited = {16},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {21},
Journal-ISO = {Entropy},
Doc-Delivery-Number = {II6LA},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000475304200070},
OA = {Green Published, gold},
DA = {2022-09-28},
}

@Article{WOS:000798019900036,
  author                     = {Padminivalli, V, S. J. R. K. and Rao, M. V. P. Chandra Sekhara},
  journal                    = {INTERNATIONAL JOURNAL OF EARLY CHILDHOOD SPECIAL EDUCATION},
  title                      = {A Review on Text Mining Techniques for the Prediction of Psychological Behaviour using Social Media},
  year                       = {2022},
  issn                       = {1308-5581},
  number                     = {3},
  pages                      = {2462-2467},
  volume                     = {14},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Text mining (TM)or text data mining mainly focuses on analyzing
   different types of data that is available in various social networking
   sites and also from other types. Social media reduced the gaps between
   individuals. Prior to its advent, it was not only difficult to
   communicate but also was time taking process. Social media service is
   the fast and inexpensive means to communicate with the people. Using
   text mining algorithms many applications such as emotion recognition,
   sentiment analysis, users behavior analysis, stress based analysis and
   psychological analysis can be developed. To improve the analysis of data
   several machine learning (ML) algorithms can be adopted to overcome the
   issues in text mining process. This paper discusses and analyzes various
   text mining algorithms, text analysis and their performances.},
  address                    = {INST FINE ARTS, ESKISEHIR, 26470, TURKEY},
  affiliation                = {Padminivalli, VSJRK (Corresponding Author), Acharya Nagarjuna Univ, Dept Comp Sci \& Engn, Dr YSR Anu Coll Engn \& Technol, Guntur, Andhra Pradesh, India. Padminivalli, S. J. R. K., V, Acharya Nagarjuna Univ, Dept Comp Sci \& Engn, Dr YSR Anu Coll Engn \& Technol, Guntur, Andhra Pradesh, India. Rao, M. V. P. Chandra Sekhara, RVR \& JC Coll Engn, Dept CSBS, Guntur, Andhra Pradesh, India.},
  author-email               = {srivallivasantham@gmail.com manukondach@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {1J6HZ},
  doi                        = {10.9756/INT-JECSE/V14I3.293},
  journal-iso                = {Int. J. Early Child. Spec. Educ.},
  keywords                   = {text mining; machine learning; sentiment analysis},
  language                   = {English},
  number-of-cited-references = {32},
  orcid-numbers              = {PADMINIVALLI, S J R K/0000-0001-8338-6268 Rao, M.V.P. Chandra Sekhara/0000-0002-6676-0454},
  publisher                  = {ANADOLU UNIV},
  research-areas             = {Education \& Educational Research},
  researcherid-numbers       = {PADMINIVALLI, S J R K/ADK-8684-2022 Rao, M.V.P. Chandra Sekhara/ABH-6843-2020},
  times-cited                = {0},
  type                       = {Review},
  unique-id                  = {WOS:000798019900036},
  usage-count-last-180-days  = {7},
  usage-count-since-2013     = {7},
  web-of-science-categories  = {Education, Special},
  web-of-science-index       = {Emerging Sources Citation Index (ESCI)},
}

@InProceedings{WOS:000652198600074,
  author                     = {Ul Haque, Md Inzamam and Valles, Damian},
  booktitle                  = {2019 IEEE 10TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS \& MOBILE COMMUNICATION CONFERENCE (UEMCON)},
  title                      = {Facial Expression Recognition Using DCNN and Development of an iOS App for Children with ASD to Enhance Communication Abilities},
  year                       = {2019},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  editor                     = {Chakrabarti, S and Saha, HN},
  note                       = {IEEE 10th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference (UEMCON), Columbia Univ, New York, NY, OCT 10-12, 2019},
  organization               = {IEEE; IEEE New York Sect; IEEE R1 Reg; IEEE USA; Inst Engn \& Management; Univ Engn \& Management},
  pages                      = {476-482},
  publisher                  = {IEEE},
  status                     = {Aceito},
  abstract                   = {In this paper, continued work of a research project is discussed which
   achieved the end goal of the project - to build a mobile device
   application that can teach children with Autism Spectrum Disorder (ASD)
   to recognize human facial expressions utilizing computer vision and
   image processing. Universally, there are seven facial expressions
   categories: angry, disgust, happy, sad, fear, surprise, and neutral. To
   recognize all these facial expressions and to predict the current mood
   of a person is a difficult task for a child. A child with ASD, this
   problem presents itself in a more sophisticated manner due to the nature
   of the disorder. The main goal of this research was to develop a deep
   Convolutional Neural Network (DCNN) for facial expression recognition,
   which can help young children with ASD to recognize facial expressions,
   using mobile devices. The Kaggle's FER2013 and Karolinska Directed
   Emotional Faces (KDEF) dataset have been used to train and test with the
   DCNN model, which can classify facial expressions from different
   viewpoints and in different lighting contrasts. An 86.44\% accuracy was
   achieved with good generalizability for the DCNN model. The results show
   an improvement of the DCNN accuracy in dealing with lighting contrast
   changes, and the implementation of image processing before performing
   the facial expression classification. As a byproduct of this research
   project, an app suitable for the iOS platform was developed for running
   both the DCNN model and image processing algorithm. The app can be used
   by speechlanguage pathologies, teacher, care-takers, and parents as a
   technological tool when working with children with ASD.},
  affiliation                = {Ul Haque, MI (Corresponding Author), Texas State Univ, Ingram Sch Engn, San Marcos, TX 78666 USA. Ul Haque, Md Inzamam; Valles, Damian, Texas State Univ, Ingram Sch Engn, San Marcos, TX 78666 USA.},
  author-email               = {m\_h536@txstate.edu dvalles@txstate.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {BR4QI},
  isbn                       = {978-1-7281-3885-5},
  keywords                   = {Facial Expression Recognition; Autism; Deep Learning; Convolutional Neural Network; iOS App},
  keywords-plus              = {EMOTION-RECOGNITION; SOCIAL-ADJUSTMENT},
  language                   = {English},
  number-of-cited-references = {22},
  orcid-numbers              = {Valles, Damian/0000-0002-5684-9498},
  priority                   = {prio2},
  research-areas             = {Computer Science; Engineering; Telecommunications},
  researcherid-numbers       = {Valles, Damian/AAV-2603-2021},
  times-cited                = {5},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000652198600074},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Computer Science, Information Systems; Engineering, Electrical \& Electronic; Telecommunications},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000457525200014,
  author                     = {Khan, Rizwan Ahmed and Meyer, Alexandre and Konik, Hubert and Bouakaz, Saida},
  journal                    = {FRONTIERS OF COMPUTER SCIENCE},
  title                      = {Saliency-based framework for facial expression recognition},
  year                       = {2019},
  issn                       = {2095-2228},
  month                      = {FEB},
  number                     = {1},
  pages                      = {183-198},
  volume                     = {13},
  abstract                   = {This article proposes a novel framework for the recognition of six
   universal facial expressions. The framework is based on three sets of
   features extracted from a face image: entropy, brightness, and local
   binary pattern. First, saliency maps are obtained using the
   state-of-the-art saliency detection algorithm frequency-tuned salient
   region detection. The idea is to use saliency maps to determine
   appropriate weights or values for the extracted features (i.e.,
   brightness and entropy). We have performed a visual experiment to
   validate the performance of the saliency detection algorithm against the
   human visual system. Eye movements of 15 subjects were recorded using an
   eye-tracker in free-viewing conditions while they watched a collection
   of 54 videos selected from the Cohn-Kanade facial expression database.
   The results of the visual experiment demonstrated that the obtained
   saliency maps are consistent with the data on human fixations. Finally,
   the performance of the proposed framework is demonstrated via
   satisfactory classification results achieved with the Cohn-Kanade
   database, FG-NET FEED database, and Dartmouth database of children's
   faces.},
  address                    = {CHAOYANG DIST, 4, HUIXINDONGJIE, FUSHENG BLDG, BEIJING 100029, PEOPLES R CHINA},
  affiliation                = {Khan, RA; Meyer, A (Corresponding Author), Univ Lyon 1, Univ Lyon, CNRS, UMR5205,LIRIS, F-69622 Villeurbanne, France. Khan, RA (Corresponding Author), Barrett Hodgson Univ, Fac Informat Technol, Karachi 74900, Pakistan. Khan, Rizwan Ahmed; Meyer, Alexandre; Bouakaz, Saida, Univ Lyon 1, Univ Lyon, CNRS, UMR5205,LIRIS, F-69622 Villeurbanne, France. Khan, Rizwan Ahmed, Barrett Hodgson Univ, Fac Informat Technol, Karachi 74900, Pakistan. Konik, Hubert, Univ Jean Monnet, UMR5516, Lab Hubert Curien, F-42000 St Etienne, France.},
  author-email               = {Rizwan-Ahmed.Khan@liris.cnrs.fr Alexandre.Meyer@liris.cnrs.fr},
  da                         = {2022-09-28},
  doc-delivery-number        = {HJ9ND},
  doi                        = {10.1007/s11704-017-6114-9},
  eissn                      = {2095-2236},
  journal-iso                = {Front.. Comput. Sci.},
  keywords                   = {facial expression recognition; classification; salient regions; entropy; brightness; local binary pattern},
  keywords-plus              = {TEXTURE CLASSIFICATION; INFORMATION FUSION; EMOTION; FACES; MODEL},
  language                   = {English},
  number-of-cited-references = {64},
  oa                         = {Green Submitted},
  orcid-numbers              = {Khan, PhD, Rizwan Ahmed/0000-0003-0819-800X MEYER, Alexandre/0000-0002-0249-1048},
  publisher                  = {HIGHER EDUCATION PRESS},
  research-areas             = {Computer Science},
  researcherid-numbers       = {Khan, PhD, Rizwan Ahmed/N-7134-2018},
  times-cited                = {10},
  type                       = {Article},
  unique-id                  = {WOS:000457525200014},
  usage-count-last-180-days  = {5},
  usage-count-since-2013     = {23},
  web-of-science-categories  = {Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory \& Methods},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000821586800009,
  author                     = {Wan, Guobin and Deng, Fuhao and Jiang, Zijian and Song, Sifan and Hu, Di and Chen, Lifu and Wang, Haibo and Li, Miaochun and Chen, Gong and Yan, Ting and Su, Jionglong and Zhang, Jiaming},
  journal                    = {COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE},
  title                      = {FECTS: A Facial Emotion Cognition and Training System for Chinese Children with Autism Spectrum Disorder},
  year                       = {2022},
  issn                       = {1687-5265},
  month                      = {APR 27},
  volume                     = {2022},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Traditional training methods such as card teaching, assistive
   technologies (e.g., augmented reality/virtual reality games and
   smartphone apps), DVDs, human-computer interactions, and human-robot
   interactions are widely applied in autistic rehabilitation training in
   recent years. In this article, we propose a novel framework for
   human-computer/robot interaction and introduce a preliminary
   intervention study for improving the emotion recognition of Chinese
   children with an autism spectrum disorder. The core of the framework is
   the Facial Emotion Cognition and Training System (FECTS, including six
   tasks to train children with ASD to match, infer, and imitate the facial
   expressions of happiness, sadness, fear, and anger) based on Simon
   Baron-Cohen's E-S (empathizing-systemizing) theory. Our system may be
   implemented on PCs, smartphones, mobile devices such as PADs, and
   robots. The training record (e.g., a tracked record of emotion
   imitation) of the Chinese autistic children interacting with the device
   implemented using our FECTS will be uploaded and stored in the database
   of a cloud-based evaluation system. Therapists and parents can access
   the analysis of the emotion learning progress of these autistic children
   using the cloud-based evaluation system. Deep-learning algorithms of
   facial expressions recognition and attention analysis will be deployed
   in the back end (e.g., devices such as a PC, a robotic system, or a
   cloud system) implementing our FECTS, which can perform real-time
   tracking of the imitation quality and attention of the autistic children
   during the expression imitation phase. In this preliminary clinical
   study, a total of 10 Chinese autistic children aged 3-8 are recruited,
   and each of them received a single 20-minute training session every day
   for four consecutive days. Our preliminary results validated the
   feasibility of the developed FECTS and the effectiveness of our
   algorithms based on Chinese children with an autism spectrum disorder.
   To verify that our FECTS can be further adapted to children from other
   countries, children with different cultural/sociological/linguistic
   contexts should be recruited in future studies.},
  address                    = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
  affiliation                = {Zhang, JM (Corresponding Author), Shenzhen Inst Artificial Intelligence \& Robot Soc, Shenzhen 518172, Peoples R China. Yan, T (Corresponding Author), Chinese Acad Sci, Brain Cognit \& Brain Dis Inst,CAS Key Lab Brain C, Shenzhen Inst Adv Technol,Guangdong Prov Key Lab, Shenzhen Hong Kong Inst Brain Sci,Shenzhen Key La, Shenzhen 518055, Guangdong, Peoples R China. Su, JL (Corresponding Author), Xian Jiaotong Liverpool Univ, XJTLU Entrepreneur Coll Taicang, Sch AI \& Adv Comp, Suzhou 215123, Jiangsu, Peoples R China. Zhang, JM (Corresponding Author), Chinese Univ Hong Kong Shenzhen, Inst Robot \& Intelligent Mfg, Shenzhen 518172, Peoples R China. Wan, Guobin, Shenzhen Maternal \& Child Hlth Hosp, Shenzhen 518000, Peoples R China. Deng, Fuhao; Jiang, Zijian; Zhang, Jiaming, Shenzhen Inst Artificial Intelligence \& Robot Soc, Shenzhen 518172, Peoples R China. Song, Sifan, Xian Jiaotong Liverpool Univ, Dept Math Sci, Suzhou 215123, Peoples R China. Hu, Di, Univ Maryland, Robert H Smith Sch Business, College Pk, MA USA. Chen, Lifu, DoGoodly Int Educ Ctr Shenzhen Co Ltd, Shenzhen 518219, Peoples R China. Chen, Lifu, Smart Children Educ Ctr, Shenzhen 518219, Peoples R China. Wang, Haibo, China Univ Min \& Technol, Sch Informat \& Control Engn, Xuzhou 221116, Jiangsu, Peoples R China. Li, Miaochun, Guangdong Pharmaceut Univ, Dept Informat Management \& Informat Syst, Zhongshan 511436, Peoples R China. Chen, Gong, Sunwoda Elect Co Ltd, Shiyan St, Shenzhen 518000, Guangdong, Peoples R China. Yan, Ting, Chinese Acad Sci, Brain Cognit \& Brain Dis Inst,CAS Key Lab Brain C, Shenzhen Inst Adv Technol,Guangdong Prov Key Lab, Shenzhen Hong Kong Inst Brain Sci,Shenzhen Key La, Shenzhen 518055, Guangdong, Peoples R China. Su, Jionglong, Xian Jiaotong Liverpool Univ, XJTLU Entrepreneur Coll Taicang, Sch AI \& Adv Comp, Suzhou 215123, Jiangsu, Peoples R China. Zhang, Jiaming, Chinese Univ Hong Kong Shenzhen, Inst Robot \& Intelligent Mfg, Shenzhen 518172, Peoples R China.},
  article-number             = {9213526},
  author-email               = {ting.yan@siat.ac.cn jionglong.su@xjtlu.edu.cn zhangjiaming@cuhk.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {2S1VE},
  doi                        = {10.1155/2022/9213526},
  eissn                      = {1687-5273},
  funding-acknowledgement    = {Shenzhen Science and Technology Innovation Commission {[}JCYJ20180508152240368]; National Natural Science Foundation of China {[}31800900]; Key Realm R\&D Program of Guangdong Province {[}2019B030335001]; Sanming Project of Medicine in Shenzhen {[}SZSM201512009]; Open Program of Neusoft Corporation {[}SKLSAOP1702]},
  funding-text               = {THe authors would like to thank Dr. Guobin Wan and his colleagues in Shenzhen Maternal and Child Health Hospital and Mr. Lifu Chen and his colleagues in DoGoodly International Education Center (Shenzhen) Co. Ltd., and Smart Children Education Center (Shenzhen), for giving many constructive suggestions in designing the Facial Emotion Cognition and Training System. 1is work was supported by the Shenzhen Science and Technology Innovation Commission (Grant no. JCYJ20180508152240368). 1is work was also supported by the National Natural Science Foundation of China (Grant no. 31800900), the Key Realm R\&D Program of Guangdong Province (2019B030335001), the Sanming Project of Medicine in Shenzhen (SZSM201512009), and the Open Program of Neusoft Corporation (item number SKLSAOP1702).},
  journal-iso                = {Comput. Intell. Neurosci.},
  language                   = {English},
  number-of-cited-references = {45},
  oa                         = {gold, Green Published},
  orcid-numbers              = {Li, Miaochun/0000-0001-5604-7738 Song, Sifan/0000-0002-7940-650X Jiang, Zijian/0000-0002-0537-8817 Hu, Di/0000-0002-2842-1478},
  publisher                  = {HINDAWI LTD},
  research-areas             = {Mathematical \& Computational Biology; Neurosciences \& Neurology},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000821586800009},
  usage-count-last-180-days  = {7},
  usage-count-since-2013     = {7},
  web-of-science-categories  = {Mathematical \& Computational Biology; Neurosciences},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000829543400008,
  author                     = {Torres, Juan Manuel Mayor and Clarkson, Tessa and Hauschild, Kathryn M. and Luhmann, Christian C. and Lerner, Matthew D. and Riccardi, Giuseppe},
  journal                    = {BIOLOGICAL PSYCHIATRY-COGNITIVE NEUROSCIENCE AND NEUROIMAGING},
  title                      = {Facial Emotions Are Accurately Encoded in the Neural Signal of Those With Autism Spectrum Disorder: A Deep Learning Approach},
  year                       = {2022},
  issn                       = {2451-9022},
  month                      = {JUL},
  number                     = {7},
  pages                      = {688-695},
  volume                     = {7},
  status                     = {Rejeitado - Escopo},
  abstract                   = {BACKGROUND: Individuals with autism spectrum disorder (ASD) exhibit
   frequent behavioral deficits in facial emotion recognition (FER). It
   remains unknown whether these deficits arise because facial emotion
   information is not encoded in their neural signal or because it is
   encodes but fails to translate to FER behavior (deployment). This
   distinction has functional implications, including constraining when
   differences in social information processing occur in ASD, and guiding
   interventions (i.e., developing prosthetic FER vs. reinforcing existing
   skills).METHODS: We utilized a discriminative and contemporary machine
   learning approach-deep convolutional neural networks-to classify facial
   emotions viewed by individuals with and without ASD (N = 88) from
   concurrently recorded electroencephalography signals.RESULTS: The
   convolutional neural network classified facial emotions with high
   accuracy for both ASD and non-ASD groups, even though individuals with
   ASD performed more poorly on the concurrent FER task. In fact,
   convolutional neural network accuracy was greater in the ASD group and
   was not related to behavioral performance. This pattern of results
   replicated across three independent participant samples. Moreover,
   feature importance analyses suggested that a late temporal window of
   neural activity (1000-1500 ms) may be uniquely important in facial
   emotion classification for individuals with ASD. CONCLUSIONS: Our
   results reveal for the first time that facial emotion information is
   encoded in the neural signal of individuals with (and without) ASD.
   Thus, observed difficulties in behavioral FER associated with ASD likely
   arise from difficulties in decoding or deployment of facial emotion
   information within the neural signal. Interventions should focus on
   capitalizing on this intact encoding rather than promoting compensation
   or FER prostheses.},
  address                    = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  affiliation                = {Clarkson, T (Corresponding Author), Temple Univ, Dept Psychol, Philadelphia, PA 19122 USA. Torres, Juan Manuel Mayor; Riccardi, Giuseppe, Univ Trento, Dept Informat Engn \& Comp Sci, Povo, Italy. Clarkson, Tessa, Temple Univ, Dept Psychol, Philadelphia, PA 19122 USA. Hauschild, Kathryn M.; Luhmann, Christian C.; Lerner, Matthew D., SUNY Stony Brook, Dept Psychol, Stony Brook, NY USA. Luhmann, Christian C., SUNY Stony Brook, Inst Adv Computat Sci, Stony Brook, NY USA. Lerner, Matthew D., Univ Virginia, Dept Psychol, Charlottesville, VA USA.},
  author-email               = {tessa.clarkson@temple.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {3D8JW},
  doi                        = {10.1016/j.bpsc.2021.03.015},
  eissn                      = {2451-9030},
  funding-acknowledgement    = {National Institute of Mental Health {[}R01MH110585]; Alan Alda Fund for Communication; American Psychological Association; Association for Psychological Science; American Psychological Foundation; Jefferson Scholars Foundation; International Max Planck Research; National Institute Of Mental Health of the National Institutes of Health {[}F31MH122091]; Temple University Public Policy Lab Graduate Fellowship; American Psychological Association (APA) Dissertation Research Award; National Science Foundation; Dr. Phillip J Bersh Memorial Student Award; {[}1531492]},
  funding-text               = {MDL was supported by the National Institute of Mental Health (Grant No. R01MH110585) , grants from the Alan Alda Fund for Communication, the American Psychological Association, and Association for Psychological Science, as well as Fellowships from the American Psychological Foundation, Jefferson Scholars Foundation, and International Max Planck Research. TC was supported by the National Institute Of Mental Health of the National Institutes of Health under Award Number F31MH122091. The content is solely the responsibility of the authors and does not neces- sarily represent the official views of the National Institutes of Health. TC was also supported by Temple University Public Policy Lab Graduate Fellowship, American Psychological Association (APA) Dissertation Research Award, and the Dr. Phillip J Bersh Memorial Student Award. We would like to thank Stony Brook Research Computing and Cyber-infrastructure, and the Institute for Advanced Computational Science at Stony Brook University for access to the SeaWulf computing system, which was made possible by a \$1.4 M National Science Foundation grant (\#1531492) . Portions of this article were presented at the 2018 International Society for Autism Research Annual Meeting. Data from the primary study can be found at the National Database for Autism Research (NDAR) . Code for the CNN can be found here: https://github.com /meiyor/Deep-Learning-Emotion-Decoding-using-EEG-data-from- Autism-individuals. Data for replication samples can be made available upon request to the corresponding authors or MDL. The authors report no biomedical financial interests or potential conflicts of interest.},
  journal-iso                = {Biol. Psychiat.-Cogn. Neurosci. Neuroimag.},
  keywords-plus              = {HIGH-FUNCTIONING ADULTS; RECOGNITION; CONNECTIONS; MECHANISMS; CHILDREN},
  language                   = {English},
  number-of-cited-references = {52},
  orcid-numbers              = {Luhmann, Christian/0000-0002-9773-1672},
  publisher                  = {ELSEVIER},
  research-areas             = {Neurosciences \& Neurology},
  researcherid-numbers       = {Mayor-Torres, Juan/AAT-8664-2021},
  times-cited                = {5},
  type                       = {Article},
  unique-id                  = {WOS:000829543400008},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {5},
  web-of-science-categories  = {Neurosciences},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@InProceedings{WOS:000695438100048,
  author                     = {Huffaker, Jordan S. and Kummerfeld, Jonathan K. and Lasecki, Walter S. and Ackerman, Mark S.},
  booktitle                  = {PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20)},
  title                      = {Crowdsourced Detection of Emotionally Manipulative Language},
  year                       = {2020},
  address                    = {1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES},
  note                       = {CHI Conference on Human Factors in Computing Systems (CHI), ELECTR NETWORK, APR 25-30, 2020},
  organization               = {Assoc Comp Machinery; ACM SIGCHI},
  publisher                  = {ASSOC COMPUTING MACHINERY},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Detecting rhetoric that manipulates readers' emotions requires
   distinguishing intrinsically emotional content (IEC; e.g., a parent
   losing a child) from emotionally manipulative language (EML; e.g., using
   fear-inducing language to spread anti-vaccine propaganda). However, this
   remains an open classification challenge for both automatic and
   crowdsourcing approaches. Machine Learning approaches only work in
   narrow domains where labeled training data is available, and non-expert
   annotators tend to conflate IEC with EML. We introduce an approach,
   anchor comparison, that leverages workers' ability to identify and
   remove instances of EML in text to create a paraphrased ``anchor
   text{''}, which is then used as a comparison point to classify EML in
   the original content. We evaluate our approach with a dataset of
   news-style text snippets and show that precision and recall can be tuned
   for system builders' needs. Our contribution is a crowdsourcing approach
   that enables non-expert disentanglement of social references from
   content.},
  affiliation                = {Huffaker, JS (Corresponding Author), Univ Michigan, Comp Sci \& Engn, Ann Arbor, MI 48109 USA. Huffaker, Jordan S.; Kummerfeld, Jonathan K.; Lasecki, Walter S.; Ackerman, Mark S., Univ Michigan, Comp Sci \& Engn, Ann Arbor, MI 48109 USA. Lasecki, Walter S.; Ackerman, Mark S., Univ Michigan, Sch Informat, Ann Arbor, MI USA.},
  author-email               = {jhuffak@umich.edu jkummerf@umich.edu wlasecki@umich.edu ackerm@umich.edu},
  book-group-author          = {ACM},
  da                         = {2022-09-28},
  doc-delivery-number        = {BS1SH},
  doi                        = {10.1145/3313831.3376375},
  funding-acknowledgement    = {U.S. NASA {[}NNX16AC66A]; DARPA {[}D19AP00079]},
  funding-text               = {This research was supported in part by the U.S. NASA (NNX16AC66A) andDARPA(D19AP00079) agencies. This article solely refects the opinions and conclusions of its authors and notNASA, DARPA, or anyothergovernment entity. We also thank the members of the SocialWorlds Research Group and Cromalab.},
  isbn                       = {978-1-4503-6708-0},
  keywords                   = {Crowdsourcing; Media Manipulation; Rhetoric; Emotion},
  keywords-plus              = {FEAR; TERRORISM; JUDGMENTS; RISKS; ANGER},
  language                   = {English},
  number-of-cited-references = {88},
  orcid-numbers              = {Huffaker, Jordan/0000-0003-2876-9842},
  research-areas             = {Computer Science},
  times-cited                = {1},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000695438100048},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Computer Science, Cybernetics; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Computer Science, Theory \& Methods},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000473770200001,
  author                     = {Althobaiti, Turke and Katsigiannis, Stamos and West, Daune and Ramzan, Naeem},
  journal                    = {IEEE ACCESS},
  title                      = {Examining Human-Horse Interaction by Means of Affect Recognition via Physiological Signals},
  year                       = {2019},
  issn                       = {2169-3536},
  pages                      = {77857-77867},
  volume                     = {7},
  status                     = {Rejeitado - Escopo e Confuso},
  abstract                   = {For some time, equine-assisted therapy (EAT), i.e., the use of
   horse-related activities for therapeutic reasons, has been recognised as
   a useful approach in the treatment of many mental health issues such as
   post-traumatic stress disorder (PTSD), depression, and anxiety. However,
   despite the interest in EAT, few scientific studies have focused on
   understanding the complex emotional response that horses seem to elicit
   in human riders and handlers. In this work, the potential use of affect
   recognition techniques based on physiological signals is examined for
   the task of assessing the interaction between humans and horses in terms
   of the emotional response of the humans to this interaction.
   Electroencephalography (EEG), electrocardiography (ECG), and
   electromyography (EMG) signals were captured from humans interacting
   with horses, and machine learning techniques were applied in order to
   predict the self-reported emotional states of the human subjects in
   terms of valence and arousal. Supervised classification experiments
   demonstrated the potential of this approach for affect recognition
   during human-horse interaction, reaching an F1-score of 78.27\% for
   valence and 65.49\% for arousal.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Althobaiti, T (Corresponding Author), Univ West Scotland, Sch Comp Engn \& Phys Sci, Paisley PA1 2BE, Renfrew, Scotland. Althobaiti, T (Corresponding Author), Nothern Border Univ, Rafha Community Coll, Rafha, Saudi Arabia. Althobaiti, Turke; Katsigiannis, Stamos; West, Daune; Ramzan, Naeem, Univ West Scotland, Sch Comp Engn \& Phys Sci, Paisley PA1 2BE, Renfrew, Scotland. Althobaiti, Turke, Nothern Border Univ, Rafha Community Coll, Rafha, Saudi Arabia.},
  author-email               = {Turke.Althobaiti@uws.ac.uk},
  da                         = {2022-09-28},
  doc-delivery-number        = {IG4JP},
  doi                        = {10.1109/ACCESS.2019.2922037},
  journal-iso                = {IEEE Access},
  keywords                   = {Affective computing; ECG; EEG; EMG; emotion recognition; equine assisted therapy (EAT); human-horse interaction; physiological signals},
  keywords-plus              = {EQUINE-ASSISTED THERAPY; CHILDREN; EEG; DATABASE},
  language                   = {English},
  number-of-cited-references = {55},
  oa                         = {Green Published, gold, Green Accepted},
  orcid-numbers              = {West, Daune/0000-0002-7245-8825 Ramzan, Naeem/0000-0002-5088-1462},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Computer Science; Engineering; Telecommunications},
  times-cited                = {8},
  type                       = {Article},
  unique-id                  = {WOS:000473770200001},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {16},
  web-of-science-categories  = {Computer Science, Information Systems; Engineering, Electrical \& Electronic; Telecommunications},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{ WOS:000631808500006,
Author = {Foo, Lee Sze and Yap, Wun-She and Hum, Yan Chai and Kadim, Zulaikha and
   Hon, Hock Woon and Tee, Yee Kai},
Book-Group-Author = {IEEE},
Title = {Real-Time Baby Crying Detection in the Noisy Everyday Environment},
Booktitle = {2020 11TH IEEE CONTROL AND SYSTEM GRADUATE RESEARCH COLLOQUIUM (ICSGRC)},
Year = {2020},
Pages = {26-31},
Note = {11th IEEE Control and System Graduate Research Colloquium (ICSGRC), Shah
   Alam, MALAYSIA, AUG 08, 2020},
Organization = {Univ Teknologi Mara; IEEE; IEEE Malaysia Sect Control Syst Soc Chapter;
   Univ Teknologi Mara, Fac Elect Engn; Univ Teknologi Mara, ASPRG},
Abstract = {Baby crying detection is an important component in child monitoring,
   diagnostics, as well as emotion detection systems. This study proposed a
   real-time baby crying detection algorithm that monitors the noisy
   environment for baby crying on a second-by-second basis. The algorithm
   detected baby crying through five acoustic features - average frequency,
   pitch frequency, short-time energy (STE) acceleration, zero-crossing
   rate (ZCR), and Mel-Frequency cepstral coefficients (MFCCs). The
   thresholds for each feature in classifying an audio segment as
   ``crying{''} were set by extracting and examining the distribution of
   the features of noise-free crying and non-crying samples collected from
   an audio database freely available on the Internet. Later, the algorithm
   was tested using noisy crying and non-crying samples downloaded from
   YouTube, where an accuracy of 89.20\% was obtained for the offline
   testing. In order to test the robustness and performance of the designed
   algorithm, online testings were also conducted using three customly
   composed noisy samples containing both crying and non-crying segments.
   The online accuracy obtained was 80.77\%, lower compared to the offline
   testing which was mainly caused by the extra noise introduced by the
   experimental settings. With more advanced equipment, it should be
   possible to increase the online testing to be closer to the offline
   testing accuracy, paving the way to use the designed algorithm for
   reliable real-time second-by-second baby crying detection.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Foo, LS (Corresponding Author), Univ Tunku Abdul Rahman, Petaling Jaya, Malaysia.
   Foo, Lee Sze; Yap, Wun-She; Hum, Yan Chai; Tee, Yee Kai, Univ Tunku Abdul Rahman, Petaling Jaya, Malaysia.
   Kadim, Zulaikha; Hon, Hock Woon, MIMOS Berhad, Kuala Lumpur, Malaysia.},
ISBN = {978-1-7281-5313-1},
Keywords = {baby crying; real-time; live; crying detection},
Keywords-Plus = {AUTOMATIC DETECTION},
Research-Areas = {Automation \& Control Systems; Engineering},
Web-of-Science-Categories  = {Automation \& Control Systems; Engineering, Electrical \& Electronic},
Author-Email = {eesze96@1utar.my
   yapws@utar.edu.my
   humyc@utar.edu.my
   zulaikha.kadim@mimos.my
   hockwoon.hon@mimos.my
   teeyeekai@gmail.com},
ResearcherID-Numbers = {Yap, Wun-She/ABB-5158-2021
   Chai, Hum Yan/H-9021-2018
   Foo, Lee Sze/AAI-5506-2021
   Tee, Yee Kai/O-1677-2015},
ORCID-Numbers = {Chai, Hum Yan/0000-0002-9657-8311
   Tee, Yee Kai/0000-0002-0263-6358},
Number-of-Cited-References = {20},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BR1EK},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000631808500006},
DA = {2022-09-28},
}

@article{ WOS:000717219500001,
Author = {Pereira, Monica and Meng, Hongying and Hone, Kate},
Title = {Prediction of Communication Effectiveness During Media Skills Training
   Using Commercial Automatic Non-verbal Recognition Systems},
Journal = {FRONTIERS IN PSYCHOLOGY},
Year = {2021},
Volume = {12},
Month = {SEP 29},
Abstract = {It is well recognised that social signals play an important role in
   communication effectiveness. Observation of videos to understand
   non-verbal behaviour is time-consuming and limits the potential to
   incorporate detailed and accurate feedback of this behaviour in
   practical applications such as communication skills training or
   performance evaluation. The aim of the current research is twofold: (1)
   to investigate whether off-the-shelf emotion recognition technology can
   detect social signals in media interviews and (2) to identify which
   combinations of social signals are most promising for evaluating
   trainees' performance in a media interview. To investigate this,
   non-verbal signals were automatically recognised from practice on-camera
   media interviews conducted within a media training setting with a sample
   size of 34. Automated non-verbal signal detection consists of multimodal
   features including facial expression, hand gestures, vocal behaviour and
   `honest' signals. The on-camera interviews were categorised into
   effective and poor communication exemplars based on communication skills
   ratings provided by trainers and neutral observers which served as a
   ground truth. A correlation-based feature selection method was used to
   select signals associated with performance. To assess the accuracy of
   the selected features, a number of machine learning classification
   techniques were used. Naive Bayes analysis produced the best results
   with an F-measure of 0.76 and prediction accuracy of 78\%. Results
   revealed that a combination of body movements, hand movements and facial
   expression are relevant for establishing communication effectiveness in
   the context of media interviews. The results of the current study have
   implications for the automatic evaluation of media interviews with a
   number of potential application areas including enhancing communication
   training including current media skills training.},
Publisher = {FRONTIERS MEDIA SA},
Address = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Pereira, M (Corresponding Author), London Metropolitan Univ, Sch Social Sci, Dept Psychol, London, England.
   Pereira, Monica, London Metropolitan Univ, Sch Social Sci, Dept Psychol, London, England.
   Meng, Hongying, Brunel Univ London, Coll Engn Design \& Phys Sci, Dept Elect \& Comp Engn, London, England.
   Hone, Kate, Brunel Univ London, Coll Engn Design \& Phys Sci, Dept Comp Sci, London, England.},
DOI = {10.3389/fpsyg.2021.675721},
Article-Number = {675721},
ISSN = {1664-1078},
Keywords = {social signals detection; commercial technologies; communication skills;
   training; non-verbal signals; media interviews; multimodal fusion},
Keywords-Plus = {FACIAL EXPRESSIONS; IMPRESSION-FORMATION; THIN SLICES; EMOTION;
   BEHAVIOR; ADOLESCENTS; VALIDATION; JUDGMENTS; CHILDREN},
Research-Areas = {Psychology},
Web-of-Science-Categories  = {Psychology, Multidisciplinary},
Author-Email = {monica.pereira@londonmet.ac.uk},
ResearcherID-Numbers = {Meng, Hongying/O-5192-2014},
ORCID-Numbers = {Meng, Hongying/0000-0002-8836-1382},
Funding-Acknowledgement = {UK Defence Science and Technology Laboratory},
Funding-Text = {The authors acknowledge support of the UK Defence Science and Technology
   Laboratory. The Engineering and Physical Sciences Research Council (DTP
   Grant) is also acknowledged.},
Number-of-Cited-References = {95},
Times-Cited = {0},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {4},
Journal-ISO = {Front. Psychol.},
Doc-Delivery-Number = {WV4PG},
Web-of-Science-Index = {Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000717219500001},
OA = {Green Accepted, Green Published, gold},
DA = {2022-09-28},
}

@article{ WOS:000526271800001,
Author = {Puli, Akshay and Kushki, Azadeh},
Title = {Toward Automatic Anxiety Detection in Autism: A Real-Time Algorithm for
   Detecting Physiological Arousal in the Presence of Motion},
Journal = {IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING},
Year = {2020},
Volume = {67},
Number = {3},
Pages = {646-657},
Month = {MAR},
Abstract = {Objective: Anxiety is a significant clinical concern in autism spectrum
   disorder (ASD) due to its negative impact on physical and psychological
   health. Treatment of anxiety in ASD remains a challenge due to
   difficulties with self-awareness and communication of anxiety symptoms.
   To reduce these barriers to treatment, physiological markers of
   autonomic arousal, collected through wearable sensors, have been
   proposed as real-time, objective, and language-free measures of anxiety.
   A critical limitation of the existing anxiety detection systems is that
   physiological arousal is not specific to anxiety and can occur with
   other user states such as physical activity. This can result in false
   positives, which can hinder the operation of these systems in real-world
   situations. The objective of this paper was to address this challenge by
   proposing an approach for real-time detection and mitigation of physical
   activity effects. Methods: A novel multiple model Kalman-like filter is
   proposed to integrate heart rate and accelerometry signals. The filter
   tracks user heart rate under different motion assumptions and chooses
   the appropriate model for anxiety detection based on user motion
   conditions. Results: Evaluation of the algorithm using data from a
   sample of children with ASD shows a significant reduction in false
   positives compared to the state-of-the-art, and an overall arousal
   detection accuracy of 93\%. Conclusion: The proposed method is able to
   reduce false detections due to user motion and effectively detect
   arousal states during movement periods. Significance: The results add to
   the growing evidence supporting the feasibility of wearable technologies
   for anxiety detection and management in naturalistic settings.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Kushki, A (Corresponding Author), Univ Toronto, Bloorview Res Inst, Toronto, ON M4G 1R8, Canada.
   Kushki, A (Corresponding Author), Univ Toronto, Inst Biomat \& Biomed Engn, Toronto, ON M4G 1R8, Canada.
   Puli, Akshay; Kushki, Azadeh, Univ Toronto, Bloorview Res Inst, Toronto, ON M4G 1R8, Canada.
   Puli, Akshay; Kushki, Azadeh, Univ Toronto, Inst Biomat \& Biomed Engn, Toronto, ON M4G 1R8, Canada.},
DOI = {10.1109/TBME.2019.2919273},
ISSN = {0018-9294},
EISSN = {1558-2531},
Keywords = {Heart rate; Kalman filters; Physiology; Biomedical measurement;
   Mathematical model; Microsoft Windows; Real-time systems; Multimodal
   Kalman filter; anxiety detection; autism; ASD},
Keywords-Plus = {SPECTRUM DISORDERS; EMOTION RECOGNITION; STRESS-DETECTION; HEART-RATE;
   CHILDREN},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Biomedical},
Author-Email = {akushki@hollandbloorview.ca},
ORCID-Numbers = {Puli, Akshay/0000-0003-0800-7541},
Funding-Acknowledgement = {Natural Sciences and Engineering Research Council (NSERC) of Canada;
   Kids Health Brain Network},
Funding-Text = {This work was supported by the Natural Sciences and Engineering Research
   Council (NSERC) of Canada and Kids Health Brain Network.},
Number-of-Cited-References = {41},
Times-Cited = {4},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {20},
Journal-ISO = {IEEE Trans. Biomed. Eng.},
Doc-Delivery-Number = {LD8IQ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000526271800001},
DA = {2022-09-28},
}

@InProceedings{WOS:000476892400027,
  author                     = {De Feyter, Floris and Van Beeck, Kristof and Goedeme, Toon},
  booktitle                  = {ADVANCED CONCEPTS FOR INTELLIGENT VISION SYSTEMS, ACIVS 2018},
  title                      = {Automatically Selecting the Best Pictures for an Individualized Child Photo Album},
  year                       = {2018},
  address                    = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
  editor                     = {BlancTalon, J and Helbert, D and Philips, W and Popescu, D and Scheunders, P},
  note                       = {19th International Conference on Advanced Concepts for Intelligent Vision Systems (ACIVS), Poitiers, FRANCE, SEP 24-27, 2018},
  organization               = {Univ Antwerp},
  pages                      = {321-332},
  publisher                  = {SPRINGER INTERNATIONAL PUBLISHING AG},
  series                     = {Lecture Notes in Computer Science},
  volume                     = {11182},
  status                     = {Rejeitado - Escopo},
  abstract                   = {In this paper we investigate the best way to automatically compose a
   photo album for an individual child from a large collection of
   photographs taken during a school year. For this, we efficiently combine
   state-of-the-art identification algorithms to select relevant photos,
   with an aesthetics estimation algorithm to only keep the best images.
   For the identification task, we achieved 86\% precision for 86\% recall
   on a real-life dataset containing lots of specific challenges of this
   application. Indeed, playing children appear in non-standard poses and
   facial expressions, can be dressed up or have their faces painted etc.
   In a top-1 sense, our system was able to correctly identify 89.2\% of
   the faces in close-up. Apart from facial recognition, we discuss and
   evaluate extending the identification system with person
   re-identification. To select out the best-looking photos from the
   identified child photos to fill the album with, we propose an automatic
   assessment technique that takes into account the aesthetic photo quality
   as well as the emotions in the photos. Our experiments show that this
   measure correlates well with a manually labeled general appreciation
   score.},
  affiliation                = {De Feyter, F (Corresponding Author), Katholieke Univ Leuven, EAVISE, Leuven, Belgium. De Feyter, Floris; Van Beeck, Kristof; Goedeme, Toon, Katholieke Univ Leuven, EAVISE, Leuven, Belgium.},
  author-email               = {floris.defeyter@kuleuven.be},
  da                         = {2022-09-28},
  doc-delivery-number        = {BN2LG},
  doi                        = {10.1007/978-3-030-01449-0\_27},
  eissn                      = {1611-3349},
  isbn                       = {978-3-030-01449-0; 978-3-030-01448-3},
  issn                       = {0302-9743},
  keywords                   = {Face recognition; Person re-identification; Aesthetics analysis; Emotion classification; Child identification},
  language                   = {English},
  number-of-cited-references = {12},
  oa                         = {Green Accepted},
  orcid-numbers              = {De Feyter, Floris/0000-0003-2690-0181 Goedeme, Toon/0000-0002-7477-8961},
  research-areas             = {Computer Science; Engineering; Imaging Science \& Photographic Technology},
  times-cited                = {2},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000476892400027},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {2},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \& Methods; Engineering, Electrical \& Electronic; Imaging Science \& Photographic Technology},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@InProceedings{WOS:000568448300017,
  author                     = {Iqbal, Tahreem and Javed, Sobia Tariq},
  booktitle                  = {2019 8TH INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES (ICICT 2019)},
  title                      = {A New Perspective towards Analysis of Human Facial Expression Using Supervised Classification Algorithms},
  year                       = {2019},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  editor                     = {Mahmood, T and Khoja, S and Ghani, S},
  note                       = {8th International Conference on Information and Communication Technologies (ICICT), Inst Business Adm, Karachi, PAKISTAN, NOV 16-17, 2019},
  pages                      = {94-99},
  publisher                  = {IEEE},
  series                     = {International Conference on Information and Communication Technologies ICICT},
  status                     = {Aceito},
  abstract                   = {As cost-effective and relatively accurate models for behavior
   classification, automatic facial expression analysis has the potential
   to be applied to multiple disciplines. The current research deals with
   real-time classification of evoked emotions in children. Reason behind
   analyzing children behavior is that they possess immature cognitive
   abilities compared to adults, they are more likely to be influenced by
   external stimuli and they are less likely to pose and hide expression.
   This research work uses a three-phase classification model to analyze
   real time captured emotion varying children facial expressions. Two
   classifiers K-Nearest Neighbor and SVM were used for classification. The
   classification model was tested using our own real time recorded
   children Facial Expression (CFE).},
  affiliation                = {Iqbal, T (Corresponding Author), Natl Univ Comp \& Emerging Sci, Dept Comp Sci, Lahore, Pakistan. Iqbal, Tahreem; Javed, Sobia Tariq, Natl Univ Comp \& Emerging Sci, Dept Comp Sci, Lahore, Pakistan.},
  author-email               = {1135009@lhr.nu.edu.pk sobia.tariq@nu.edu.pk},
  da                         = {2022-09-28},
  doc-delivery-number        = {BP8XF},
  isbn                       = {978-1-7281-2334-9},
  keywords                   = {Face Detection; Feature Extraction; Eigenface; KNN; SVM; Voila Jones; Facial Expression},
  keywords-plus              = {EMOTION; RECOGNITION; TRACKING},
  language                   = {English},
  number-of-cited-references = {34},
  priority                   = {prio1},
  research-areas             = {Computer Science; Engineering; Telecommunications},
  times-cited                = {0},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000568448300017},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {0},
  web-of-science-categories  = {Computer Science, Interdisciplinary Applications; Engineering, Electrical \& Electronic; Telecommunications},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{ WOS:000791192100011,
Author = {Wang, Mingxun and Luo, Gang and Chen, Hao},
Title = {Practice of Music Therapy for Autistic Children Based on Music Data
   Mining},
Journal = {MATHEMATICAL PROBLEMS IN ENGINEERING},
Year = {2022},
Volume = {2022},
Month = {APR 6},
Abstract = {For children with autism, music therapy has aroused great concern with
   its novelty and better influence. Music therapy, as one of the effective
   treatment methods, has an important influence on the social interaction,
   behavior, and emotion of autistic children. This study attempts to
   explore a form of applying highly specialized impromptu music therapy to
   the personal treatment of autistic children in schools for the disabled,
   as well as the design method of specific music activities. Based on
   music data mining, the machine learning method is introduced to model
   music emotion features, and various algorithms are compared to find a
   model with higher recognition rate, and, at the same time, the antinoise
   ability and generalization ability of the model are further improved.
   Finally, a music emotion cognitive model with better performance is
   established. The results show that the model can effectively promote the
   overall development of autistic children's cognitive movement, social
   communication, language communication, and cognition.},
Publisher = {HINDAWI LTD},
Address = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Wang, MX (Corresponding Author), Nanchang Hangkong Univ, Coll Air Serv \& Mus, Nanchang 330063, Jiangxi, Peoples R China.
   Wang, Mingxun, Nanchang Hangkong Univ, Coll Air Serv \& Mus, Nanchang 330063, Jiangxi, Peoples R China.
   Luo, Gang; Chen, Hao, Nanchang Hangkong Univ, Sch Informat Engn, Nanchang 330063, Jiangxi, Peoples R China.},
DOI = {10.1155/2022/4576211},
Article-Number = {4576211},
ISSN = {1024-123X},
EISSN = {1563-5147},
Research-Areas = {Engineering; Mathematics},
Web-of-Science-Categories  = {Engineering, Multidisciplinary; Mathematics, Interdisciplinary
   Applications},
Author-Email = {70633@nchu.edu.cn
   853539876@qq.com
   chenhaoshl@nchu.edu.cn},
Funding-Acknowledgement = {Jiangxis 13th Five-Year Social and Science Plan in 2019: The
   Electroencephalography Study of the Role of Music Therapy in
   Rehabilitation of Children with Autism Spectrum Disorder {[}19Y517]},
Funding-Text = {This work was supported by Jiangxis 13th Five-Year Social and Science
   Plan in 2019: The Electroencephalography Study of the Role of Music
   Therapy in Rehabilitation of Children with Autism Spectrum Disorder (no.
   19Y517).},
Number-of-Cited-References = {24},
Times-Cited = {0},
Usage-Count-Last-180-days = {9},
Usage-Count-Since-2013 = {9},
Journal-ISO = {Math. Probl. Eng.},
Doc-Delivery-Number = {0Z6NF},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000791192100011},
OA = {gold},
DA = {2022-09-28},
}

@Article{WOS:000705501000003,
  author                     = {Kalantarian, Haik and Washington, Peter and Schwartz, Jessey and Daniels, Jena and Haber, Nick and Wall, Dennis P.},
  journal                    = {JOURNAL OF HEALTHCARE INFORMATICS RESEARCH},
  title                      = {Guess What? Towards Understanding Autism from Structured Video Using Facial Affect},
  year                       = {2019},
  issn                       = {2509-4971},
  month                      = {MAR 15},
  number                     = {1, SI},
  pages                      = {43-66},
  volume                     = {3},
  status                     = {Aceito},
  abstract                   = {Autism Spectrum Disorder (ASD) is a condition affecting an estimated 1
   in 59 children in the United States. Due to delays in diagnosis and
   imbalances in coverage, it is necessary to develop new methods of care
   delivery that can appropriately empower children and caregivers by
   capitalizing on mobile tools and wearable devices for use outside of
   clinical settings. In this paper, we present a mobile charades-style
   game, Guess What?, used for the acquisition of structured video from
   children with ASD for behavioral disease research. We then apply face
   tracking and emotion recognition algorithms to videos acquired through
   Guess What? game play. By analyzing facial affect in response to various
   prompts, we demonstrate that engagement and facial affect can be
   quantified and measured using real-time image processing algorithms: an
   important first-step for future therapies, at-home screenings, and
   outcome measures based on home video. Our study of eight subjects
   demonstrates the efficacy of this system for deriving highly emotive
   structured video from children with ASD through an engaging gamified
   mobile platform, while revealing the most efficacious prompts and
   categories for producing diverse emotion in participants.},
  address                    = {CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND},
  affiliation                = {Kalantarian, H (Corresponding Author), Stanford Univ, Dept Pediat, Stanford, CA 94305 USA. Kalantarian, Haik; Washington, Peter; Schwartz, Jessey; Daniels, Jena; Haber, Nick; Wall, Dennis P., Stanford Univ, Dept Pediat, Stanford, CA 94305 USA. Wall, Dennis P., Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA.},
  author-email               = {haik@stanford.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {WE3CM},
  doi                        = {10.1007/s41666-018-0034-9},
  eissn                      = {2509-498X},
  funding-acknowledgement    = {National Institutes of Health {[}1R21HD091500-01, 1R01EB025025-01]; Hartwell Foundation; David and Lucile Packard Foundation Special Projects Grant; Beckman Center for Molecular and Genetic Medicine; Coulter Endowment Translational Research Grant; Berry Fellowship; Child Health Research Institute; Spectrum Pilot Program; Thrasher Research Fund; Dekeyser and Friends Foundation; Mosbacher Family Fund for Autism Research},
  funding-text               = {This study was supported by awards to D.P.W. by the National Institutes of Health (1R21HD091500-01 and 1R01EB025025-01). Additionally, we acknowledge support to D.P.W. from the Hartwell Foundation, the David and Lucile Packard Foundation Special Projects Grant, Beckman Center for Molecular and Genetic Medicine, Coulter Endowment Translational Research Grant, Berry Fellowship, Child Health Research Institute, Spectrum Pilot Program, and Thrasher Research Fund. The Dekeyser and Friends Foundation, the Mosbacher Family Fund for Autism Research, and Peter Sullivan provided additional funding.},
  journal-iso                = {J. Healthc. Inform. Res.},
  keywords                   = {Autism; Emotion; Mobile},
  keywords-plus              = {EARLY BEHAVIORAL INTERVENTION; SOCIAL COMMUNICATION; CHILDREN},
  language                   = {English},
  number-of-cited-references = {39},
  oa                         = {Green Published},
  orcid-numbers              = {washington, peter/0000-0003-3276-4411},
  priority                   = {prio1},
  publisher                  = {SPRINGERNATURE},
  research-areas             = {Computer Science; Health Care Sciences \& Services; Medical Informatics},
  times-cited                = {16},
  type                       = {Article},
  unique-id                  = {WOS:000705501000003},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {3},
  web-of-science-categories  = {Computer Science, Information Systems; Health Care Sciences \& Services; Medical Informatics},
  web-of-science-index       = {Emerging Sources Citation Index (ESCI)},
}

@Article{WOS:000513275100008,
  author                     = {Jaliaawala, Muhammad Shoaib and Khan, Rizwan Ahmed},
  journal                    = {ARTIFICIAL INTELLIGENCE REVIEW},
  title                      = {Can autism be catered with artificial intelligence-assisted intervention technology? A comprehensive survey},
  year                       = {2020},
  issn                       = {0269-2821},
  month                      = {FEB},
  number                     = {2},
  pages                      = {1039-1069},
  volume                     = {53},
  status                     = {Aceito},
  abstract                   = {This article presents an extensive literature review of technology based
   intervention methodologies for individuals facing autism spectrum
   disorder (ASD). Reviewed methodologies include: contemporary computer
   aided systems, computer vision assisted technologies and virtual reality
   (VR) or artificial intelligence (AI)-assisted interventions. The
   research over the past decade has provided enough demonstrations that
   individuals with ASD have a strong interest in technology based
   interventions, which are useful in both, clinical settings as well as at
   home and classrooms. Despite showing great promise, research in
   developing an advanced technology based intervention that is clinically
   quantitative for ASD is minimal. Moreover, the clinicians are generally
   not convinced about the potential of the technology based interventions
   due to non-empirical nature of published results. A major reason behind
   this lack of acceptability is that a vast majority of studies on
   distinct intervention methodologies do not follow any specific standard
   or research design. We conclude from our findings that there remains a
   gap between the research community of computer science, psychology and
   neuroscience to develop an AI assisted intervention technology for
   individuals suffering from ASD. Following the development of a
   standardized AI based intervention technology, a database needs to be
   developed, to devise effective AI algorithms.},
  address                    = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  affiliation                = {Khan, RA (Corresponding Author), Barrett Hodgson Univ, Fac IT, Karachi, Pakistan. Khan, RA (Corresponding Author), Univ Claude Bernard Lyon1, LIRIS, Villeurbanne, France. Khan, Rizwan Ahmed, Barrett Hodgson Univ, Fac IT, Karachi, Pakistan. Jaliaawala, Muhammad Shoaib, Hamdard Univ, Fac Engn Sci \& Technol, Karachi, Pakistan. Khan, Rizwan Ahmed, Univ Claude Bernard Lyon1, LIRIS, Villeurbanne, France. Jaliaawala, Muhammad Shoaib, Natl Univ Comp \& Emerging Sci, FAST NU, Karachi, Pakistan.},
  author-email               = {sjaliawala@gmail.com rizwan17@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {KL2RC},
  doi                        = {10.1007/s10462-019-09686-8},
  eissn                      = {1573-7462},
  journal-iso                = {Artif. Intell. Rev.},
  keywords                   = {Computer aided systems (CAS); Computer vision assisted technologies (CVAT); Autism spectrum disorder (ASD); Facial expression recognition; Artificial intelligence; Virtual reality},
  keywords-plus              = {HIGH-FUNCTIONING AUTISM; FACIAL EXPRESSION RECOGNITION; SPECTRUM DISORDERS; ASPERGER-SYNDROME; VIRTUAL ENVIRONMENTS; EMOTION RECOGNITION; SOCIAL-INTERACTION; CHILDREN; SKILLS; ADULTS},
  language                   = {English},
  number-of-cited-references = {103},
  oa                         = {Green Submitted},
  orcid-numbers              = {Khan, PhD, Rizwan Ahmed/0000-0003-0819-800X jaliawala, muhammad shoaib/0000-0002-7322-6508},
  priority                   = {prio2},
  publisher                  = {SPRINGER},
  research-areas             = {Computer Science},
  researcherid-numbers       = {Khan, PhD, Rizwan Ahmed/N-7134-2018},
  times-cited                = {18},
  type                       = {Article},
  unique-id                  = {WOS:000513275100008},
  usage-count-last-180-days  = {12},
  usage-count-since-2013     = {52},
  web-of-science-categories  = {Computer Science, Artificial Intelligence},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000691677100041,
  author                     = {Aue, Tatjana and Hoeppli, Marie -Eve and Scharnowski, Frank and Steyrl, David},
  journal                    = {JOURNAL OF AFFECTIVE DISORDERS},
  title                      = {Contributions of diagnostic, cognitive, and somatovisceral information to the prediction of fear ratings in spider phobic and non-spider-fearful individuals},
  year                       = {2021},
  issn                       = {0165-0327},
  month                      = {NOV 1},
  pages                      = {296-304},
  volume                     = {294},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Background: Physiological responding is a key characteristic of fear
   responses. Yet, it is unknown whether the time-consuming measurement of
   somatovisceral responses ameliorates the prediction of individual fear
   re-sponses beyond the accuracy reached by the consideration of
   diagnostic (e.g., phobic vs. non phobic) and cognitive (e.g., risk
   estimation) factors, which can be more easily assessed. Method: We
   applied a machine learning approach to data of an experiment, in which
   spider phobic and non-spider fearful participants (diagnostic factor)
   faced pictures of spiders. For each experimental trial, partici-pants
   specified their personal risk of encountering the spider (cognitive
   factor), as well as their subjective fear (outcome variable) on
   quasi-continuous scales, while diverse somatovisceral responses were
   registered (heart rate, electrodermal activity, respiration, facial
   muscle activity). Results: The machine-learning analyses revealed that
   fear ratings were predominantly predictable by the diag-nostic factor.
   Yet, when allowing for learning of individual patterns in the data,
   somatovisceral responses contributed additional information on the fear
   ratings, yielding a prediction accuracy of 81\% explained variance.
   Moreover, heart rate prior to picture onset, but not heart rate
   reactivity increased predictive power. Limitations: Fear was solely
   assessed by verbal reports, only 27 females were considered, and no
   generalization to other anxiety disorders is possible. Conclusions:
   After training the algorithm to learn about individual-specific
   responding, somatovisceral patterns can be successfully exploited. Our
   findings further point to the possibility that the expectancy-related
   autonomic state throughout the experiment predisposes an individual to
   experience specific levels of fear, with less influ-ence of the actual
   visual stimulations.},
  address                    = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  affiliation                = {Aue, T (Corresponding Author), Univ Bern, Inst Psychol, Fabr Str 8, CH-3012 Bern, Switzerland. Aue, Tatjana; Hoeppli, Marie -Eve, Univ Geneva, Swiss Ctr Affect Sci, Geneva, Switzerland. Aue, Tatjana, Univ Bern, Inst Psychol, Fabr Str 8, CH-3012 Bern, Switzerland. Hoeppli, Marie -Eve, Cincinnati Childrens Hosp Med Ctr, Ctr Understanding Pediat Pain CUPP, Cincinnati, OH 45229 USA. Hoeppli, Marie -Eve, Cincinnati Childrens Hosp Med Ctr, Div Behav Med \& Clin Psychol, Cincinnati, OH 45229 USA. Scharnowski, Frank; Steyrl, David, Univ Vienna, Dept Cognit Emot \& Methods Psychol, Vienna, Austria. Scharnowski, Frank; Steyrl, David, Univ Zurich, Dept Psychiat Psychotherapy \& Psychosomat, Zurich, Switzerland.},
  author-email               = {tatjana.aue@unibe.ch},
  da                         = {2022-09-28},
  doc-delivery-number        = {UK0PJ},
  doi                        = {10.1016/j.jad.2021.07.040},
  earlyaccessdate            = {JUL 2021},
  eissn                      = {1573-2517},
  funding-acknowledgement    = {Swiss National Science Foundation {[}PZ00P1\_121590]},
  funding-text               = {This research was supported by Grant No. PZ00P1\_121590 of the Swiss National Science Foundation to Tatjana Aue.},
  journal-iso                = {J. Affect. Disord.},
  keywords                   = {Subjective fear; Phobia; Threat; Psychophysiology; Machine learning},
  keywords-plus              = {PANIC DISORDER; EMOTION RECOGNITION; ANXIETY; PATTERNS; EXPOSURE; THREAT; BIASES},
  language                   = {English},
  number-of-cited-references = {64},
  oa                         = {hybrid, Green Published},
  orcid-numbers              = {Hoeppli, Marie-Eve/0000-0002-9872-7739 Aue, Tatjana/0000-0001-9480-1711 Scharnowski, Frank/0000-0002-9973-2521 Steyrl, David/0000-0002-3215-8115},
  publisher                  = {ELSEVIER},
  research-areas             = {Neurosciences \& Neurology; Psychiatry},
  times-cited                = {1},
  type                       = {Article},
  unique-id                  = {WOS:000691677100041},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {4},
  web-of-science-categories  = {Clinical Neurology; Psychiatry},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@article{ WOS:000530252300003,
Author = {Bagirathan, Anandhi and Selvaraj, Jerritta and Gurusamy, Anusuya and
   Das, Himangshu},
Title = {Recognition of positive and negative valence states in children with
   autism spectrum disorder (ASD) using discrete wavelet transform (DWT)
   analysis of electrocardiogram signals (ECG)},
Journal = {JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING},
Year = {2021},
Volume = {12},
Number = {1, SI},
Pages = {405-416},
Month = {JAN},
Abstract = {Children with autism spectrum disorder (ASD) are deficit in
   communication, social skills, empathy, emotional responsiveness and have
   significant behavioral pattern. They have difficulty in understanding
   other feelings and their own emotions. This leads to the sudden
   emotional outburst and aggressive behavior in these children. Parents,
   caretakers and doctors find it very difficult to prevent such extreme
   behaviors. Learning the positive and negative valence leads in
   determining the early indications before the onset of emotional
   outbursts in children with ASD. The present study measures the psycho
   physiological electrocardiogram (ECG) signal from the typically
   developed (TD) children and children with ASD in the age group of 5-11
   years. Personalized protocol was developed for every child with ASD to
   induce positive and negative valence and ECG data was collected using
   wearable Shimmer ECG device. The heart rate variability (HRV) and the
   QRS amplitude were derived from ECG signal using Pan-Tompkins algorithm
   and eleven features were extracted using DWT (db2, db4 and db8) mother
   wavelet. The significant features of ECG, HRV and QRS amplitude were
   classified using the K nearest neighbor (KNN), support vector machine
   (SVM) and ensemble classifier. Ensemble and KNN classifier achieved
   maximum accuracy of 81\% and 76.2\% for children with ASD and Ensemble
   and SVM classifiers obtained maximum accuracy of 87.4\% and 83.8\% for
   TD children using HRV data.},
Publisher = {SPRINGER HEIDELBERG},
Address = {TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY},
Type = {Article},
Language = {English},
Affiliation = {Selvaraj, J (Corresponding Author), Vels Inst Sci Technol \& Adv Studies VISTAS, Chennai, Tamil Nadu, India.
   Bagirathan, Anandhi; Selvaraj, Jerritta, Vels Inst Sci Technol \& Adv Studies VISTAS, Chennai, Tamil Nadu, India.
   Gurusamy, Anusuya; Das, Himangshu, Natl Inst Empowerment Persons Multiple Disabil Di, Chennai, Tamil Nadu, India.},
DOI = {10.1007/s12652-020-01985-1},
EarlyAccessDate = {MAY 2020},
ISSN = {1868-5137},
EISSN = {1868-5145},
Keywords = {Autism spectrum disorder (ASD); Heart rate variability (HRV);
   Pan-Tompkins algorithm; K nearest neighbor (KNN)},
Keywords-Plus = {EMOTION RECOGNITION; PARKINSONS-DISEASE; FACIAL EXPRESSION;
   YOUNG-CHILDREN; RESPONSES; AROUSAL; EEG; FEATURES; SYSTEM},
Research-Areas = {Computer Science; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Telecommunications},
Author-Email = {sn.jerritta@gmail.com},
ResearcherID-Numbers = {B, Anandhi/AAN-6251-2021},
ORCID-Numbers = {B, Anandhi/0000-0003-3034-8078},
Number-of-Cited-References = {42},
Times-Cited = {4},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {12},
Journal-ISO = {J. Ambient Intell. Humaniz. Comput.},
Doc-Delivery-Number = {QJ7OB},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000530252300003},
DA = {2022-09-28},
}

@Article{WOS:000757299300029,
  author                     = {Lauer, Luisa and Altmeyer, Kristin and Malone, Sarah and Barz, Michael and Bruenken, Roland and Sonntag, Daniel and Peschel, Markus},
  journal                    = {SENSORS},
  title                      = {Investigating the Usability of a Head-Mounted Display Augmented Reality Device in Elementary School Children},
  year                       = {2021},
  month                      = {OCT},
  number                     = {19},
  volume                     = {21},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Augmenting reality via head-mounted displays (HMD-AR) is an emerging
   technology in education. The interactivity provided by HMD-AR devices is
   particularly promising for learning, but presents a challenge to human
   activity recognition, especially with children. Recent technological
   advances regarding speech and gesture recognition concerning Microsoft's
   HoloLens 2 may address this prevailing issue. In a within-subjects study
   with 47 elementary school children (2nd to 6th grade), we examined the
   usability of the HoloLens 2 using a standardized tutorial on multimodal
   interaction in AR. The overall system usability was rated ``good{''}.
   However, several behavioral metrics indicated that specific interaction
   modes differed in their efficiency. The results are of major importance
   for the development of learning applications in HMD-AR as they partially
   deviate from previous findings. In particular, the well-functioning
   recognition of children's voice commands that we observed represents a
   novelty. Furthermore, we found different interaction preferences in
   HMD-AR among the children. We also found the use of HMD-AR to have a
   positive effect on children's activity-related achievement emotions.
   Overall, our findings can serve as a basis for determining general
   requirements, possibilities, and limitations of the implementation of
   educational HMD-AR environments in elementary school classrooms.},
  address                    = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  affiliation                = {Lauer, L (Corresponding Author), Saarland Univ, Dept Phys, Campus C6-3, D-66123 Saarbrucken, Germany. Lauer, Luisa; Peschel, Markus, Saarland Univ, Dept Phys, Campus C6-3, D-66123 Saarbrucken, Germany. Altmeyer, Kristin; Malone, Sarah; Bruenken, Roland, Saarland Univ, Dept Educ, Campus A4-2, D-66123 Saarbrucken, Germany. Barz, Michael; Sonntag, Daniel, German Res Ctr Artificial Intelligence DFKI, Interact Machine Learning Dept, Stuhlsatzenhausweg 3, D-66123 Saarbrucken, Germany. Barz, Michael; Sonntag, Daniel, Oldenburg Univ, Appl Artificial Intelligence, Marie Curie Str 1, D-26129 Oldenburg, Germany.},
  article-number             = {6623},
  author-email               = {luisa.lauer@uni-saarland.de kristin.altmeyer@uni-saarland.de s.malone@mx.uni-saarland.de michael.barz@dfki.de roland.bruenken@uni-saarland.de daniel.sonntag@dfki.de markus.peschel@uni-saarland.de},
  da                         = {2022-09-28},
  doc-delivery-number        = {ZC1OX},
  doi                        = {10.3390/s21196623},
  eissn                      = {1424-8220},
  funding-acknowledgement    = {Bundesministerium fur Bildung und Forschung {[}01JD1811A, 01JD1811C]; Deutsche Forschungsgemeinschaft (DFG, German Research Foundation); Saarland University},
  funding-text               = {This research was funded by Bundesministerium fur Bildung und Forschung, grant numbers 01JD1811A and 01JD1811C. We acknowledge support by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) and Saarland University within the funding programme Open Access Publishing.},
  journal-iso                = {Sensors},
  keywords                   = {head-mounted displays; augmented reality; human activity recognition; usability; elementary education},
  keywords-plus              = {CHALLENGES; EDUCATION; EMOTIONS},
  language                   = {English},
  number-of-cited-references = {60},
  oa                         = {Green Published, gold},
  orcid-numbers              = {Bruenken, Roland/0000-0001-6038-8746 Barz, Michael/0000-0001-6730-2466 Lauer, Luisa/0000-0002-0015-0821 Malone, Sarah/0000-0001-8610-2611 Peschel, Markus/0000-0002-1334-2531},
  publisher                  = {MDPI},
  research-areas             = {Chemistry; Engineering; Instruments \& Instrumentation},
  researcherid-numbers       = {Bruenken, Roland/GRJ-4451-2022},
  times-cited                = {1},
  type                       = {Article},
  unique-id                  = {WOS:000757299300029},
  usage-count-last-180-days  = {5},
  usage-count-since-2013     = {8},
  web-of-science-categories  = {Chemistry, Analytical; Engineering, Electrical \& Electronic; Instruments \& Instrumentation},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000609419700007,
  author                     = {Ghosh, Lidia and Saha, Sriparna and Konar, Amit},
  journal                    = {COMPUTERS IN HUMAN BEHAVIOR},
  title                      = {Decoding emotional changes of android-garners using a fused Type-2 fuzzy deep neural network},
  year                       = {2021},
  issn                       = {0747-5632},
  month                      = {MAR},
  volume                     = {116},
  status                     = {Aceito},
  abstract                   = {With the fastest growing popularity of gaming applications on android
   phone, analyzing emotion changes of steadfast android-gamers have become
   a study of utmost interest among most of the psychologists. Recently,
   some android games are producing negative impacts to the gamers; even in
   the worst cases the effect is becoming life-threatening too. Most of the
   existing research works are based on psychological view-point of
   exploring the impact (positive/negative) of playing android games for
   the child and adult age-group. However, the online recognition of
   emotional state changes of the android-gamers while playing video games
   may be relatively unexplored. To fill this void, the present study
   proposes a novel method of identifying the emotional state changes of
   android-gamers by decoding their brain signals and facial images
   simultaneously during playing video games. Besides above, the second
   novelty of the paper lies in designing a multimodal fusion method
   between brain signals and facial images for the said application. To
   address this challenge, the paper proposes a fused type-2 fuzzy deep
   neural network (FT2FDNN) which integrates the brain signal processing
   approach by a general type-2 fuzzy reasoning algorithm with the flavor
   of the image/video processing approach using a deep convolutional neural
   network. FT2FDNN uses multiple modalities to extract the similar
   information (here, emotional changes) simultaneously from the type-2
   fuzzy and deep neural representations. The proposed fused type-2 fuzzy
   deep learning paradigm demonstrates promising results in classifying the
   emotional changes of gamers with high classification accuracy. Thus the
   proposed work explores a new era for future researchers.},
  address                    = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
  affiliation                = {Saha, S (Corresponding Author), Maulana Abul Kalam Azad Univ Technol, Dept Comp Sci \& Engn, Kolkata, W Bengal, India. Ghosh, Lidia; Konar, Amit, Jadavpur Univ, Dept Elect \& Tele Commun Engn, Kolkata, W Bengal, India. Saha, Sriparna, Maulana Abul Kalam Azad Univ Technol, Dept Comp Sci \& Engn, Kolkata, W Bengal, India.},
  article-number             = {106640},
  author-email               = {lidiaghosh.bits@gmail.com sahasriparna@gmail.com konaramit@yahoo.co.in},
  da                         = {2022-09-28},
  doc-delivery-number        = {PU6NY},
  doi                        = {10.1016/j.chb.2020.106640},
  eissn                      = {1873-7692},
  funding-acknowledgement    = {Ministry of Human Resource Development; UGC},
  funding-text               = {The first and third authors thankfully acknowledged the fund provided by Ministry of Human Resource Development for RUSA-II project granted to Jadavpur University, India. The second author is thankful to the University for providing research seed money and UGC Start-up Grant under the scheme of Basic Scientific Research. The work is approved by Institutional Ethics Committee.},
  journal-iso                = {Comput. Hum. Behav.},
  keywords                   = {BCI; Electroencephalography; Deep learning; Type-2 fuzzy set; Emotion recognition; Android games},
  keywords-plus              = {AUTONOMIC NERVOUS-SYSTEM; VIDEO GAME; RECOGNITION; EEG; DESENSITIZATION; BEHAVIOR; STATES},
  language                   = {English},
  number-of-cited-references = {54},
  orcid-numbers              = {Ghosh, Lidia/0000-0002-5146-955X},
  priority                   = {prio3},
  publisher                  = {PERGAMON-ELSEVIER SCIENCE LTD},
  research-areas             = {Psychology},
  researcherid-numbers       = {于, 于增臣/AAH-4657-2021},
  times-cited                = {3},
  type                       = {Article},
  unique-id                  = {WOS:000609419700007},
  usage-count-last-180-days  = {5},
  usage-count-since-2013     = {24},
  web-of-science-categories  = {Psychology, Multidisciplinary; Psychology, Experimental},
  web-of-science-index       = {Social Science Citation Index (SSCI)},
}

@InProceedings{WOS:000502114100045,
  author                     = {Imbernon Cuadrado, Luis-Eduardo and Manjarres Riesco, Angeles and de la Paz Lopez, Felix},
  booktitle                  = {FROM BIOINSPIRED SYSTEMS AND BIOMEDICAL APPLICATIONS TO MACHINE LEARNING, PT II},
  title                      = {FER in Primary School Children for Affective Robot Tutors},
  year                       = {2019},
  address                    = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
  editor                     = {Vicente, JMF and AlvarezSanchez, JR and Lopez, FD and Moreo, JT and Adeli, H},
  note                       = {8th International Work-Conference on the Interplay Between Natural and Artificial Computation (IWINAC), Almeria, SPAIN, JUN 03-07, 2019},
  organization               = {Spanish CYTED; Red Nacl Computac Nat \& Artificial, Programa Grupos Excelencia Fundac Seneca \& Apliquem Microones 21 s l},
  pages                      = {461-471},
  publisher                  = {SPRINGER INTERNATIONAL PUBLISHING AG},
  series                     = {Lecture Notes in Computer Science},
  volume                     = {11487},
  status                     = {Aceito},
  abstract                   = {In the last few years, robotics has attracted much interest as a tool to
   support education through social interaction. Since Social-Emotional
   Learning (SEL) influences academic success, affective robot tutors have
   a great potential within education. In this article we report on our
   research in recognition of facial emotional expressions, aimed at
   improving ARTIE, an integrated environment for the development of
   affective robot tutors. A Full Convolutional Neural Network (FCNN) model
   has been trained with the Fer2013 dataset, and then validated with
   another dataset containing facial images of primary school children,
   which has been compiled during computing lab sessions. Our first
   prototype recognizes primary school children facial emotional
   expressions with 69,15\% accuracy. As a future work we intend to further
   refine the ARTIE Emotional Component with a view to integrating the main
   singularities of primary school children emotional expression.},
  affiliation                = {Cuadrado, LEI (Corresponding Author), Sopra Steria, Madrid, Spain. Lopez, FD (Corresponding Author), Univ Nacl Educ Distancia, Dept Artificial Intelligence, Madrid, Spain. Imbernon Cuadrado, Luis-Eduardo, Sopra Steria, Madrid, Spain. Manjarres Riesco, Angeles; de la Paz Lopez, Felix, Univ Nacl Educ Distancia, Dept Artificial Intelligence, Madrid, Spain.},
  author-email               = {imbernon@gmail.com amanja@dia.uned.es delapaz@dia.uned.es},
  da                         = {2022-09-28},
  doc-delivery-number        = {BO1QW},
  doi                        = {10.1007/978-3-030-19651-6\_45},
  eissn                      = {1611-3349},
  isbn                       = {978-3-030-19651-6; 978-3-030-19650-9},
  issn                       = {0302-9743},
  keywords                   = {Emotion recognition; Affective robot tutors; Facial emotional expression; Social emotional learning},
  keywords-plus              = {EMOTION},
  language                   = {English},
  number-of-cited-references = {21},
  orcid-numbers              = {de la Paz López, Félix/0000-0002-1530-159X Imbernon Cuadrado, Luis-Eduardo/0000-0002-9570-4419 Manjarres Riesco, Angeles/0000-0001-5441-3642},
  priority                   = {prio1},
  research-areas             = {Automation \& Control Systems; Computer Science; Engineering; Robotics},
  researcherid-numbers       = {de la Paz López, Félix/B-9958-2008},
  times-cited                = {1},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000502114100045},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {3},
  web-of-science-categories  = {Automation \& Control Systems; Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Engineering, Biomedical; Robotics},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@InProceedings{WOS:000494315600125,
  author                     = {Askari, Farzaneh and Feng, Haunghao and Sweeny, Timothy and Mahoor, Mohammad H.},
  booktitle                  = {2018 27TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (IEEE RO-MAN 2018)},
  title                      = {A Pilot Study on Facial Expression Recognition Ability of Autistic Children Using Ryan, A Rear-Projected Humanoid Robot},
  year                       = {2018},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  editor                     = {Cabibihan, JJ and Mastrogiovanni, F and Pandey, AK and Rossi, S and Staffa, M},
  note                       = {27th IEEE International Symposium on Robot and Human Interactive Communication (IEEE RO-MAN), Nanjing, PEOPLES R CHINA, AUG 27-31, 2018},
  organization               = {IEEE; IEEE Robot \& Automat Soc; Robot Soc Japan; Korea Robot Soc; Nanjing Forestry Univ},
  pages                      = {790-795},
  publisher                  = {IEEE},
  series                     = {IEEE RO-MAN},
  status                     = {Aceito},
  abstract                   = {Rear-projected robots use computer graphics technology to create facial
   animations and project them on a mask to show the robot's facial cues
   and expressions. These types of robots are becoming commercially
   available, though more research is required to understand how they can
   be effectively used as a socially assistive robotic agent. This paper
   presents the results of a pilot study on comparing the facial expression
   recognition abilities of children with Autism Spectrum Disorder (ASD)
   with typically developing (TD) children using a rear-projected humanoid
   robot called Ryan. Six children with ASD and six TD children
   participated in this research, where Ryan showed them six basic
   expressions (i.e. anger, disgust, fear, happiness, sadness, and
   surprise) with different intensity levels. Participants were asked to
   identify the expressions portrayed by Ryan. The results of our study
   show that there is not any general impairment in expression recognition
   ability of the ASD group comparing to the TD control group; however,
   both groups showed deficiencies in identifying disgust and fear.
   Increasing the intensity of Ryan's facial expressions significantly
   improved the expression recognition accuracy. Both groups were
   successful to recognize the expressions demonstrated by Ryan with high
   average accuracy.},
  affiliation                = {Askari, F (Corresponding Author), Univ Denver, Dept Elect \& Comp Engn, Denver, CO 80208 USA. Askari, Farzaneh; Feng, Haunghao; Mahoor, Mohammad H., Univ Denver, Dept Elect \& Comp Engn, Denver, CO 80208 USA. Sweeny, Timothy, Univ Denver, Dept Psychol, Denver, CO 80208 USA. Mahoor, Mohammad H., DreamFace Technol LLC, Denver, CO USA.},
  author-email               = {farzaneh.askari@du.edu howard.k.finn@gmail.com Timothy.Sweeny@du.edu mmahoor@du.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {BO0ZX},
  funding-acknowledgement    = {National Science Foundation {[}IIS-1450933, CNS-1427872]},
  funding-text               = {This research is partially supported by grants IIS-1450933 and CNS-1427872 from the National Science Foundation. We would like to thank Mr. Hojjat Abdollahi, a PhD student in the Computer Vision and Social Robotics Laboratory at the University of Denver, for his help to program the robot application.},
  isbn                       = {978-1-5386-7980-7},
  issn                       = {1944-9445},
  keywords-plus              = {PERCEPTION; EMOTIONS; DEFICITS},
  language                   = {English},
  number-of-cited-references = {33},
  oa                         = {Green Submitted},
  priority                   = {prio3},
  research-areas             = {Computer Science; Engineering; Robotics},
  times-cited                = {2},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000494315600125},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {3},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \& Electronic; Robotics},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@InProceedings{WOS:000598539700016,
  author                     = {Romanova, I and Alaverdyan, R.},
  booktitle                  = {INTERNATIONAL SCIENTIFIC AND PRACTICAL CONFERENCE MODELING IN EDUCATION 2019},
  title                      = {Automated Robotic System For Autism Treatment},
  year                       = {2019},
  address                    = {2 HUNTINGTON QUADRANGLE, STE 1NO1, MELVILLE, NY 11747-4501 USA},
  editor                     = {Tsvetkov, YB and Romanova, IK},
  note                       = {International Scientific and Practical Conference on Modeling in Education, Bauman Moscow State Tech Univ, Moscow, RUSSIA, JUN 19-21, 2019},
  publisher                  = {AMER INST PHYSICS},
  series                     = {AIP Conference Proceedings},
  volume                     = {2195},
  status                     = {Aceito},
  abstract                   = {This article is about possible implementation of automated robotic
   system for children's autism treatment. There are many well established
   approaches to autism treatment, however all of them are done manually by
   therapists and parents. In order to reduce manual work, we try to
   partially automate ABA method -, the one which has practically proved to
   be useful and scientifically well researched, - applied behaviour
   analysis based treatment. Simply put ABA, among others, includes
   time-consuming and laborious procedures which can be transferred into
   computational tasks. That allows us to apply computational algorithms
   from data mining and computer vision domain. Therefore, we propose that
   automation and further data intellectual analysis can increase
   productivity and effectiveness of applied method. In this work we first
   look at socio-economic and social aspects of issue, analyse frequently
   used methods and then based on specifics of autism and applied treatment
   we suggest basic workflow for automation of ABA method and data
   intellectual analysis. Workflow includes video and audio data
   acquisition and preprocessing for further emotion and behaviour
   recognition in correlation to external events. Similarly, suggested
   system allows monitoring and possible alteration of child's performance
   throughout the course of treatment. System implementation is heavily
   dependent on data mining and computer vision technologies, such as
   neural networks, clustering algorithms, video segmentation, feature
   extraction etc. Desired performance of overall system and its units is
   made possible by the function of feedback, on hardware and soft are
   level.},
  affiliation                = {Romanova, I (Corresponding Author), Bauman Moscow State Tech Univ, Moscow 105005, Russia. Romanova, I; Alaverdyan, R., Bauman Moscow State Tech Univ, Moscow 105005, Russia.},
  article-number             = {020016},
  author-email               = {irina.romanova@mail.ru rob188@mail.ru},
  da                         = {2022-09-28},
  doc-delivery-number        = {BQ5CN},
  doi                        = {10.1063/1.5140116},
  isbn                       = {978-0-7354-1946-9},
  issn                       = {0094-243X},
  keywords-plus              = {SPECTRUM DISORDERS; CHILDREN; METAANALYSIS},
  language                   = {English},
  number-of-cited-references = {22},
  orcid-numbers              = {Romanova, Irina/0000-0002-5757-350X},
  priority                   = {prio1},
  research-areas             = {Education \& Educational Research; Mathematics},
  researcherid-numbers       = {Romanova, Irina/GQI-3518-2022},
  times-cited                = {0},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000598539700016},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Education, Scientific Disciplines; Mathematics, Applied},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@InProceedings{WOS:000458669700022,
  author                     = {Rusli, Nazreen and Yusof, Hazlina Md and Sidek, Shahrul Naim and Ishak, Nor Izzati},
  booktitle                  = {2018 IEEE-EMBS CONFERENCE ON BIOMEDICAL ENGINEERING AND SCIENCES (IECBES)},
  title                      = {GLCM Correlation Approach for Blood Vessel Identification in Thermal Image},
  year                       = {2018},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), Kuching, MALAYSIA, DEC 03-06, 2018},
  organization               = {SARAWAK Convent Bur; Physiolog Measurement},
  pages                      = {112-116},
  publisher                  = {IEEE},
  series                     = {IEEE EMBS Conference on Biomedical Engineering and Sciences},
  status                     = {Aceito},
  abstract                   = {The maturity of detection in emotions via thermal camera is evolving
   recently since it is able to detect the hot parts of human face
   composition replicating the area of blood vessels. The notion of
   non-invasive tools for data gatherings via a thermal camera has also
   been vigorously highlighted. We hypothesize that, the impact of
   cutaneous temperature changes due to blood flows in the blood vessels
   could be correlated to specific emotion state for healthy as well as
   autistic children. The autistic children are less able to present
   emotion through facial expression. In this work, healthy children were
   assigned as subjects prior to the development of the algorithm for
   thermal imaging analysis to form a reference model. Facial thermal
   distribution was analyzed and a technique using Correlation in Gray
   Level Co-occurrence Matrices (GLCM) was proposed to determine the blood
   vessels' region. A k-Nearest Neighbor (k-NN) classifier shows a
   promising result for the proposed method and suggests that these
   analyses are momentous for distinguishing between five basic emotions
   and it could be used as non-verbal mediums to help on autistic children.},
  affiliation                = {Rusli, N (Corresponding Author), Int Islamic Univ Malaysia, Dept Mechatron Engn, Kuala Lumpur, Malaysia. Rusli, Nazreen; Yusof, Hazlina Md; Sidek, Shahrul Naim; Ishak, Nor Izzati, Int Islamic Univ Malaysia, Dept Mechatron Engn, Kuala Lumpur, Malaysia.},
  author-email               = {nazreen.rusli@live.iium.edu.my myhazlina@iium.edu.my snaim@iium.edu.my izzati.ishak@live.iium.edu.my},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BM0DK},
  funding-acknowledgement    = {Ministry of Higher Education (MOHE) under the Fundamental Research Grant Scheme (FRGS) {[}FRGS16-030-0529]},
  funding-text               = {We wish to express our gratitude to the Ministry of Higher Education (MOHE) for funding the project under the Fundamental Research Grant Scheme (FRGS), Grant no:FRGS16-030-0529},
  isbn                       = {978-1-5386-2471-5},
  issn                       = {2374-3220},
  keywords                   = {Autistic; Emotion; GLCM; Thermal Imaging; Texture Analysis},
  language                   = {English},
  number-of-cited-references = {14},
  oa                         = {Green Accepted},
  orcid-numbers              = {Sidek, Shahrul Naim/0000-0002-3204-1347},
  priority                   = {prio1},
  research-areas             = {Engineering},
  researcherid-numbers       = {Sidek, Shahrul Naim/A-5275-2015},
  times-cited                = {1},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000458669700022},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {0},
  web-of-science-categories  = {Engineering, Biomedical},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000848611200001,
  author                     = {Hu, Bo},
  journal                    = {INTERNATIONAL JOURNAL OF HUMANOID ROBOTICS},
  title                      = {Analysis of Art Therapy for Children with Autism by Using the Implemented Artificial Intelligence System},
  year                       = {2022},
  issn                       = {0219-8436},
  month                      = {JUN},
  number                     = {03},
  volume                     = {19},
  status                     = {Aceito},
  abstract                   = {Autism is a disease that manifests as social communication disorders and
   repetitive sensory movements. The current incidence of autism continues
   to rise globally, and the number of children with autism is also
   increasing. The treatment of autistic children in China is mainly
   intervention, but experienced autism instructors are lacking. Here,
   machine learning and artificial intelligence (AI) algorithms are adopted
   as the technical foundation to build a system that measures the
   emotional performance of autistic students in the classroom based on
   actual application scenarios. By analyzing the classroom learning data
   of autistic children, the students' emotions can be effectively judged
   to help teachers evaluate and track their classroom performance and
   reduce teachers' burden. Results demonstrate that when the matrix
   composed of frame sequences and key point coordinates is used as the
   input, the spatio-temporal graph convolutional network is determined as
   the principal model of action recognition, with an accuracy of 90\%, and
   the participation score can be obtained by calculating the action
   response time. In the experimental process of facial expression
   recognition, the random forest's classification accuracy of the feature
   point sequence based on images can reach 99\%. Therefore, the random
   forest is determined as the principal classifier for facial expression
   recognition. After the relationship between expression intensity,
   pleasure, and expression category is analyzed, the scoring method is
   designed. The experiment also discovers that painting can be a
   rehabilitation therapy for children with autism. The above results can
   provide a theoretical foundation for the treatment of autistic children.},
  address                    = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
  affiliation                = {Hu, B (Corresponding Author), Tianjin Heping Dist Wanquan Primary Sch Tianjin, Tianjin, Peoples R China. Hu, B (Corresponding Author), Chinese Acad Sci, Inst Psychol, Child Dev \& Educ Psychol, Beijing, Peoples R China. Hu, Bo, Tianjin Heping Dist Wanquan Primary Sch Tianjin, Tianjin, Peoples R China. Hu, Bo, Chinese Acad Sci, Inst Psychol, Child Dev \& Educ Psychol, Beijing, Peoples R China.},
  author-email               = {9820200073@nankai.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {4I6YB},
  doi                        = {10.1142/S0219843622400023},
  earlyaccessdate            = {APR 2022},
  eissn                      = {1793-6942},
  journal-iso                = {Int. J. Humanoid Robot.},
  keywords                   = {Children; autism; artificial intelligence; art therapy},
  keywords-plus              = {SPECTRUM DISORDERS; INTERVENTION},
  language                   = {English},
  number-of-cited-references = {39},
  priority                   = {prio3},
  publisher                  = {WORLD SCIENTIFIC PUBL CO PTE LTD},
  research-areas             = {Robotics},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000848611200001},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {0},
  web-of-science-categories  = {Robotics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{ WOS:000647216000001,
Author = {Dapogny, Arnaud and Grossard, Charline and Hun, Stephanie and Serret,
   Sylvie and Grynszpan, Ouriel and Dubuisson, Severine and Cohen, David
   and Bailly, Kevin},
Title = {On Automatically Assessing Children's Facial Expressions Quality: A
   Study, Database, and Protocol},
Journal = {FRONTIERS IN COMPUTER SCIENCE},
Year = {2019},
Volume = {1},
Month = {OCT 11},
Abstract = {While there exists a number of serious games geared toward helping
   children with ASD to produce facial expressions, most of them fail to
   provide a precise feedback to help children to adequately learn. In the
   scope of the JEMImE project, which aims at developing such serious game
   platform, we introduce throughout this paper a machine learning approach
   for discriminating between facial expressions and assessing the quality
   of the emotional display. In particular, we point out the limits in
   generalization capacities of models trained on adult subjects. To
   circumvent this issue in the design of our system, we gather a large
   database depicting children's facial expressions to train and validate
   the models. We describe our protocol to elicit facial expressions and
   obtain quality annotations, and empirically show that our models obtain
   high accuracies in both classification and quality assessment of
   children's facial expressions. Furthermore, we provide some insight on
   what the models learn and which features are the most useful to
   discriminate between the various facial expressions classes and
   qualities. This new model trained on the dedicated dataset has been
   integrated into a proof of concept of the serious game.},
Publisher = {FRONTIERS MEDIA SA},
Address = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Bailly, K (Corresponding Author), Sorbonne Univ, CNRS, Inst Systemes Intelligents \& Robot, ISIR, Paris, France.
   Dapogny, Arnaud; Grynszpan, Ouriel; Dubuisson, Severine; Bailly, Kevin, Sorbonne Univ, CNRS, Inst Systemes Intelligents \& Robot, ISIR, Paris, France.
   Grossard, Charline, GHU Pitie Salpetriere Charles Foix, AP HP, Serv Psychiat Enfant \& Adolescent, Paris, France.
   Hun, Stephanie; Serret, Sylvie; Cohen, David, Univ Nice Sophia Antipolis, Cognit Behav Technol CoBTeK, Nice, France.},
DOI = {10.3389/fcomp.2019.00005},
Article-Number = {5},
EISSN = {2624-9898},
Keywords = {facial expression recognition; expression quality; random forests;
   emotion; children; dataset},
Keywords-Plus = {FACE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications},
Author-Email = {kevin.bailly@sorbonne-universite.fr},
Funding-Acknowledgement = {French National Agency (ANR) {[}ANR-13-CORD-0004, ANR-17-CE33-0002]},
Funding-Text = {This work has been supported by the French National Agency (ANR) in the
   frame of its Technological Research CONTINT program (JEMImE, project
   number ANR-13-CORD-0004) and JCJC program (FACIL, project
   ANR-17-CE33-0002).},
Number-of-Cited-References = {26},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Front. Comput. Sci.-Switz},
Doc-Delivery-Number = {VJ9PP},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000647216000001},
OA = {gold},
DA = {2022-09-28},
}

@Article{WOS:000656125700001,
  author                     = {Doi, Hirokazu and Tsumura, Norimichi and Kanai, Chieko and Masui, Kenta and Mitsuhashi, Ryota and Nagasawa, Takumi},
  journal                    = {FRONTIERS IN PSYCHIATRY},
  title                      = {Automatic Classification of Adult Males With and Without Autism Spectrum Disorder by Non-contact Measurement of Autonomic Nervous System Activation},
  year                       = {2021},
  issn                       = {1664-0640},
  month                      = {MAY 17},
  volume                     = {12},
  status                     = {Aceito},
  abstract                   = {People with autism spectrum disorder (ASD) exhibit atypicality in
   various domains of behavior. Previous psychophysiological studies have
   revealed an atypical pattern of autonomic nervous system (ANS)
   activation induced by psychosocial stimulation. Thus, it might be
   feasible to develop a novel assessment tool to evaluate the risk of ASD
   by measuring ANS activation in response to emotional stimulation. The
   present study investigated whether people with ASD could be
   automatically classified from neurotypical adults based solely on
   physiological data obtained by the recently introduced non-contact
   measurement of pulse wave. We video-recorded faces of adult males with
   and without ASD while watching emotion-inducing video clips. Features
   reflective of ANS activation were extracted from the temporal
   fluctuation of facial skin coloration and entered into a
   machine-learning algorithm. Though the performance was modest, the
   gradient boosting classifier succeeded in classifying people with and
   without ASD, which indicates that facial skin color fluctuation contains
   information useful for detecting people with ASD. Taking into
   consideration the fact that the current study recruited only
   high-functioning adults who have relatively mild symptoms and probably
   developed some compensatory strategies, ASD screening by non-contact
   measurement of pulse wave could be a promising assessment tool to
   evaluate ASD risk.},
  address                    = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
  affiliation                = {Doi, H (Corresponding Author), Kokushikan Univ, Sch Sci \& Engn, Grad Sch Engn, Tokyo, Japan. Doi, H (Corresponding Author), Nagasaki Univ, Grad Sch Biomed Sci, Nagasaki, Japan. Doi, Hirokazu, Kokushikan Univ, Sch Sci \& Engn, Grad Sch Engn, Tokyo, Japan. Doi, Hirokazu, Nagasaki Univ, Grad Sch Biomed Sci, Nagasaki, Japan. Tsumura, Norimichi; Masui, Kenta; Mitsuhashi, Ryota; Nagasawa, Takumi, Chiba Univ, Grad Sch Engn, Chiba, Japan. Kanai, Chieko, Showa Univ, Med Inst Dev Disabil Res, Tokyo, Japan. Kanai, Chieko, Wayo Womens Univ, Fac Humanities, Chiba, Japan.},
  article-number             = {625978},
  author-email               = {hdoi\_0407@yahoo.co.jp},
  da                         = {2022-09-28},
  doc-delivery-number        = {SK3NO},
  doi                        = {10.3389/fpsyt.2021.625978},
  funding-acknowledgement    = {Japan Society for the Promotion of Science (JSPS) {[}17K01904]; Joint Usage/Research Program of Medical Institute of Developmental Disabilities Research, Showa University},
  funding-text               = {This research was financed by Japan Society for the Promotion of Science (JSPS) Grant-in-Aid for Scientific Research(C) Grant No. 17K01904 to HD and was supported by a grant from the Joint Usage/Research Program of Medical Institute of Developmental Disabilities Research, Showa University to HD.},
  journal-iso                = {Front. Psychiatry},
  keywords                   = {ASD; autonomic nervous system; emotion; non-contact measurement; digital phenotyping; pulse wave; color},
  keywords-plus              = {HEART-RATE-VARIABILITY; ALEXITHYMIA; CHILDREN; RECOGNITION; PREVALENCE; MECHANISMS; RESPONSES},
  language                   = {English},
  number-of-cited-references = {57},
  oa                         = {gold, Green Published},
  priority                   = {prio3},
  publisher                  = {FRONTIERS MEDIA SA},
  research-areas             = {Psychiatry},
  times-cited                = {3},
  type                       = {Article},
  unique-id                  = {WOS:000656125700001},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {7},
  web-of-science-categories  = {Psychiatry},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000620958700001,
  author                     = {Feng, Yulong and Xiao, Wei and Wu, Teng and Zhang, Jianwei and Xiang, Jing and Guo, Hong},
  journal                    = {COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE},
  title                      = {A New Recognition Method for the Auditory Evoked Magnetic Fields},
  year                       = {2021},
  issn                       = {1687-5265},
  month                      = {FEB 11},
  volume                     = {2021},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Magnetoencephalography (MEG) is a persuasive tool to study the human
   brain in physiology and psychology. It can be employed to obtain the
   inference of change between the external environment and the internal
   psychology, which requires us to recognize different single trial
   event-related magnetic fields (ERFs) originated from different
   functional areas of the brain. Current recognition methods for the
   single trial data are mainly used for event-related potentials (ERPs) in
   the electroencephalography (EEG). Although the MEG shares the same
   signal sources with the EEG, much less interference from the other brain
   tissues may give the MEG an edge in recognition of the ERFs. In this
   work, we propose a new recognition method for the single trial auditory
   evoked magnetic fields (AEFs) through enhancing the signal. We find that
   the signal strength of the single trial AEFs is concentrated in the
   primary auditory cortex of the temporal lobe, which can be clearly
   displayed in the 2D images. These 2D images are then recognized by an
   artificial neural network (ANN) with 100\% accuracy, which realizes the
   automatic recognition for the single trial AEFs. The method not only may
   be combined with the source estimation algorithm to improve its accuracy
   but also paves the way for the implementation of the brain-computer
   interface (BCI) with the MEG.},
  address                    = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
  affiliation                = {Wu, T; Guo, H (Corresponding Author), Peking Univ, State Key Lab Adv Opt Commun Syst \& Networks, Dept Elect, Beijing 100871, Peoples R China. Wu, T; Guo, H (Corresponding Author), Peking Univ, Ctr Quantum Informat Technol, Beijing 100871, Peoples R China. Feng, Yulong; Xiao, Wei; Wu, Teng; Guo, Hong, Peking Univ, State Key Lab Adv Opt Commun Syst \& Networks, Dept Elect, Beijing 100871, Peoples R China. Feng, Yulong; Xiao, Wei; Wu, Teng; Guo, Hong, Peking Univ, Ctr Quantum Informat Technol, Beijing 100871, Peoples R China. Zhang, Jianwei, Peking Univ, Sch Phys, Beijing 100871, Peoples R China. Xiang, Jing, Cincinnati Childrens Hosp Med Ctr, Div Neurol, MEG Ctr, Cincinnati, OH 45229 USA.},
  article-number             = {6645270},
  author-email               = {fylong@pku.edu.cn xiao\_wei@pku.edu.cn wuteng@pku.edu.cn james@pku.edu.cn jing.xiang@cchmc.org hongguo@pku.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {QL3CU},
  doi                        = {10.1155/2021/6645270},
  eissn                      = {1687-5273},
  funding-acknowledgement    = {National Natural Science Foundation of China {[}61571018, 61531003, 91436210]; National Key Research and Development Program},
  funding-text               = {This project was supported by the National Natural Science Foundation of China (61571018, 61531003, and 91436210) and the National Key Research and Development Program. The authors are grateful for the open data source in Brainstorm software, and its website is http://neuroimage.usc.edu/brainstorm.},
  journal-iso                = {Comput. Intell. Neurosci.},
  keywords-plus              = {EVENT-RELATED POTENTIALS; CONTINUOUS WAVELET TRANSFORM; MAGNETOENCEPHALOGRAPHY; DIFFERENTIATION; GENERATION; ARTIFACTS; ATTENTION; EMOTION},
  language                   = {English},
  number-of-cited-references = {63},
  oa                         = {Green Published, gold},
  orcid-numbers              = {Wu, Teng/0000-0002-7707-7474},
  publisher                  = {HINDAWI LTD},
  research-areas             = {Mathematical \& Computational Biology; Neurosciences \& Neurology},
  researcherid-numbers       = {Guo, Hong/B-1428-2013 Wu, Teng/G-2255-2019},
  times-cited                = {1},
  type                       = {Article},
  unique-id                  = {WOS:000620958700001},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {10},
  web-of-science-categories  = {Mathematical \& Computational Biology; Neurosciences},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000486598700001,
  author                     = {Vaidya, Chandan J. and You, Xiaozhen and Mostofsky, Stewart and Pereira, Francisco and Berl, Madison M. and Kenworthy, Lauren},
  journal                    = {JOURNAL OF CHILD PSYCHOLOGY AND PSYCHIATRY},
  title                      = {Data-driven identification of subtypes of executive function across typical development, attention deficit hyperactivity disorder, and autism spectrum disorders},
  year                       = {2020},
  issn                       = {0021-9630},
  month                      = {JAN},
  number                     = {1},
  pages                      = {51-61},
  volume                     = {61},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Background Impairment of executive function (EF), the goal-directed
   regulation of thoughts, actions, and emotions, drives negative outcomes
   and is common across neurodevelopmental disorders including attention
   deficit hyperactivity disorder (ADHD) and autism spectrum disorder
   (ASD). A primary challenge to its amelioration is heterogeneity in
   symptom expression within and across disorders. Parsing this
   heterogeneity is necessary to attain diagnostic precision, a goal of the
   NIMH Research Domain Criteria Initiative. We aimed to identify
   transdiagnostic subtypes of EF that span the normal to impaired spectrum
   and establish their predictive and neurobiological validity. Methods
   Community detection was applied to clinical parent-report measures in
   8-14-year-old children with and without ADHD and ASD from two
   independent cohorts (discovery N = 320; replication N = 692) to identify
   subgroups with distinct behavioral profiles. Support vector machine
   (SVM) classification was used to predict subgroup membership of unseen
   cases. Preliminary neurobiological validation was obtained with existing
   functional magnetic resonance imaging (fMRI) data on a subsample (N =
   84) by testing hypotheses about sensitivity of EF subgroups versus DSM
   categories. Results We observed three transdiagnostic EF subtypes
   characterized by behavioral profiles that were defined by relative
   weakness in: (a) flexibility and emotion regulation; (b) inhibition; and
   (c) working memory, organization, and planning. The same tripartite
   structure was also present in the typically developing children. SVM
   trained on the discovery sample and tested on the replication sample
   classified subgroup membership with 77.0\% accuracy. Split-half SVM
   classification on the combined sample (N = 1,012) yielded 88.9\%
   accuracy (this SVM is available for public use). As hypothesized,
   frontal-parietal engagement was better distinguished by EF subtype than
   DSM diagnosis and the subgroup characterized with inflexibility failed
   to modulate right IPL activation in response to increased executive
   demands. Conclusions The observed transdiagnostic subtypes refine
   current diagnostic nosology and augment clinical decision-making for
   personalizing treatment of executive dysfunction in children.},
  address                    = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
  affiliation                = {Vaidya, CJ (Corresponding Author), Georgetown Univ, Dept Psychol, 306 White Gravenor, Washington, DC 20057 USA. Vaidya, Chandan J., Georgetown Univ, Dept Psychol, 306 White Gravenor, Washington, DC 20057 USA. Vaidya, Chandan J.; You, Xiaozhen; Berl, Madison M.; Kenworthy, Lauren, Childrens Natl Hlth Syst, Childrens Res Inst, Washington, DC USA. Mostofsky, Stewart, Kennedy Krieger Inst, Ctr Neurodev \& Imaging Res, Baltimore, MD USA. Pereira, Francisco, NIMH, Machine Learning Team, Bethesda, MD 20892 USA.},
  author-email               = {cjv2@georgetown.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {KW9FO},
  doi                        = {10.1111/jcpp.13114},
  earlyaccessdate            = {SEP 2019},
  eissn                      = {1469-7610},
  funding-acknowledgement    = {NIH {[}MH110512, MH084961, MH065395, MH078160, MH085328, NS048527, HD040677-07, 1U54HD090257]},
  funding-text               = {Funding support by NIH: MH110512, MH084961, MH065395, MH078160, MH085328, NS048527, HD040677-07 and 1U54HD090257. L.K. receives royalties from the sale of the Behavior Rating Inventory of Executive Function. The remaining authors have declared that they have no competing or potential conflicts of interest.},
  journal-iso                = {J. Child Psychol. Psychiatry},
  keywords                   = {Attention deficit hyperactivity disorder; autism spectrum disorders; functional MRI (fMRI); individual differences; machine learning},
  keywords-plus              = {PERFORMANCE-BASED MEASURES; DEFICIT/HYPERACTIVITY DISORDER; ADHD SYMPTOMS; CHILDREN; CHILDHOOD; ADOLESCENCE; BEHAVIOR; INDIVIDUALS; IMPAIRMENTS; DIVERSITY},
  language                   = {English},
  number-of-cited-references = {52},
  oa                         = {Green Accepted},
  orcid-numbers              = {You, Xiaozhen/0000-0003-3592-0985},
  publisher                  = {WILEY},
  research-areas             = {Psychology; Psychiatry},
  researcherid-numbers       = {Kenworthy, Lauren/AAI-7572-2020 Hassan, Randa/AAW-3624-2021 You, Xiaozhen/AAD-4583-2019},
  times-cited                = {29},
  type                       = {Article},
  unique-id                  = {WOS:000486598700001},
  usage-count-last-180-days  = {12},
  usage-count-since-2013     = {25},
  web-of-science-categories  = {Psychology, Developmental; Psychiatry; Psychology},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000652780200001,
  author                     = {Tang, Tong Boon and Chong, Jie Sheng and Kiguchi, Masashi and Funane, Tsukasa and Lu, Cheng-Kai},
  journal                    = {IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING},
  title                      = {Detection of Emotional Sensitivity Using fNIRS Based Dynamic Functional Connectivity},
  year                       = {2021},
  issn                       = {1534-4320},
  pages                      = {894-904},
  volume                     = {29},
  status                     = {Rejeitado - Escopo},
  abstract                   = {In this study, we proposed an analytical framework to identify dynamic
   task-based functional connectivity (FC) features as new biomarkers of
   emotional sensitivity in nursing students, by using a combination of
   unsupervised and supervised machine learning techniques. The dynamic FC
   was measured by functional Near-Infrared Spectroscopy (fNIRS), and
   computed using a sliding window correlation (SWC) analysis. A k-means
   clustering technique was applied to derive four recurring connectivity
   states. The states were characterized by both graph theory and
   semi-metric analysis. Occurrence probability and state transition were
   extracted as dynamic FC network features, and a Random Forest (RF)
   classifier was implemented to detect emotional sensitivity. The proposed
   method was trialled on 39 nursing students and 19 registered nurses
   during decision-making, where we assumed registered nurses have
   developed strategies to cope with emotional sensitivity. Emotional
   stimuli were selected from International Affective Digitized Sound
   System (IADS) database. Experiment results showed that registered nurses
   demonstrated single dominant connectivity state of task-relevance, while
   nursing students displayed in two states and had higher level of
   task-irrelevant state connectivity. The results also showed that
   students were more susceptive to emotional stimuli, and the derived
   dynamic FC features provided a stronger discriminating power than heart
   rate variability (accuracy of 81.65\% vs 71.03\%) as biomarkers of
   emotional sensitivity. This work forms the first study to demonstrate
   the stability of fNIRS based dynamic FC states as a biomarker. In
   conclusion, the results support that the state distribution of dynamic
   FC could help reveal the differentiating factors between the nursing
   students and registered nurses during decision making, and it is
   anticipated that the biomarkers might be used as indicators when
   developing professional training related to emotional sensitivity.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Tang, TB; Chong, JS (Corresponding Author), Univ Teknol PETRONAS, CISIR, Bandar Seri Iskandar 32610, Malaysia. Tang, Tong Boon; Chong, Jie Sheng; Lu, Cheng-Kai, Univ Teknol PETRONAS, CISIR, Bandar Seri Iskandar 32610, Malaysia. Kiguchi, Masashi; Funane, Tsukasa, Hitachi Ltd, Res \& Dev Grp, Ctr Exploratory Res, Tokyo 1858601, Japan.},
  author-email               = {tongboon.tang@utp.edu.my cjs\_chong@hotmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {SF5FA},
  doi                        = {10.1109/TNSRE.2021.3078460},
  eissn                      = {1558-0210},
  funding-acknowledgement    = {Ministry of Higher Education, Malaysia, under Higher Institution Centre of Excellence (HICoE) Scheme},
  funding-text               = {This work was supported by the Ministry of Higher Education, Malaysia, under Higher Institution Centre of Excellence (HICoE) Scheme awarded to Centre for Intelligent Signal and Imaging Research (CISIR).},
  journal-iso                = {IEEE Trans. Neural Syst. Rehabil. Eng.},
  keywords                   = {Sensitivity; Medical services; Task analysis; Feature extraction; Heart rate variability; Emotion recognition; Biomarkers; Emotional sensitivity; dynamic functional connectivity; functional Near-Infrared Spectroscopy; Random Forest; heart rate variability},
  keywords-plus              = {GLOBAL SIGNAL; DISORDER; NETWORKS; BRAIN; STATE; VARIABILITY; CHILDREN; STRESS},
  language                   = {English},
  number-of-cited-references = {67},
  oa                         = {hybrid},
  orcid-numbers              = {Funane, Tsukasa/0000-0001-6819-0320 CHONG, JIE SHENG/0000-0002-7513-5124 Lu, ChengKai/0000-0002-5819-0754 Kiguchi, Masashi/0000-0002-9513-7792 Tang, Tong Boon/0000-0002-5721-6828},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Engineering; Rehabilitation},
  researcherid-numbers       = {Funane, Tsukasa/E-9634-2011 Kiguchi, Masashi/P-7697-2018},
  times-cited                = {3},
  type                       = {Article},
  unique-id                  = {WOS:000652780200001},
  usage-count-last-180-days  = {8},
  usage-count-since-2013     = {22},
  web-of-science-categories  = {Engineering, Biomedical; Rehabilitation},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000449007900002,
  author                     = {Simoes, Marco and Monteiro, Raquel and Andrade, Joao and Mouga, Susana and Franca, Felipe and Oliveira, Guiomar and Carvalho, Paulo and Castelo-Branco, Miguel},
  journal                    = {FRONTIERS IN NEUROSCIENCE},
  title                      = {A Novel Biomarker of Compensatory Recruitment of Face Emotional Imagery Networks in Autism Spectrum Disorder},
  year                       = {2018},
  issn                       = {1662-453X},
  month                      = {NOV 1},
  volume                     = {12},
  status                     = {Aceito},
  abstract                   = {Imagery of facial expressions in Autism Spectrum Disorder (ASD) is
   likely impaired but has been very difficult to capture at a
   neurophysiological level. We developed an approach that allowed to
   directly link observation of emotional expressions and imagery in ASD,
   and to derive biomarkers that are able to classify abnormal imagery in
   ASD. To provide a handle between perception and action imagery cycles it
   is important to use visual stimuli exploring the dynamical nature of
   emotion representation. We conducted a case-control study providing a
   link between both visualization and mental imagery of dynamic facial
   expressions and investigated source responses to pure face-expression
   contrasts. We were able to replicate the same highly group
   discriminative neural signatures during action observation (dynamical
   face expressions) and imagery, in the precuneus. Larger activation in
   regions involved in imagery for the ASD group suggests that this effect
   is compensatory. We conducted a machine learning procedure to
   automatically identify these group differences, based on the EEG
   activity during mental imagery of facial expressions. We compared two
   classifiers and achieved an accuracy of 81\% using 15 features (both
   linear and non-linear) of the signal from theta, high-beta and gamma
   bands extracted from right-parietal locations (matching the precuneus
   region), further confirming the findings regarding standard statistical
   analysis. This robust classification of signals resulting from imagery
   of dynamical expressions in ASD is surprising because it far and
   significantly exceeds the good classification already achieved with
   observation of neutral face expressions (74\%). This novel neural
   correlate of emotional imagery in autism could potentially serve as a
   clinical interventional target for studies designed to improve facial
   expression recognition, or at least as an intervention biomarker.},
  address                    = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
  affiliation                = {Castelo-Branco, M (Corresponding Author), Univ Coimbra, Coimbra Inst Biomed Imaging \& Translat Res, Inst Ciencias Nucl Aplicadas Saude, Coimbra, Portugal. Castelo-Branco, M (Corresponding Author), Univ Coimbra, Fac Med, Coimbra, Portugal. Simoes, Marco; Monteiro, Raquel; Mouga, Susana; Oliveira, Guiomar; Castelo-Branco, Miguel, Univ Coimbra, Coimbra Inst Biomed Imaging \& Translat Res, Inst Ciencias Nucl Aplicadas Saude, Coimbra, Portugal. Simoes, Marco; Monteiro, Raquel; Andrade, Joao; Mouga, Susana; Oliveira, Guiomar; Castelo-Branco, Miguel, Univ Coimbra, Fac Med, Coimbra, Portugal. Simoes, Marco; Carvalho, Paulo, Univ Coimbra, Ctr Informat \& Syst, Coimbra, Portugal. Mouga, Susana; Oliveira, Guiomar, Ctr Hosp, Hosp Pediat, Neurodev \& Autism Unit, Child Dev Ctr, Coimbra, Portugal. Mouga, Susana; Oliveira, Guiomar, Univ Coimbra, Coimbra, Portugal. Franca, Felipe, Univ Fed Rio de Janeiro, COPPE, FESC, Rio De Janeiro, Brazil. Oliveira, Guiomar, Univ Coimbra, Univ Clin Pediat, Fac Med, Coimbra, Portugal. Oliveira, Guiomar, Ctr Hosp, Hosp Pediat, Ctr Invest \& Formacao Clin, Coimbra, Portugal.},
  article-number             = {791},
  author-email               = {mcbranco@med.uc.pt},
  da                         = {2022-09-28},
  doc-delivery-number        = {GY9XZ},
  doi                        = {10.3389/fnins.2018.00791},
  funding-acknowledgement    = {FCT - Portuguese national funding agency for science, research and technology {[}POCI-01-0145-FEDER-016428, CENTRO-01-0145-FEDER-000016, FCT-UID/4539/2013 - COMPETE, POCI-01-0145-FEDER-007440, POCI-01-0145-FEDER-30852, SFRH/BD/77044/2011, SFRH/BD/102779/2014]; BRAINTRAIN Project - Taking imaging into the therapeutic domain: Self-regulation of brain systems for mental disorders - FP7 HEALTH 2013 INNOVATION {[}1 602186 20]},
  funding-text               = {This work was supported by FCT - Portuguese national funding agency for science, research and technology {[}Grants PAC MEDPERSYST, POCI-01-0145-FEDER-016428, BIGDATIMAGE, CENTRO-01-0145-FEDER-000016 financed by Centro 2020 FEDER, COMPETE, FCT-UID/4539/2013 - COMPETE, POCI-01-0145-FEDER-007440, POCI-01-0145-FEDER-30852, Fellowships SFRH/BD/77044/2011 and SFRH/BD/102779/2014, and the BRAINTRAIN Project - Taking imaging into the therapeutic domain: Self-regulation of brain systems for mental disorders - FP7 HEALTH 2013 INNOVATION 1 602186 20, 2013, FLAD Life Sciences 2016.},
  journal-iso                = {Front. Neurosci.},
  keywords                   = {emotional facial expression; mental imagery; EEG biomarker; machine learning; autism spectrum disorder; dynamic expressions},
  keywords-plus              = {FACIAL EXPRESSIONS; CLINICAL-APPLICATIONS; MENTAL-IMAGERY; BRAIN; SELF; MIND; CONNECTIVITY; PERSPECTIVE; FMRI; PERCEPTION},
  language                   = {English},
  number-of-cited-references = {72},
  oa                         = {gold, Green Published},
  orcid-numbers              = {Castelo-Branco, Miguel/0000-0003-4364-6373 França, Felipe/0000-0002-8980-6208 Simões, Marco/0000-0003-3713-2464 Oliveira, Guiomar/0000-0002-7049-1277 França, Felipe M. G./0000-0002-8980-6208 Mouga, Susana/0000-0003-1072-4208},
  priority                   = {prio3},
  publisher                  = {FRONTIERS MEDIA SA},
  research-areas             = {Neurosciences \& Neurology},
  researcherid-numbers       = {Castelo-Branco, Miguel/F-3866-2019 França, Felipe/AAC-4302-2021 Simões, Marco/U-3815-2019 Oliveira, Guiomar/AAF-3911-2021 França, Felipe M. G./J-5910-2012 Mouga, Susana/AAF-6690-2020 Mouga, Susana/AAE-3497-2022},
  times-cited                = {13},
  type                       = {Article},
  unique-id                  = {WOS:000449007900002},
  usage-count-last-180-days  = {4},
  usage-count-since-2013     = {12},
  web-of-science-categories  = {Neurosciences},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000439557000051,
  author                     = {Bi, Kun and Chattun, Mohammad Ridwan and Liu, Xiaoxue and Wang, Qiang and Tian, Shui and Zhang, Siqi and Lu, Qing and Yao, Zhijian},
  journal                    = {JOURNAL OF AFFECTIVE DISORDERS},
  title                      = {Abnormal early dynamic individual patterns of functional networks in low gamma band for depression recognition},
  year                       = {2018},
  issn                       = {0165-0327},
  month                      = {OCT 1},
  pages                      = {366-374},
  volume                     = {238},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Background: The functional networks are associated with emotional
   processing in depression. The mapping of dynamic spatio-temporal brain
   networks is used to explore individual performance during early negative
   emotional processing. However, the dysfunctions of functional networks
   in low gamma band and their discriminative potentialities during early
   period of emotional face processing remain to be explored.
   Methods: Functional brain networks were constructed from the MEG
   recordings of 54 depressed patients and 54 controls in low gamma band
   (30-48 Hz). Dynamic connectivity regression (DCR) algorithm analyzed the
   individual change points of time series in response to emotional stimuli
   and constructed individualized spatiotemporal patterns. The nodal
   characteristics of patterns were calculated and fed into support vector
   machine (SVM). Performance of the classification algorithm in low gamma
   band was validated by dynamic topological characteristics of individual
   patterns in comparison to alpha and beta band.
   Results: The best discrimination accuracy of individual spatio-temporal
   patterns was 91.01\% in low gamma band. Individual temporal patterns had
   better results compared to group-averaged temporal patterns in all
   bands. The most important discriminative networks included affective
   network (AN) and fronto-parietal network (FPN) in low gamma band.
   Limitations: The sample size is relatively small. High gamma band was
   not considered.
   Conclusions: The abnormal dynamic functional networks in low gamma band
   during early emotion processing enabled depression recognition. The
   individual information processing is crucial in the discovery of
   abnormal spatio-temporal patterns in depression during early negative
   emotional processing. Individual spatio-temporal patterns may reflect
   the real dynamic function of subjects while group-averaged data may
   neglect some individual information.},
  address                    = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  affiliation                = {Lu, Q (Corresponding Author), Southeast Univ, Sch Biol Sci \& Med Engn, Nanjing 210096, Jiangsu, Peoples R China. Lu, Q (Corresponding Author), Southeast Univ, Key Lab Child Dev \& Learning Sci, Nanjing 210096, Jiangsu, Peoples R China. Yao, ZJ (Corresponding Author), Nanjing Med Univ, Affiliated Nanjing Brain Hosp, Dept Psychiat, Nanjing 210029, Jiangsu, Peoples R China. Yao, ZJ (Corresponding Author), Nanjing Univ, Nanjing Brain Hosp, Med Sch, Nanjing 210093, Jiangsu, Peoples R China. Bi, Kun; Tian, Shui; Zhang, Siqi; Lu, Qing, Southeast Univ, Sch Biol Sci \& Med Engn, Nanjing 210096, Jiangsu, Peoples R China. Bi, Kun; Tian, Shui; Zhang, Siqi; Lu, Qing, Southeast Univ, Key Lab Child Dev \& Learning Sci, Nanjing 210096, Jiangsu, Peoples R China. Chattun, Mohammad Ridwan; Liu, Xiaoxue; Yao, Zhijian, Nanjing Med Univ, Affiliated Nanjing Brain Hosp, Dept Psychiat, Nanjing 210029, Jiangsu, Peoples R China. Wang, Qiang; Yao, Zhijian, Nanjing Univ, Nanjing Brain Hosp, Med Sch, Nanjing 210093, Jiangsu, Peoples R China.},
  author-email               = {luq@seu.edu.cn zjyao@njmu.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {GN9VF},
  doi                        = {10.1016/j.jad.2018.05.078},
  eissn                      = {1573-2517},
  funding-acknowledgement    = {National High-tech Research and Development Program of China {[}2015AA020509]; National Natural Science Foundation of China {[}81571639, 81371522, 61372032]; Clinical Medicine Technology Foundation of Jiangsu Province {[}BL2014009]},
  funding-text               = {The work was supported by the grants of: the National High-tech Research and Development Program of China (2015AA020509); the National Natural Science Foundation of China (81571639, 81371522, 61372032); the Clinical Medicine Technology Foundation of Jiangsu Province (BL2014009).},
  journal-iso                = {J. Affect. Disord.},
  keywords                   = {Dynamic connectivity regression; MEG; Depression; Individual dynamic patterns; Gamma band},
  keywords-plus              = {HUMAN BRAIN; OSCILLATORY ACTIVITY; TIME-COURSE; CONNECTIVITY; SYNCHRONIZATION; DISORDER; FACES; DYSFUNCTION; SYMPTOMS; DEFICITS},
  language                   = {English},
  number-of-cited-references = {73},
  orcid-numbers              = {Chattun, Mohammad Ridwan/0000-0001-5961-5083},
  publisher                  = {ELSEVIER},
  research-areas             = {Neurosciences \& Neurology; Psychiatry},
  times-cited                = {11},
  type                       = {Article},
  unique-id                  = {WOS:000439557000051},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {35},
  web-of-science-categories  = {Clinical Neurology; Psychiatry},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000707073100001,
  author                     = {Khullar, Vikas and Singh, Harjit Pal and Bala, Manju},
  journal                    = {APPLIED ARTIFICIAL INTELLIGENCE},
  title                      = {Meltdown/Tantrum Detection System for Individuals with Autism Spectrum Disorder},
  year                       = {2021},
  issn                       = {0883-9514},
  month                      = {DEC 15},
  number                     = {15},
  pages                      = {1708-1732},
  volume                     = {35},
  abstract                   = {The intensive and explosive behavioral problems associated with Autism
   Spectrum Disorder (ASD) are treated as `meltdown or tantrum,' and it may
   lead to hyperactivity, impulsivity, aggression, self-injury, and
   irritability. The present work aims to propose and implement a
   noninvasive real-time deep learning based Meltdown/Tantrum Detection
   System (MTDS) for ASD individuals. The noninvasive physiological signals
   (such that heart rate, skin temperature, and galvanic skin response)
   were synthetically recorded with a specially designed hardware
   prototype. The recorded physiological signals were transmitted to an
   internet connected server where deep learning algorithms such as CNN,
   LSTM, and CNN-LSTM based Meltdown/Tantrum Detection System (MTDS) were
   implemented. The trained deep learning model was capable of detecting
   abnormal states of meltdown or tantrum through real-time received
   physiological signals. The proposed MTDS system was trained and tested
   with deep learning algorithms such as CNN, LSTM and hybrid CNN-LSTM, and
   it was found that hybrid CNN-LSTM was outperformed with an average
   training and testing accuracy of 96\% with low MAE (0.10 for training
   and 0.04 for testing). Furthermore, 86\% of the ASD caregivers favored
   the proposed MTDS system.},
  address                    = {530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA},
  affiliation                = {Khullar, V (Corresponding Author), Chitkara Univ, Chitkara Univ Inst Engn \& Technol, Rajpura, Punjab, India. Khullar, Vikas, Chitkara Univ, Chitkara Univ Inst Engn \& Technol, Rajpura, Punjab, India. Khullar, Vikas; Singh, Harjit Pal; Bala, Manju, IKG Punjab Tech Univ, Kapurthala, Punjab, India. Singh, Harjit Pal, CT Inst Engn Management \& Technol, Lambri, Punjab, India. Bala, Manju, Khalsa Coll Engn \& Technol, Amritsar, Punjab, India.},
  author-email               = {vikas.khullar@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {0W9BU},
  doi                        = {10.1080/08839514.2021.1991115},
  earlyaccessdate            = {OCT 2021},
  eissn                      = {1087-6545},
  journal-iso                = {Appl. Artif. Intell.},
  keywords-plus              = {EMOTION RECOGNITION; CHILDREN; RESPONSES; TECHNOLOGIES},
  language                   = {English},
  number-of-cited-references = {46},
  oa                         = {Bronze},
  orcid-numbers              = {KHULLAR, VIKAS/0000-0002-0404-3652},
  publisher                  = {TAYLOR \& FRANCIS INC},
  research-areas             = {Computer Science; Engineering},
  researcherid-numbers       = {Singh, Dr Harjit Pal/GPW-8215-2022},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000707073100001},
  usage-count-last-180-days  = {4},
  usage-count-since-2013     = {7},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \& Electronic},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@InProceedings{WOS:000465363900052,
  author                     = {Xu Yijia and Allan, Hasegawa Johnson Mark and Nancy, McElwain L.},
  booktitle                  = {19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING MARKETS IN MULTILINGUAL SOCIETIES},
  title                      = {Infant emotional outbursts detection in infant-parent spoken interactions},
  year                       = {2018},
  address                    = {C/O EMMANUELLE FOXONET, 4 RUE DES FAUVETTES, LIEU DIT LOUS TOURILS, BAIXAS, F-66390, FRANCE},
  note                       = {19th Annual Conference of the International-Speech-Communication-Association (INTERSPEECH 2018), Hyderabad, INDIA, AUG 02-SEP 06, 2018},
  organization               = {Int Speech Commun Assoc},
  pages                      = {242-246},
  publisher                  = {ISCA-INT SPEECH COMMUNICATION ASSOC},
  series                     = {Interspeech},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Detection of infant emotional outbursts, such as crying, in large
   corpora of recorded infant speech, is essential to the study of dyadic
   social process, by which infants learn to identify and regulate their
   own emotions. Such large corpora now exist with the advent of LENA
   speech monitoring systems, but are not labeled for emotional outbursts.
   This paper reports on our efforts to manually code child utterances as
   being of type ``laugh{''}, ``cry{''}, ``fuss{''}, ``babble{''} and
   ``hiccup{''}, and to develop algorithms capable of performing the same
   task automatically. Human labelers achieve much higher rates of
   inter-coder agreement for some of these categories than for others.
   Linear discriminant analysis (LDA) achieves better accuracy on tokens
   that have been coded by two human labelers than on tokens that have been
   coded by only one labeler, but the difference is not as much as we
   expected, suggesting that the acoustic and contextual features being
   used by human labelers are not yet available to the LDA. Convolutional
   neural network and hidden markov model achieve better accuracy than LDA,
   but worse F-score, because they over-weight the prior. Discounting the
   transition probability does not solve the problem.},
  affiliation                = {Xu, YJ (Corresponding Author), Univ Illinois, Dept Elect \& Comp Engn, Urbana, IL 61801 USA. Xu, YJ (Corresponding Author), Univ Illinois, Beckman Inst Adv Sci \& Technol, Urbana, IL 61801 USA. Xu Yijia; Allan, Hasegawa Johnson Mark, Univ Illinois, Dept Elect \& Comp Engn, Urbana, IL 61801 USA. Xu Yijia; Allan, Hasegawa Johnson Mark; Nancy, McElwain L., Univ Illinois, Beckman Inst Adv Sci \& Technol, Urbana, IL 61801 USA. Nancy, McElwain L., Univ Illinois, Dept Human Dev \& Family Studies, Urbana, IL USA.},
  author-email               = {yijiaxu3@illinois.edu jhasegaw@illinois.edu mcelwn@illinois.edu},
  book-group-author          = {Int Speech Commun Assoc},
  da                         = {2022-09-28},
  doc-delivery-number        = {BM5PH},
  doi                        = {10.21437/Interspeech.2018-2429},
  funding-acknowledgement    = {Social and Behavioral Sciences Research Initiative at the University of Illinois at Urbana-Champaign},
  funding-text               = {We thank Macie Berg, Rachel Diechstetter, Emily Flammersfeld, Elizabeth Mooney, and Shreya Patel who manually annotating the LENA files. This study was supported by a seed grant from the Social and Behavioral Sciences Research Initiative at the University of Illinois at Urbana-Champaign.},
  isbn                       = {978-1-5108-7221-9},
  issn                       = {2308-457X},
  keywords                   = {infant vocalizations; infant emotional outbursts; convolutional neural network; linear discriminant analysis; hidden markov model},
  language                   = {English},
  number-of-cited-references = {9},
  research-areas             = {Computer Science; Engineering},
  times-cited                = {1},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000465363900052},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {0},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \& Methods; Engineering, Electrical \& Electronic},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000466824000006,
  author                     = {Khan, Rizwan Ahmed and Crenn, Arthur and Meyer, Alexandre and Bouakaz, Saida},
  journal                    = {IMAGE AND VISION COMPUTING},
  title                      = {A novel database of children's spontaneous facial expressions (LIRIS-CSE)},
  year                       = {2019},
  issn                       = {0262-8856},
  month                      = {MAR-APR},
  pages                      = {61-69},
  volume                     = {83-84},
  status                     = {Aceito},
  abstract                   = {Computing environment is moving towards human-centered designs instead
   of computer centered designs and human's tend to communicate wealth of
   information through affective states or expressions. Traditional Human
   Computer Interaction (HCI) based systems ignores bulk of information
   communicated through those affective states and just caters for user's
   intentional input. Generally, for evaluating and benchmarking different
   facial expression analysis algorithms, standardized databases are needed
   to enable a meaningful comparison. In the absence of comparative tests
   on such standardized databases it is difficult to find relative
   strengths and weaknesses of different facial expression recognition
   algorithms. In this article we present a novel video database for
   Children's Spontaneous facial Expressions (LIRIS-CSE). Proposed video
   database contains six basic spontaneous facial expressions shown by 12
   ethnically diverse children between the ages of 6 and 12 years with mean
   age of 7.3 years. To the best of our knowledge, this database is first
   of its kind as it records and shows spontaneous facial expressions of
   children. Previously there were few database of children expressions and
   all of them show posed or exaggerated expressions which are different
   from spontaneous or natural expressions. Thus, this database will be a
   milestone for human behavior researchers. This database will be a
   excellent resource for vision community for benchmarking and comparing
   results. In this article, we have also proposed framework for automatic
   expression recognition based on Convolutional Neural Network (CNN)
   architecture with transfer learning approach. Proposed architecture
   achieved average classification accuracy of 75\% on our proposed
   database i.e. LIRIS-CSE. (C) 2019 Elsevier B.V. All rights reserved.},
  address                    = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  affiliation                = {Khan, RA (Corresponding Author), Barrett Hodgson Univ, Fac IT, Karachi, Pakistan. Khan, RA (Corresponding Author), Univ Claude Bernard Lyon1, LIRIS, Villeurbanne, France. Khan, Rizwan Ahmed, Barrett Hodgson Univ, Fac IT, Karachi, Pakistan. Khan, Rizwan Ahmed; Crenn, Arthur; Meyer, Alexandre; Bouakaz, Saida, Univ Claude Bernard Lyon1, LIRIS, Villeurbanne, France.},
  author-email               = {rizwan17@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {HW6TR},
  doi                        = {10.1016/j.imavis.2019.02.004},
  eissn                      = {1872-8138},
  funding-acknowledgement    = {Region Auvergne-Rhone-Alpes, France},
  funding-text               = {This work was funded by the Region Auvergne-Rhone-Alpes, France (http://www.auvergnerhonealpes.fr/).},
  journal-iso                = {Image Vis. Comput.},
  keywords                   = {Facial expressions database; Spontaneous expressions; Convolutional neural network; Expression recognition; Transfer learning},
  keywords-plus              = {EMOTION RECOGNITION},
  language                   = {English},
  number-of-cited-references = {43},
  oa                         = {Bronze, Green Submitted},
  orcid-numbers              = {Khan, PhD, Rizwan Ahmed/0000-0003-0819-800X MEYER, Alexandre/0000-0002-0249-1048},
  priority                   = {prio1},
  publisher                  = {ELSEVIER},
  research-areas             = {Computer Science; Engineering; Optics},
  researcherid-numbers       = {Khan, PhD, Rizwan Ahmed/N-7134-2018},
  times-cited                = {25},
  type                       = {Article},
  unique-id                  = {WOS:000466824000006},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {5},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory \& Methods; Engineering, Electrical \& Electronic; Optics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@article{ WOS:000520872700001,
Author = {Shrawankar, Urmila and Shireen, Azra},
Title = {Suggesting teaching methods by analyzing the behavior of children with
   special needs},
Journal = {BIO-ALGORITHMS AND MED-SYSTEMS},
Year = {2020},
Volume = {16},
Number = {1},
Month = {MAR},
Abstract = {The behavioral pattern of children with special needs depends on their
   emotional and developmental disability. Any abnormal and incorrect
   pattern of behavior which is below the level of development as the
   expected norm can be considered as ``the challenging behavior.{''} For
   supporting children with behavioral problems, many interventions and
   strategies alone, or in combination, are used. Behavioral and
   developmental problems, if not treated well, in childhood, may cause a
   problem and have negative long-term and short-term effects on a child's
   personal life, education, family, and professional life. Detailed
   knowledge about the child's behavior is important to define the problem.
   Hence, the software contains the questionnaire pattern, which is divided
   into categories like parents, teachers, doctors, and friends. Child
   behavior checklist is used to define the questionnaire. Images and video
   analysis are used to detect the current emotion in the child. By
   analyzing the behavioral pattern and current emotion, the teaching
   method will be suggested.},
Publisher = {WALTER DE GRUYTER GMBH},
Address = {GENTHINER STRASSE 13, D-10785 BERLIN, GERMANY},
Type = {Article},
Language = {English},
Affiliation = {Shrawankar, U (Corresponding Author), GH Raisoni Coll Engn, Dept Comp Sci \& Engn, Nagpur, MS, India.
   Shrawankar, Urmila; Shireen, Azra, GH Raisoni Coll Engn, Dept Comp Sci \& Engn, Nagpur, MS, India.},
DOI = {10.1515/bams-2019-0038},
Article-Number = {20190038},
ISSN = {1895-9091},
EISSN = {1896-530X},
Keywords = {behavioral disorder; behavioral recognizing; data analytics; data
   preprocessing; decision tree algorithm; Microsoft emotion/face API; mood
   detection; teaching interventions; video frames detector; video
   processing},
Research-Areas = {Mathematical \& Computational Biology},
Web-of-Science-Categories  = {Mathematical \& Computational Biology},
Author-Email = {urmilag@ieee.org},
ResearcherID-Numbers = {Shrawankar, Urmila/J-8544-2016},
ORCID-Numbers = {Shrawankar, Urmila/0000-0003-4523-9501},
Number-of-Cited-References = {8},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Bio-Algorithms Med-Syst.},
Doc-Delivery-Number = {KW0OI},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000520872700001},
DA = {2022-09-28},
}

@article{ WOS:000737014300001,
Author = {Duville, Mathilde Marie and Alonso-Valerdi, Luz Maria and Ibarra-Zarate,
   David I.},
Title = {Mexican Emotional Speech Database Based on Semantic, Frequency,
   Familiarity, Concreteness, and Cultural Shaping of Affective Prosody},
Journal = {DATA},
Year = {2021},
Volume = {6},
Number = {12},
Month = {DEC},
Abstract = {In this paper, the Mexican Emotional Speech Database (MESD) that
   contains single-word emotional utterances for anger, disgust, fear,
   happiness, neutral and sadness with adult (male and female) and child
   voices is described. To validate the emotional prosody of the uttered
   words, a cubic Support Vector Machines classifier was trained on the
   basis of prosodic, spectral and voice quality features for each case
   study: (1) male adult, (2) female adult and (3) child. In addition,
   cultural, semantic, and linguistic shaping of emotional expression was
   assessed by statistical analysis. This study was registered at BioMed
   Central and is part of the implementation of a published study protocol.
   Mean emotional classification accuracies yielded 93.3\%, 89.4\% and
   83.3\% for male, female and child utterances respectively. Statistical
   analysis emphasized the shaping of emotional prosodies by semantic and
   linguistic features. A cultural variation in emotional expression was
   highlighted by comparing the MESD with the INTERFACE for Castilian
   Spanish database. The MESD provides reliable content for linguistic
   emotional prosody shaped by the Mexican cultural environment. In order
   to facilitate further investigations, a corpus controlled for linguistic
   features and emotional semantics, as well as one containing words
   repeated across voices and emotions are provided. The MESD is made
   freely available.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Duville, MM (Corresponding Author), Tecnol Monterrey, Escuela Ingn Ciencias, Ave Eugenio Garza Sada 2501, Monterrey 64849, Mexico.
   Duville, Mathilde Marie; Alonso-Valerdi, Luz Maria; Ibarra-Zarate, David I., Tecnol Monterrey, Escuela Ingn Ciencias, Ave Eugenio Garza Sada 2501, Monterrey 64849, Mexico.},
DOI = {10.3390/data6120130},
Article-Number = {130},
EISSN = {2306-5729},
Keywords = {affective computing; audio database; cross-cultural; machine learning;
   Mexican Spanish; emotional speech; paralinguistic information; discrete
   emotions},
Keywords-Plus = {AFFECTIVE NORMS; SPANISH WORDS; RECOGNITION; FEATURES; EXPRESSION;
   CLASSIFICATION; VOICE; PERCEPTION; CATEGORIES; DIALECTS},
Research-Areas = {Computer Science; Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Computer Science, Information Systems; Multidisciplinary Sciences},
Author-Email = {a00829725@itesm.mx
   lm.aloval@tec.mx
   david.ibarra@tec.mx},
ORCID-Numbers = {Duville, Mathilde Marie/0000-0001-7309-3541
   Alonso Valerdi, Luz Maria/0000-0002-2256-2958},
Funding-Acknowledgement = {Mexican National Council of Science and Technology {[}1061809]},
Funding-Text = {This work was supported by Mathilde's (M.M.D.) PhD scholarship sponsored
   by the Mexican National Council of Science and Technology (reference
   number: 1061809). The sponsor was not involved in the conduct of the
   research and/or preparation of the article, nor in the decision to
   submit the article for publication.},
Number-of-Cited-References = {72},
Times-Cited = {1},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Data},
Doc-Delivery-Number = {XY5LS},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000737014300001},
OA = {gold},
DA = {2022-09-28},
}

@Article{WOS:000835046400004,
  author                     = {Ren, Panhong and Nie, Mengjian},
  journal                    = {JOURNAL OF SENSORS},
  title                      = {Design and Analysis of a Smart Sensor-Based Early Warning Intervention Network for School Sports Bullying among Left-Behind Children},
  year                       = {2022},
  issn                       = {1687-725X},
  month                      = {JUL 18},
  volume                     = {2022},
  status                     = {Rejeitado - Escopo},
  abstract                   = {This paper constructs a sports bullying early warning intervention
   system using smart sensors to conduct in-depth research and analysis on
   early warning intervention of school sports bullying behaviors among
   left-behind children. Unlike daily behavior recognition based on motion
   sensors, school sports bullying actions are very random and difficult to
   be described by a specific motion trajectory. For the characteristics of
   violent actions and daily actions, action features in the time and
   frequency domains are extracted and action categories are recognized by
   BP neural networks; for complex actions, it is proposed to decompose
   complex actions into basic actions to improve the recognition rate. The
   algorithm of combining action features and speech features to achieve
   violence recognition is proposed. For the complexity of audio data
   features, this paper firstly preprocesses the audio data with
   preweighting, framing, and windowing and secondly extracts the MFCC
   feature parameters from the audio data and then builds a deep
   convolutional neural network to design the violence emotion recognition
   algorithm. The simulation results show that the algorithm effectively
   improves the accuracy rate of violent action recognition to 91.25\% and
   the recall rate of violent action recognition to 92.13\%. Finally, the
   LDA dimensionality reduction algorithm is introduced to address the
   problem of the high complexity of the algorithm due to the high number
   of feature dimensions. The LDA dimensionality reduction algorithm
   reduces the number of feature dimensions to 7 dimensions, which reduces
   the system running time by about 52\% and improves the recognition rate
   of specific complex actions by about 12.1\% while ensuring the overall
   system performance.},
  address                    = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
  affiliation                = {Nie, MJ (Corresponding Author), Henan Normal Univ, Xinxiang 453000, Peoples R China. Ren, Panhong, Xinxiang Med Univ, Sanquan Coll, Xinxiang 453003, Peoples R China. Nie, Mengjian, Henan Normal Univ, Xinxiang 453000, Peoples R China.},
  article-number             = {2929254},
  author-email               = {14572097@sqmc.edu.cn 2016029@htu.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {3L8XW},
  doi                        = {10.1155/2022/2929254},
  eissn                      = {1687-7268},
  funding-acknowledgement    = {project of training plan for key teachers of Sanquan College of Xinxiang Medical University {[}SQ2022GGJS10]; Henan Education Science Planning Project (Effect of a school-based sport intervention on bullying behavior of rural left-behind children) {[}2021YB0068]; Henan Provincial Philosophy and Social Science Planning Project {[}2021CTY031]},
  funding-text               = {This work was supported by the project of training plan for key teachers of Sanquan College of Xinxiang Medical University (No. SQ2022GGJS10), Henan Education Science Planning Project (Effect of a school-based sport intervention on bullying behavior of rural left-behind children, No. 2021YB0068), and Henan Provincial Philosophy and Social Science Planning Project (No. 2021CTY031).},
  journal-iso                = {J. Sens.},
  keywords-plus              = {PARENTAL MIGRATION},
  language                   = {English},
  number-of-cited-references = {27},
  oa                         = {gold},
  publisher                  = {HINDAWI LTD},
  research-areas             = {Engineering; Instruments \& Instrumentation},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000835046400004},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Engineering, Electrical \& Electronic; Instruments \& Instrumentation},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{ WOS:000694490600001,
Author = {Hsu, Shin-Min and Chen, Sue-Huei and Huang, Tsung-Ren},
Title = {Personal Resilience Can Be Well Estimated from Heart Rate Variability
   and Paralinguistic Features during Human-Robot Conversations},
Journal = {SENSORS},
Year = {2021},
Volume = {21},
Number = {17},
Month = {SEP},
Abstract = {Mental health is as crucial as physical health, but it is
   underappreciated by mainstream biomedical research and the public.
   Compared to the use of AI or robots in physical healthcare, the use of
   AI or robots in mental healthcare is much more limited in number and
   scope. To date, psychological resilience-the ability to cope with a
   crisis and quickly return to the pre-crisis state-has been identified as
   an important predictor of psychological well-being but has not been
   commonly considered by AI systems (e.g., smart wearable devices) or
   social robots to personalize services such as emotion coaching. To
   address the dearth of investigations, the present study explores the
   possibility of estimating personal resilience using physiological and
   speech signals measured during human-robot conversations. Specifically,
   the physiological and speech signals of 32 research participants were
   recorded while the participants answered a humanoid social robot's
   questions about their positive and negative memories about three periods
   of their lives. The results from machine learning models showed that
   heart rate variability and paralinguistic features were the overall best
   predictors of personal resilience. Such predictability of personal
   resilience can be leveraged by AI and social robots to improve user
   understanding and has great potential for various mental healthcare
   applications in the future.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Huang, TR (Corresponding Author), Natl Taiwan Univ, Dept Psychol, Taipei 10617, Taiwan.
   Huang, TR (Corresponding Author), MOST Joint Res Ctr AI Technol \& All Vista Healthc, Taipei 10617, Taiwan.
   Huang, TR (Corresponding Author), MOST AI Biomed Res Ctr, Tainan 70101, Taiwan.
   Hsu, Shin-Min; Chen, Sue-Huei; Huang, Tsung-Ren, Natl Taiwan Univ, Dept Psychol, Taipei 10617, Taiwan.
   Hsu, Shin-Min; Huang, Tsung-Ren, MOST Joint Res Ctr AI Technol \& All Vista Healthc, Taipei 10617, Taiwan.
   Huang, Tsung-Ren, MOST AI Biomed Res Ctr, Tainan 70101, Taiwan.},
DOI = {10.3390/s21175844},
Article-Number = {5844},
EISSN = {1424-8220},
Keywords = {automatic personality recognition; human-robot interaction; personal
   resilience; physiological signals; speech signals},
Keywords-Plus = {POSITIVE EMOTIONS; CHILDHOOD TRAUMA; LANGUAGE USE; STRESS; DISORDERS;
   RESPONSES; ANXIETY; WORDS; DEATH; SCALE},
Research-Areas = {Chemistry; Engineering; Instruments \& Instrumentation},
Web-of-Science-Categories  = {Chemistry, Analytical; Engineering, Electrical \& Electronic;
   Instruments \& Instrumentation},
Author-Email = {smhsu@mil.psy.ntu.edu.tw
   shchen@ntu.edu.tw
   tren@mil.psy.ntu.edu.tw},
ORCID-Numbers = {Huang, Tsung-Ren/0000-0003-4396-7943},
Funding-Acknowledgement = {Ministry of Science and Technology in Taiwan {[}MOST 110-2634-F-002-040,
   MOST 110-2634-F-002-042]; National Taiwan University
   {[}NTU-CC-110L9A00703, NTU-CC-110L9A00704]},
Funding-Text = {This study was financially supported by the grants MOST
   110-2634-F-002-040 and MOST 110-2634-F-002-042 from the Ministry of
   Science and Technology in Taiwan as well as the grants
   NTU-CC-110L9A00703 and NTU-CC-110L9A00704 from National Taiwan
   University.},
Number-of-Cited-References = {57},
Times-Cited = {0},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {7},
Journal-ISO = {Sensors},
Doc-Delivery-Number = {UO1VW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000694490600001},
OA = {Green Published, gold},
DA = {2022-09-28},
}

@InProceedings{WOS:000534480500016,
  author                     = {Shahin, Mostafa and Ahmed, Beena and Smith, V, Daniel and Duenser, Andreas and Epps, Julien},
  booktitle                  = {2019 IEEE 29TH INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING (MLSP)},
  title                      = {AUTOMATIC SCREENING OF CHILDREN WITH SPEECH SOUND DISORDERS USING PARALINGUISTIC FEATURES},
  year                       = {2019},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP), Pittsburgh, PA, OCT 13-16, 2019},
  organization               = {IEEE},
  publisher                  = {IEEE},
  series                     = {IEEE International Workshop on Machine Learning for Signal Processing},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Subjective screening of children with speech disorders is costly, time
   consuming and infeasible due to the limited availability of Speech and
   Language Pathologists (SLPs). Therefore, there is an increasing interest
   in automatic speech analysis of children with speech disorders as it can
   offer a practical alternative to human assessment. Paralinguistic
   features are a set of low-level descriptors commonly used in speech
   emotion recognition. However, they have not yet been examined with
   childhood speech sound disorders such as, apraxia-of-speech and
   phonological and articulation disorders. In this paper, we investigated
   the effectiveness of paralinguistic features in discriminating between
   typically developing children and those who suffer from different types
   of speech sound disorders. Two types of standard paralinguistic features
   were explored, the Geneva Minimalistic Acoustic Parameter Set (GeMAPS)
   and its extended version, (cGcMAPS) feature sets. We applied feature
   selection to find the most discriminant set of features and employed
   binary classification using a support vector machine (SVM) to
   discriminate between the two groups. The method was tested on a
   recently-released public speech corpus collected from typically
   developing children and children with various types of speech sound
   disorders. The system achieved segment-level and subject-level
   unweighted average recall (UAR) of around 78\% and 87\% respectively.},
  affiliation                = {Shahin, M (Corresponding Author), UNSW, Sch Elect Engn \& Telecommun, Sydney, NSW, Australia. Shahin, M (Corresponding Author), CSIRO, Data61, Hobart, Tas, Australia. Shahin, Mostafa; Ahmed, Beena; Epps, Julien, UNSW, Sch Elect Engn \& Telecommun, Sydney, NSW, Australia. Shahin, Mostafa; Smith, Daniel, V; Duenser, Andreas; Epps, Julien, CSIRO, Data61, Hobart, Tas, Australia.},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BP0IM},
  isbn                       = {978-1-7281-0824-7},
  issn                       = {2161-0363},
  keywords                   = {speech sound disorders; speech therapy; paralinguistic features},
  keywords-plus              = {CLASSIFICATION},
  language                   = {English},
  number-of-cited-references = {23},
  orcid-numbers              = {Shahin, Mostafa/0000-0002-1091-8531 , AndreasDuenser/0000-0002-7423-0736 Ahmed, Beena/0000-0002-1240-6572},
  research-areas             = {Computer Science; Engineering},
  researcherid-numbers       = {Shahin, Mostafa/L-7639-2016 , AndreasDuenser/I-9279-2014 Ahmed, Beena/AAA-2141-2021},
  times-cited                = {0},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000534480500016},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Electrical \& Electronic},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@InProceedings{WOS:000598571700144,
  author                     = {Kivrak, Hasan and Uluer, Pinar and Kose, Hatice and Gumuslu, Elif and Barkana, Duygun Erol and Cakmak, Furkan and Yavuz, Sirma},
  booktitle                  = {2020 29TH IEEE INTERNATIONAL CONFERENCE ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN)},
  title                      = {Physiological Data-Based Evaluation of a Social Robot Navigation System},
  year                       = {2020},
  address                    = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  note                       = {29th IEEE International Conference on Robot and Human Interactive Communication (IEEE RO-MAN), ELECTR NETWORK, AUG 31-SEP 04, 2020},
  organization               = {IEEE; IEEE Robot \& Automat Soc; Robot Soc Japan; Korean Robot Soc; Furhat Robot; EurAi; Assoc Italiana Lingusitica Computazionale; Assoc Italiana Scienze Voce; Springer; Robotics},
  pages                      = {994-999},
  publisher                  = {IEEE},
  series                     = {IEEE RO-MAN},
  abstract                   = {The aim of this work is to create a social navigation system for an
   affective robot that acts as an assistant in the audiology department of
   hospitals for children with hearing impairments. Compared to traditional
   navigation systems, this system differentiates between objects and human
   beings and optimizes several parameters to keep at a social distance
   during motion when faced with humans not to interfere with their
   personal zones. For this purpose, social robot motion planning
   algorithms are employed to generate human-friendly paths that maintain
   humans' safety and comfort during the robot's navigation. This paper
   evaluates this system compared to traditional navigation, based on the
   surveys and physiological data of the adult participants in a
   preliminary study before using the system with children. Although the
   self-report questionnaires do not show any significant difference
   between navigation profiles of the robot, analysis of the physiological
   data may be interpreted that, the participants felt comfortable and less
   threatened in social navigation case.},
  affiliation                = {Kivrak, H (Corresponding Author), Karabuk Univ, Dept Comp Engn, Karabuk, Turkey. Kivrak, H (Corresponding Author), Istanbul Tech Univ, Dept Comp Engn, Istanbul, Turkey. Kivrak, Hasan, Karabuk Univ, Dept Comp Engn, Karabuk, Turkey. Uluer, Pinar, Galatasaray Univ, Dept Comp Engn, Istanbul, Turkey. Kivrak, Hasan; Uluer, Pinar; Kose, Hatice, Istanbul Tech Univ, Dept Comp Engn, Istanbul, Turkey. Gumuslu, Elif; Barkana, Duygun Erol, Yeditepe Univ, Dept Elect \& Elect Engn, Istanbul, Turkey. Cakmak, Furkan; Yavuz, Sirma, Yildiz Tech Univ, Dept Comp Engn, Istanbul, Turkey.},
  author-email               = {kivrakh@karabuk.edu.tr puluer@gsu.edu.tr hatice.koseg@itu.edu.tr gumusluelif@gmail.com duygunerol@yeditepe.edu.tr fcakmak@yildiz.edu.tr smyavuz@yildiz.edu.tr},
  book-group-author          = {IEEE},
  da                         = {2022-09-28},
  doc-delivery-number        = {BQ5CR},
  funding-acknowledgement    = {Scientific and Technological Research Council of Turkey (TUBITAK) {[}118E214]},
  funding-text               = {This work is supported by The Scientific and Technological Research Council of Turkey (TUBITAK) under the Grant number 118E214},
  isbn                       = {978-1-7281-6075-7},
  issn                       = {1944-9445},
  keywords                   = {social navigation; personal zone; HRI; emotion recognition; deeplearning; physiological data},
  keywords-plus              = {RECOGNITION},
  language                   = {English},
  number-of-cited-references = {28},
  orcid-numbers              = {YAVUZ, SIRMA/0000-0001-8029-6689 Barkana, Duygun Erol/0000-0002-8929-0459 Kose, Hatice/0000-0003-4796-4766 Kivrak, Hasan/0000-0002-3782-309X},
  research-areas             = {Computer Science; Engineering; Robotics},
  researcherid-numbers       = {YAVUZ, SIRMA/W-9655-2019 Barkana, Duygun Erol/N-2285-2018 Kose, Hatice/C-6322-2013},
  times-cited                = {3},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000598571700144},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \& Electronic; Robotics},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{ WOS:000452599700001,
Author = {Rusli, Nazreen and Sidek, Shahrul Na'im and Yusof, Hazlina Md and Ishak,
   Nor Izzati},
Title = {Mean of Correlation Methodfor Optimization of Affective States Detection
   in Children},
Journal = {IEEE ACCESS},
Year = {2018},
Volume = {6},
Pages = {68487-68497},
Abstract = {At the moment, most of the studies on classification of affective states
   for children focus on visual observations and physiological cues, where
   all data collection for measuring physiological signals are
   contact-based and invasive. With the requirement of having the measuring
   device attached to the body approach, distraction of the subject
   normally masks the true affective states of the subject due to
   discomfort. In this paper, a non-invasive, contactless, and less
   distraction method is proposed to measure the physiological cues of the
   subjects using their thermal imprints from frontal face imaging. A
   thermal image camera is used to identify basic affective states, where
   it is a contactless and seamless device with ability to read the
   radiated thermal imprint of the subjects' facial skin temperature. This
   paper proposes an effective algorithm of texture analysis based on novel
   technique using Gray Level Co-occurrence Matrix approach to be applied
   so as to identify blood-flow region. The cues from the first order
   statistics are computed in the identified blood flow region and
   concatenated along with second order statistics cues, in order to
   construct feature vectors to administer the vital and distinguishable
   characteristic pattern between affective states in thermal images.
   Result from the fine k-NN classifier obtained promises the efficacy of
   the proposed approach to be applied in our future work in human-robot
   interaction for autistic children learning and training.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Sidek, SN (Corresponding Author), Int Islamic Univ Malaysia, Fac Engn, Dept Mechatron Engn, Kuala Lumpur 53100, Malaysia.
   Rusli, Nazreen; Sidek, Shahrul Na'im; Yusof, Hazlina Md; Ishak, Nor Izzati, Int Islamic Univ Malaysia, Fac Engn, Dept Mechatron Engn, Kuala Lumpur 53100, Malaysia.},
DOI = {10.1109/ACCESS.2018.2878144},
ISSN = {2169-3536},
Keywords = {Affective states; facial skin temperature; thermal imaging},
Keywords-Plus = {AROUSAL; RECOGNITION; PHYSIOLOGY; EMOTION; FACE; FLOW},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {snaim@iium.edu.my},
ResearcherID-Numbers = {Sidek, Shahrul Naim/A-5275-2015},
ORCID-Numbers = {Sidek, Shahrul Naim/0000-0002-3204-1347},
Funding-Acknowledgement = {Fundamental Research Grant Scheme (FRGS) through the Ministry of Higher
   Education Malaysia {[}FRGS16-030-0529, FRGS13-076-0317]},
Funding-Text = {This work was supported by the Fundamental Research Grant Scheme (FRGS)
   through the Ministry of Higher Education Malaysia under Grant
   FRGS16-030-0529 and Grant FRGS13-076-0317.},
Number-of-Cited-References = {51},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {HD5VR},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000452599700001},
OA = {gold},
DA = {2022-09-28},
}

@Article{WOS:000576776700017,
  author                     = {Ghosh, Lidia and Saha, Sriparna and Konar, Amit},
  journal                    = {APPLIED SOFT COMPUTING},
  title                      = {Bi-directional Long Short-Term Memory model to analyze psychological effects on gamers},
  year                       = {2020},
  issn                       = {1568-4946},
  month                      = {OCT},
  volume                     = {95},
  status                     = {Rejeitado - Escopo},
  abstract                   = {With the increasing popularity of android gaming applications on smart
   phones, detection of emotional states of hard-core gamers become the
   interest of study among psychologists. Although there exist a few
   interesting research works on the impact of video games over the child
   and adult group, most of them are only able to throw light on
   psychological aspects associated with the said cognitive task. The
   real-time detection of emotional states of the player while playing
   video game is still an unexplored area of research. The present work
   feels the void by proposing a novel scheme of detecting the emotional
   changes of human subject from their electroencephalographic (EEG) signal
   acquired during their engagement in playing video games. The problem is
   formulated in the settings of pattern classification, which involves
   four main steps: Data collection, pre-processing and artifact removal,
   feature extraction and classification.
   The novelty of the work lies in extracting the emotional content with a
   high recognition rate from the acquired EEG response using a deep
   learning algorithm. The primary contribution of the paper lies in
   efficient usage of a novel phase-sensitive Common Spatial Pattern
   algorithm for feature extraction and design of an attention-based
   Bi-directional Long Short-Term Memory (Bi-LSTM) network for classifying
   the emotional states of a video-game player into five classes:
   happiness, sadness, surprise, anger and neutral. Moreover, the scarcity
   of labeled data in EEG-based brain-computer interfacing (BCI) tasks is a
   serious issue while understanding the performance capabilities of the
   data-driven deep-learning models. Therefore, the present work also makes
   an attempt to handle the scarcity in the dimension of the extracted
   feature using a novel feature augmentation algorithm before feeding the
   feature-vector to the proposed Bi-LSTM network. Experiments undertaken
   yield productive and conclusive results that validate the efficacy of
   the proposed framework with the accuracy rate of 88.71\%. (C) 2020
   Elsevier B.V. All rights reserved.},
  address                    = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  affiliation                = {Saha, S (Corresponding Author), Maulana Abul Kalam Azad Univ Technol, Dept Comp Sci \& Engn, Kolkata, W Bengal, India. Ghosh, Lidia; Konar, Amit, Jadavpur Univ, Dept Elect \& Telecommun Engn, Kolkata, W Bengal, India. Saha, Sriparna, Maulana Abul Kalam Azad Univ Technol, Dept Comp Sci \& Engn, Kolkata, W Bengal, India.},
  article-number             = {106573},
  author-email               = {lidiaghosh.bits@gmail.com sahasriparna@gmail.com konaramit@yahoo.co.in},
  da                         = {2022-09-28},
  doc-delivery-number        = {NZ0IZ},
  doi                        = {10.1016/j.asoc.2020.106573},
  eissn                      = {1872-9681},
  funding-acknowledgement    = {RUSA-2 project; UGC},
  funding-text               = {The first and third authors thankfully acknowledged the fund provided by RUSA-2 project granted to Jadavpur University, India. The second author is thankful to the University for providing research seed money and UGC Start-up Grant under the scheme of Basic Scientific Research. The work is approved by Institutional Ethics Committee.},
  journal-iso                = {Appl. Soft. Comput.},
  keywords                   = {Emotion; Games; Deep learning; Long Short-Term Memory model; Electroencephalography},
  keywords-plus              = {COMMON SPATIAL-PATTERNS; VIDEO GAME; BEHAVIOR; REAL; DESENSITIZATION; CLASSIFICATION},
  language                   = {English},
  number-of-cited-references = {74},
  orcid-numbers              = {Ghosh, Lidia/0000-0002-5146-955X},
  publisher                  = {ELSEVIER},
  research-areas             = {Computer Science},
  times-cited                = {3},
  type                       = {Article},
  unique-id                  = {WOS:000576776700017},
  usage-count-last-180-days  = {6},
  usage-count-since-2013     = {26},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000518894200002,
  author                     = {Drimalla, Hanna and Scheffer, Tobias and Landwehr, Niels and Baskow, Irina and Roepke, Stefan and Behnia, Behnoush and Dziobek, Isabel},
  journal                    = {NPJ DIGITAL MEDICINE},
  title                      = {Towards the automatic detection of social biomarkers in autism spectrum disorder: introducing the simulated interaction task (SIT)},
  year                       = {2020},
  issn                       = {2398-6352},
  month                      = {FEB 28},
  number                     = {1},
  volume                     = {3},
  abstract                   = {Social interaction deficits are evident in many psychiatric conditions
   and specifically in autism spectrum disorder (ASD), but hard to assess
   objectively. We present a digital tool to automatically quantify
   biomarkers of social interaction deficits: the simulated interaction
   task (SIT), which entails a standardized 7-min simulated dialog via
   video and the automated analysis of facial expressions, gaze behavior,
   and voice characteristics. In a study with 37 adults with ASD without
   intellectual disability and 43 healthy controls, we show the potential
   of the tool as a diagnostic instrument and for better description of
   ASD-associated social phenotypes. Using machine-learning tools, we
   detected individuals with ASD with an accuracy of 73\%, sensitivity of
   67\%, and specificity of 79\%, based on their facial expressions and
   vocal characteristics alone. Especially reduced social smiling and
   facial mimicry as well as a higher voice fundamental frequency and
   harmony-to-noise-ratio were characteristic for individuals with ASD. The
   time-effective and cost-effective computer-based analysis outperformed a
   majority vote and performed equal to clinical expert ratings.},
  address                    = {HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY},
  affiliation                = {Drimalla, H (Corresponding Author), Humboldt Univ, Dept Psychol, Unter Linden 6, D-10099 Berlin, Germany. Drimalla, H (Corresponding Author), Humboldt Univ, Berlin Sch Mind \& Brain, Unter Linden 6, D-10099 Berlin, Germany. Drimalla, H (Corresponding Author), Univ Potsdam, Hasso Plattner Inst, Digital Hlth Ctr, Prof Dr Helmert Str 2-3, D-14482 Potsdam, Germany. Drimalla, Hanna; Baskow, Irina; Dziobek, Isabel, Humboldt Univ, Dept Psychol, Unter Linden 6, D-10099 Berlin, Germany. Drimalla, Hanna; Dziobek, Isabel, Humboldt Univ, Berlin Sch Mind \& Brain, Unter Linden 6, D-10099 Berlin, Germany. Drimalla, Hanna, Univ Potsdam, Hasso Plattner Inst, Digital Hlth Ctr, Prof Dr Helmert Str 2-3, D-14482 Potsdam, Germany. Scheffer, Tobias; Landwehr, Niels, Univ Potsdam, Inst Comp Sci, Neuen Palais 10, D-14469 Potsdam, Germany. Landwehr, Niels, Leibniz Inst Agr Engn \& Bioecon, Max Eyth Allee 100, D-14469 Potsdam, Germany. Baskow, Irina; Roepke, Stefan; Behnia, Behnoush, Charite Univ Med Berlin, Dept Psychiat, Campus Benjamin Franklin,Hindenburgdamm 30, D-12203 Berlin, Germany.},
  article-number             = {25},
  author-email               = {hanna.drimalla@hu-berlin.de},
  da                         = {2022-09-28},
  doc-delivery-number        = {KT3DA},
  doi                        = {10.1038/s41746-020-0227-5},
  funding-acknowledgement    = {Berlin School of Mind and Brain {[}GSC 86/1-3]; German Research Foundation {[}LA/3270/1-1]; German Research Foundation (DFG); Open Access Publication Fund of Humboldt-Universitat zu Berlin},
  funding-text               = {H.D. was partially funded by the Berlin School of Mind and Brain under grant GSC 86/1-3. N.L. was partially funded by the German Research Foundation under grant LA/3270/1-1. We thank Christian Knauth for implementing the SIT into a stand-alone application for Windows and Mac. We acknowledge support by the German Research Foundation (DFG) and the Open Access Publication Fund of Humboldt-Universitat zu Berlin.},
  journal-iso                = {npj Digit. Med.},
  keywords-plus              = {FACIAL EMOTION RECOGNITION; HIGH-FUNCTIONING AUTISM; CHILDREN; MIMICRY; INDIVIDUALS; EXPRESSIONS; VOLUNTARY; DEFICITS; SPEECH; GENDER},
  language                   = {English},
  number-of-cited-references = {80},
  oa                         = {Green Published, gold},
  orcid-numbers              = {Drimalla, Hanna/0000-0003-3783-7237 Dziobek, Isabel/0000-0003-0150-5353},
  publisher                  = {NATURE PORTFOLIO},
  research-areas             = {Health Care Sciences \& Services; Medical Informatics},
  researcherid-numbers       = {Dziobek, isabel/ABB-4928-2020},
  times-cited                = {13},
  type                       = {Article},
  unique-id                  = {WOS:000518894200002},
  usage-count-last-180-days  = {7},
  usage-count-since-2013     = {14},
  web-of-science-categories  = {Health Care Sciences \& Services; Medical Informatics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000722305600004,
  author                     = {Li, Jing and Chen, Zejin and Li, Gongfa and Ouyang, Gaoxiang and Li, Xiaoli},
  journal                    = {NEUROCOMPUTING},
  title                      = {Automatic classification of ASD children using appearance-based features from videos},
  year                       = {2022},
  issn                       = {0925-2312},
  month                      = {JAN 22},
  pages                      = {40-50},
  volume                     = {470},
  status                     = {Aceito},
  abstract                   = {Early diagnosis of Autism Spectrum Disorder (ASD) plays a crucial role
   in the intervention of ASD. Traditionally, an ASD child needs to be
   diagnosed by a psychiatrist in the hospital, which is expensive,
   time-consuming, and influenced by expertise. In this paper, we present
   an objective, convenient, and effective method for classifying ASD
   children from raw video sequences by integrating the appearance-based
   features from facial expressions, head pose, and head trajectory. To
   better extract facial expression features, we propose a novel
   attention-based facial expression recognition algorithm to focus on key
   face areas like eyebrows, mouth, etc. Moreover, we use accumulative
   histogram to individually extract temporal and spatial information from
   facial expression, head pose and head trajectory of the video sequence.
   After fusing these three kinds of features, we feed them to Long
   Short-Term Memory (LSTM) and achieve a classification accuracy of 96.7\%
   on our self-collected ASD video dataset. (c) 2021 Published by Elsevier
   B.V.},
  address                    = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  affiliation                = {Li, J (Corresponding Author), Tianjin Univ Technol, Sch Comp Sci \& Engn, Tianjin 300384, Peoples R China. Li, J (Corresponding Author), Wuhan Univ Sci \& Technol, Minist Educ, Met Equipment \& Control Technol, Wuhan 430081, Peoples R China. Li, Jing, Tianjin Univ Technol, Sch Comp Sci \& Engn, Tianjin 300384, Peoples R China. Chen, Zejin, Nanchang Univ, Sch Informat Engn, Nanchang 330031, Jiangxi, Peoples R China. Li, Jing; Li, Gongfa, Wuhan Univ Sci \& Technol, Minist Educ, Met Equipment \& Control Technol, Wuhan 430081, Peoples R China. Ouyang, Gaoxiang; Li, Xiaoli, Beijing Normal Univ, State Key Lab Cognit Neurosci \& Learning, Beijing 100875, Peoples R China. Ouyang, Gaoxiang; Li, Xiaoli, Beijing Normal Univ, IDG McGovern Inst Brain Res, Beijing 100875, Peoples R China.},
  da                         = {2022-09-28},
  doc-delivery-number        = {XC9BK},
  doi                        = {10.1016/j.neucom.2021.10.074},
  earlyaccessdate            = {NOV 2021},
  eissn                      = {1872-8286},
  funding-acknowledgement    = {National Natural Science Foundation of China {[}61963027]; National Key Research and Development Program of China; Open Fund of Key Laboratory of Metallurgical Equipment and Control Technology of Ministry of Education, Wuhan University of Science and Technology {[}2017B02]},
  funding-text               = {This work is supported by National Natural Science Foundation of China under Grant 61963027, National Key Research and Development Program of China, Open Fund of Key Laboratory of Metallurgical Equipment and Control Technology of Ministry of Education, Wuhan University of Science and Technology under Grant 2017B02.},
  journal-iso                = {Neurocomputing},
  keywords                   = {Autism Spectrum Disorder (ASD); Appearance-base Features; Facial Expression Recognition; Head Pose; Long Short-Term Memory (LSTM)},
  keywords-plus              = {FACIAL EXPRESSION RECOGNITION; NEURAL-NETWORK; AUTISM; EMOTION; INFANTS; BRAIN},
  language                   = {English},
  number-of-cited-references = {62},
  orcid-numbers              = {Chen, Zejin/0000-0002-7705-1143},
  priority                   = {prio2},
  publisher                  = {ELSEVIER},
  research-areas             = {Computer Science},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000722305600004},
  usage-count-last-180-days  = {9},
  usage-count-since-2013     = {20},
  web-of-science-categories  = {Computer Science, Artificial Intelligence},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000579186700025,
  author                     = {Han, Jing and Zhang, Zixing and Pantic, Maja and Schuller, Bjoern},
  journal                    = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE},
  title                      = {Internet of emotional people: Towards continual affective computing cross cultures via audiovisual signals},
  year                       = {2021},
  issn                       = {0167-739X},
  month                      = {JAN},
  pages                      = {294-306},
  volume                     = {114},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Despite considerable advances achieved in affective computing over the
   past decade, the related learning paradigms still remain in an isolated
   fashion, i.e., models are often designed and developed task-dependently.
   Nevertheless, with the inherently heterogeneous and dynamic property of
   multiple tasks, we inevitably face several challenges, such as the
   implementation feasibility, when dealing with the growing number of new
   tasks of interest. For this reason, in this study, we endeavour to shed
   some fresh light on shifting the conventional isolated affective
   computing into a lifelong learning paradigm, namely continual affective
   computing. As the first tentative work in audio and video domains, we
   explore the lifelong learning algorithm of elastic weight consolidation
   for this benchmark work, in an application of well-established
   audiovisual emotion recognition in a cross-culture scenario, i.e.,
   French and German emotion recognition. To evaluate the feasibility and
   effectiveness of the introduced lifelong learning, we perform extensive
   experiments across the RECOLA and SEWA databases. The empirical results
   show that the implemented lifelong learning approach remarkably
   outperforms other baselines in most cases, and is even competitive to
   the joint training process in some cases, indicating its capability when
   handling the sequential learning process with multiple tasks. (C) 2020
   Elsevier B.V. All rights reserved.},
  address                    = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  affiliation                = {Han, J (Corresponding Author), Univ Augsburg, Chair Embedded Intelligence Hlth Care \& Wellbeing, D-86159 Augsburg, Germany. Han, Jing; Schuller, Bjoern, Univ Augsburg, Chair Embedded Intelligence Hlth Care \& Wellbeing, D-86159 Augsburg, Germany. Zhang, Zixing; Schuller, Bjoern, Imperial Coll London, GLAM Grp Language Audio \& Mus, London SW7 2AZ, England. Pantic, Maja, Imperial Coll London, Intelligent Behav Understanding Grp, London SW7 2AZ, England.},
  author-email               = {jing.han@informatik.uni-augsburg.de},
  da                         = {2022-09-28},
  doc-delivery-number        = {OC5HG},
  doi                        = {10.1016/j.future.2020.08.002},
  eissn                      = {1872-7115},
  funding-acknowledgement    = {TransAtlantic Platform ``Digging into Data'' collaboration grant (ACLEW: Analysing Child Language Experiences Around The World); UK's Economic \& Social Research Council {[}HJ-253479]},
  funding-text               = {This work was supported by the TransAtlantic Platform ``Digging into Data'' collaboration grant (ACLEW: Analysing Child Language Experiences Around The World), with the support of the UK's Economic \& Social Research Council through the research Grant No. HJ-253479. We also would like to thank our colleague Maximilian Schmitt for his help with the BoVW feature extraction of the RECOLA database.},
  journal-iso                = {Futur. Gener. Comp. Syst.},
  keywords                   = {Affective computing; Continual learning; Elastic weight consolidation; Emotional intelligence},
  keywords-plus              = {RECOGNITION; IOT; FRAMEWORK; THINGS; AROUSAL},
  language                   = {English},
  number-of-cited-references = {83},
  orcid-numbers              = {Han, Jing/0000-0001-5776-6849},
  publisher                  = {ELSEVIER},
  research-areas             = {Computer Science},
  times-cited                = {6},
  type                       = {Article},
  unique-id                  = {WOS:000579186700025},
  usage-count-last-180-days  = {4},
  usage-count-since-2013     = {28},
  web-of-science-categories  = {Computer Science, Theory \& Methods},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000444193800001,
  author                     = {Daniels, Jena and Schwartz, Jessey N. and Voss, Catalin and Haber, Nick and Fazel, Azar and Kline, Aaron and Washington, Peter and Feinstein, Carl and Winograd, Terry and Wall, Dennis P.},
  journal                    = {NPJ DIGITAL MEDICINE},
  title                      = {Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism},
  year                       = {2018},
  issn                       = {2398-6352},
  month                      = {AUG 2},
  volume                     = {1},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Although standard behavioral interventions for autism spectrum disorder
   (ASD) are effective therapies for social deficits, they face criticism
   for being time-intensive and overdependent on specialists. Earlier
   starting age of therapy is a strong predictor of later success, but
   waitlists for therapies can be 18 months long. To address these
   complications, we developed Superpower Glass, a
   machine-learning-assisted software system that runs on Google Glass and
   an Android smartphone, designed for use during social interactions. This
   pilot exploratory study examines our prototype tool's potential for
   social-affective learning for children with autism. We sent our tool
   home with 14 families and assessed changes from intake to conclusion
   through the Social Responsiveness Scale (SRS-2), a facial affect
   recognition task (EGG), and qualitative parent reports. A
   repeated-measures one-way ANOVA demonstrated a decrease in SRS-2 total
   scores by an average 7.14 points (F(1,13) = 33.20, p = <. 001, higher
   scores indicate higher ASD severity). EGG scores also increased by an
   average 9.55 correct responses (F(1,10) = 11.89, p = <. 01). Parents
   reported increased eye contact and greater social acuity. This
   feasibility study supports using mobile technologies for potential
   therapeutic purposes.},
  address                    = {HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY},
  affiliation                = {Wall, DP (Corresponding Author), Stanford Univ, Dept Pediat, Div Syst Med, Palo Alto, CA 94304 USA. Wall, DP (Corresponding Author), Stanford Univ, Dept Psychiat \& Behav Sci, Palo Alto, CA 94304 USA. Wall, DP (Corresponding Author), Stanford Univ, Dept Biomed Data Sci, Palo Alto, CA 94304 USA. Daniels, Jena; Schwartz, Jessey N.; Haber, Nick; Fazel, Azar; Kline, Aaron; Wall, Dennis P., Stanford Univ, Dept Pediat, Div Syst Med, Palo Alto, CA 94304 USA. Voss, Catalin; Washington, Peter; Winograd, Terry, Stanford Univ, Dept Comp Sci, Palo Alto, CA 94304 USA. Feinstein, Carl; Wall, Dennis P., Stanford Univ, Dept Psychiat \& Behav Sci, Palo Alto, CA 94304 USA. Wall, Dennis P., Stanford Univ, Dept Biomed Data Sci, Palo Alto, CA 94304 USA.},
  article-number             = {32},
  author-email               = {dpwall@stanford.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {GT1CA},
  doi                        = {10.1038/s41746-018-0035-3},
  funding-acknowledgement    = {NIH {[}1R01EB025025-01, 1R21HD091500-01]; Hartwell Foundation; Bill and Melinda Gates Foundation; Coulter Foundation; Lucile Packard Foundation; Stanford's Precision Health and Integrated Diagnostics Center (PHIND); Beckman Center; Bio-X Center; Predictives and Diagnostics Accelerator Program; Child Health Research Institute (CHRI); Human-Centered AI (HAI); Google; EUNICE KENNEDY SHRIVER NATIONAL INSTITUTE OF CHILD HEALTH \& HUMAN DEVELOPMENT {[}R21HD091500] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF BIOMEDICAL IMAGING AND BIOENGINEERING {[}R01EB025025] Funding Source: NIH RePORTER},
  funding-text               = {We would like to thank the participating families for their important contributions. The work was supported in part by funds to D. P. W. from NIH (1R01EB025025-01 and 1R21HD091500-01), the Hartwell Foundation, Bill and Melinda Gates Foundation, Coulter Foundation, Lucile Packard Foundation, and program grants from Stanford's Precision Health and Integrated Diagnostics Center (PHIND), Beckman Center, Bio-X Center, the Predictives and Diagnostics Accelerator Program, the Child Health Research Institute (CHRI), and Human-Centered AI (HAI). We also acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer, and Peter Sullivan. In-kind material grants included a gift from Google (35 units of Google Glass version 1) and Amazon Web Services Founder Support.},
  journal-iso                = {npj Digit. Med.},
  keywords-plus              = {SPECTRUM DISORDERS; EMOTION RECOGNITION; BEHAVIORAL INTERVENTIONS; 4-YEAR-OLD CHILDREN; COMMUNICATION; PREVALENCE; DIAGNOSIS; QUESTIONNAIRE; EXPRESSIONS; LANGUAGE},
  language                   = {English},
  number-of-cited-references = {76},
  oa                         = {Green Published, gold},
  orcid-numbers              = {washington, peter/0000-0003-3276-4411 Wall, Dennis/0000-0002-7889-9146},
  publisher                  = {NATURE RESEARCH},
  research-areas             = {Health Care Sciences \& Services; Medical Informatics},
  times-cited                = {35},
  type                       = {Article},
  unique-id                  = {WOS:000444193800001},
  usage-count-last-180-days  = {3},
  usage-count-since-2013     = {23},
  web-of-science-categories  = {Health Care Sciences \& Services; Medical Informatics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{ WOS:000618556100004,
Author = {Luo Fang and Jiang Liming and Tian Xuetao and Xiao Mengge and Ma Yanzhen
   and Zhang Sheng},
Title = {Shyness prediction and language style model construction of elementary
   school students},
Journal = {ACTA PSYCHOLOGICA SINICA},
Year = {2021},
Volume = {53},
Number = {2},
Pages = {155-169},
Month = {FEB},
Abstract = {The present study aimed to explore a new method of measuring shyness
   based on 1306 elementary school students' online writing texts. A
   supervised learning method was used to map students' labels (tagged by
   their results of scale) with their text features (extracted from online
   writing texts based on a psychological dictionary) to build a machine
   learning model. Key feature sets for different dimensions of shyness
   were built and a machine learning model was constructed based on the
   selected feature to achieve automatic prediction.
   The labels were obtained through ``National School Children Shyness
   Scale{''} completed online by elementary students. The scale includes
   three dimensions of shyness: shy behavior, shy cognition and shy
   emotion. Students with Z-scores of each dimension over 1 were labeled as
   shy and others were labeled as normal. Students' online writing texts
   were collected from ``TeachGrid{''} (https://www.jiaokee.com/), an
   online learning platform wherein students writing texts.
   The dictionary applied in the present study was Textmind, a widely used
   Chinese psychological dictionary developed based on Linguistic Inquiry
   and Word Count (LIWC). The dictionary was compiled mainly based on the
   corpus of adults. To ensure the validity of extracted features, we
   modified the original dictionary by expanding the categories and
   vocabulary with the real writing text of elementary students. The
   revised dictionary contained 118 categories.
   Features were extracted based on the revised dictionary. Chi-square
   algorithm was applied to identify the features that can distinguish
   between shy and normal groups to the greatest extent. Three sets of key
   features confirmed a significant lexical difference between shy and
   normal individuals. Among the selected features, some were shared by
   multiple dimensions reflecting the universal textual expression of shy
   individuals (e.g., The average number of words per sentence and the
   frequency of social words of shy individuals were less than that of
   normal counterparts.), and there were certain features reflected the
   unique characteristics of certain dimension (Perception words predicted
   shy behavior reflecting that high shy behavior individuals frequently
   felt being watched).
   Based on the selected features, Python 3.6.2 was used to construct the
   six prediction modes: Decision Tree, Random Forest, Support Vector
   Machine, Logistic Stitch Regression, K-Nearest Neighbor and Multilayer
   Perceptron. Overall, random forests have achieved the best results in
   the present study. The F1 score was 0.582, 0.552 and 0.545 for behavior
   cognition and emotion showing the feasibility of automatically
   predicting shyness characteristics of elementary school students based
   on textual language. The implication of word embedding, and deep
   learning models would improve the final prediction.},
Publisher = {SCIENCE PRESS},
Address = {16 DONGHUANGCHENGGEN NORTH ST, BEIJING 100717, PEOPLES R CHINA},
Type = {Article},
Language = {Chinese},
Affiliation = {Zhang, S (Corresponding Author), Beijing Normal Univ, Collaborat Innovat Ctr Assessment Basic Educ Qual, Beijing 100875, Peoples R China.
   Luo Fang; Jiang Liming; Xiao Mengge, Beijing Normal Univ, Sch Psychol, Beijing 100875, Peoples R China.
   Tian Xuetao, Beijing Jiaotong Univ, Sch Comp \& Informat Technol, Beijing 100044, Peoples R China.
   Ma Yanzhen; Zhang Sheng, Beijing Normal Univ, Collaborat Innovat Ctr Assessment Basic Educ Qual, Beijing 100875, Peoples R China.},
DOI = {10.3724/SP.J.1041.2021.00155},
ISSN = {0439-755X},
Keywords = {shyness; online writing; psychological dictionary; text mining; language
   style model},
Keywords-Plus = {PERSONALITY EXPRESSION; SOCIAL MEDIA; WORD USE; BEHAVIOR;
   CLASSIFICATION; EMOTIONALITY; RECOGNITION; ANXIETY; CONTEXT; TRAITS},
Research-Areas = {Psychology},
Web-of-Science-Categories  = {Psychology, Multidisciplinary},
Author-Email = {zhangsheng@bnu.edu.cn},
Number-of-Cited-References = {106},
Times-Cited = {0},
Usage-Count-Last-180-days = {16},
Usage-Count-Since-2013 = {29},
Journal-ISO = {Acta Psychol. Sin.},
Doc-Delivery-Number = {QH8XG},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000618556100004},
OA = {Bronze},
DA = {2022-09-28},
}

@article{ WOS:000821530700005,
Author = {Giacomucci, Giulia and Galdo, Giulia and Polito, Cristina and Berti,
   Valentina and Padiglioni, Sonia and Mazzeo, Salvatore and Chiaro,
   Eleonora and De Cristofaro, Maria Teresa and Bagnoli, Silvia and
   Nacmias, Benedetta and Sorbi, Sandro and Bessi, Valentina},
Title = {Unravelling neural correlates of empathy deficits in Subjective
   Cognitive Decline, Mild Cognitive Impairment and Alzheimer's Disease},
Journal = {BEHAVIOURAL BRAIN RESEARCH},
Year = {2022},
Volume = {428},
Month = {JUN 25},
Abstract = {Empathy is the ability to understand (cognitive empathy) and to feel
   (affective empathy) what others feel. The aim of the study was to assess
   empathy deficit and neuronal correlates in Subjective Cognitive Decline
   (SCD), Mild Cognitive Impairment (MCI) and Alzheimer's Disease (AD)
   dementia. Twenty-four SCD, 41 MCI and 46 CE patients were included.
   Informer-rated Interpersonal Reactivity Index was used to explore
   cognitive (Perspective Taking-PT, Fantasy-FT) and affective (Empathic
   Concern-EC, Personal Distress-PD) empathy, before (T0) and after (T1)
   cognitive symptoms' onset. Emotion recognition ability was tested
   through Ekman-60 Faces Test. Cerebral FDG-PET SPM analysis was used to
   explore neural correlates underlying empathy deficits. FT-T1 scores were
   lower in AD compared to SCD (13.0 +/- 8.0 vs 19.1 +/- 4,7 p = 0.008),
   PD-T1 score were higher in AD compared to MCI and to SCD (27.00 +/-
   10.00 vs 25.3 +/- 5.9 vs 20.5 +/- 5.6, p = 0.001). A positive
   correlation was found between PT-T1 and metabolic disfunction of right
   middle gyms (MFG) in MCI and AD. In AD group, a positive correlation
   between PT-T1 and insula and superior temporal gyms (STG) metabolism was
   detected. A negative correlation was found between PD-T1 and superior
   parietal lobule metabolism in MCI, and between PT-T1 and STG metabolism
   in AD. Impairment of cognitive empathy starts at MCI stage. Increase of
   PD starts from preclinical phases and seems to be to be dissociated from
   cognitive decline. Loss of PT is related to a progressive involvement
   starting from right MFG in prodromal stage, extending to insula and STG
   in dementia. Heightened emotional contagion is probably related to
   derangement of mirror neurons systems in parietal regions in prodromal
   stages, and to impairment of temporal emotion inhibition system in
   advanced phases. Further studies are needed to clarify if alterations in
   emotional contagion might be a predictive feature of a cognitive decline
   driven by AD.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Bessi, V (Corresponding Author), Univ Florence, Dept Neurosci Psychol Drug Res \& Child Hlth, Azienda Osped Univ Careggi, Largo Brambilla 3, I-50134 Florence, Italy.
   Giacomucci, Giulia; Galdo, Giulia; Mazzeo, Salvatore; Bagnoli, Silvia; Nacmias, Benedetta; Sorbi, Sandro; Bessi, Valentina, Univ Florence, Dept Neurosci Psychol Drug Res \& Child Hlth, Florence, Italy.
   Polito, Cristina; Mazzeo, Salvatore; Nacmias, Benedetta; Sorbi, Sandro, IRCCS Fdn Don Carlo Gnocchi, Florence, Italy.
   Berti, Valentina, Univ Florence, Dept Biomed Expt \& Clin Sci Mario Serio, Florence, Italy.
   Berti, Valentina; De Cristofaro, Maria Teresa, Azienda Osped Univ Careggi, Nucl Med Unit, Florence, Italy.
   Padiglioni, Sonia, Reg Referral Ctr Relat Critical Tuscany Reg, Florence, Italy.
   Padiglioni, Sonia, AOU Careggi, Res \& Innovat Ctr Dementia CRIDEM, Florence, Italy.
   Chiaro, Eleonora, Univ Florence, Florence, Italy.},
DOI = {10.1016/j.bbr.2022.113893},
Article-Number = {113893},
ISSN = {0166-4328},
EISSN = {1872-7549},
Keywords = {Alzheimer's Disease; Mild Cognitive Impairment; Subjective Cognitive
   Decline; Empathy},
Keywords-Plus = {FACIAL EMOTION RECOGNITION; ASSOCIATION WORKGROUPS; DIAGNOSTIC
   GUIDELINES; DOUBLE DISSOCIATION; NATIONAL INSTITUTE; NORMATIVE VALUES;
   TEMPORAL-LOBE; DEMENTIA; MEMORY; RECOMMENDATIONS},
Research-Areas = {Behavioral Sciences; Neurosciences \& Neurology},
Web-of-Science-Categories  = {Behavioral Sciences; Neurosciences},
Author-Email = {valentina.bessi@unifi.it},
ORCID-Numbers = {Mazzeo, Salvatore/0000-0001-5985-5060},
Funding-Acknowledgement = {Tuscany Region {[}20RSVB-PREVIEW]},
Funding-Text = {This research project was funded by Tuscany Region (GRANT n.
   20RSVB-PREVIEW: PRedicting the EVolution of SubjectIvE Cognitive Decline
   to Alzheimer's Disease With machine learning).},
Number-of-Cited-References = {70},
Times-Cited = {0},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {3},
Journal-ISO = {Behav. Brain Res.},
Doc-Delivery-Number = {2S0ZV},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000821530700005},
OA = {Bronze},
DA = {2022-09-28},
}

@inproceedings{ WOS:000457843608078,
Author = {Vicol, Paul and Tapaswi, Makarand and Castrejon, Lluis and Fidler, Sanja},
Book-Group-Author = {IEEE},
Title = {MovieGraphs: Towards Understanding Human-Centric Situations from Videos},
Booktitle = {2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)},
Series = {IEEE Conference on Computer Vision and Pattern Recognition},
Year = {2018},
Pages = {8581-8590},
Note = {31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018},
Organization = {IEEE; CVF; IEEE Comp Soc},
Abstract = {There is growing interest in artificial intelligence to build socially
   intelligent robots. This requires machines to have the ability to
   ``read{''} people's emotions, motivations, and other factors that affect
   behavior. Towards this goal, we introduce a novel dataset called
   MovieGraphs which provides detailed, graph-based annotations of social
   situations depicted in movie clips. Each graph consists of several types
   of nodes, to capture who is present in the clip, their emotional and
   physical attributes, their relationships (i.e., parent/child), and the
   interactions between them. Most interactions are associated with topics
   that provide additional details, and reasons that give motivations for
   actions. In addition, most interactions and many attributes are grounded
   in the video with time stamps. We provide a thorough analysis of our
   dataset, showing interesting common-sense correlations between different
   social aspects of scenes, as well as across scenes over time. We propose
   a method for querying videos and text with graphs, and show that: 1) our
   graphs contain rich and sufficient information to summarize and localize
   each scene; and 2) subgraphs allow us to describe situations at an
   abstract level and retrieve multiple semantically relevant situations.
   We also propose methods for interaction understanding via ordering, and
   reason understanding. MovieGraphs is the first benchmark to focus on
   inferred properties of human-centric situations, and opens up an
   exciting avenue towards socially-intelligent AI agents.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Vicol, P (Corresponding Author), Univ Toronto, Toronto, ON, Canada.
   Vicol, P (Corresponding Author), Vector Inst, Toronto, ON, Canada.
   Vicol, Paul; Tapaswi, Makarand; Fidler, Sanja, Univ Toronto, Toronto, ON, Canada.
   Vicol, Paul; Tapaswi, Makarand; Fidler, Sanja, Vector Inst, Toronto, ON, Canada.
   Castrejon, Lluis, Montreal Inst Learning Algorithms, Montreal, PQ, Canada.},
DOI = {10.1109/CVPR.2018.00895},
ISSN = {1063-6919},
ISBN = {978-1-5386-6420-9},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {pvicol@cs.toronto.edu
   makarand@cs.toronto.edu
   lluis.enric.castrejon.subira@umontreal.ca
   fidler@cs.toronto.edu},
Funding-Acknowledgement = {DARPA Explainable AI (XAI) program; NSERC; MERL; Comcast},
Funding-Text = {Supported by the DARPA Explainable AI (XAI) program, NSERC, MERL, and
   Comcast. We thank NVIDIA for their donation of GPUs. We thank Relu
   Patrascu for infrastructure support, and we thank the Upwork annotators.},
Number-of-Cited-References = {43},
Times-Cited = {26},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BL9NZ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000457843608078},
OA = {Green Submitted},
DA = {2022-09-28},
}

@Article{WOS:000501385900001,
  author                     = {Khullar, Vikas and Bala, Manju and Singh, Harjit Pal},
  journal                    = {ADVANCES IN AUTISM},
  title                      = {Interactive video-player to improve social smile in individuals with autism spectrum disorder},
  year                       = {2019},
  issn                       = {2056-3868},
  month                      = {DEC 4},
  number                     = {2},
  pages                      = {109-119},
  volume                     = {6},
  status                     = {Aceito},
  abstract                   = {Purpose The purpose of this paper is to propose and develop a live
   interaction-based video player system named LIV4Smile for the
   improvement of the social smile in individuals with autism spectrum
   disorder (ASD). Design/methodology/approach The proposed LIV4Smile
   intervention was a video player that operated by detecting smile using a
   convolutional neural network (CNN)-based algorithm. To maintain a live
   interaction, a CNN-based smile detector was configured and used in this
   system. The statistical test was also conducted to validate the
   performance of the system. Findings The significant improvement was
   observed in smile responses of individuals with ASD with the utilization
   of the proposed LIV4Smile system in a real-time environment.
   Originality/value The main aim of this study was to address the
   inclusive practices for children with autism. The proposed CNN
   algorithm-based LIV4Smile intervention resulted in high accuracy in
   facial smile detection.},
  address                    = {HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND},
  affiliation                = {Khullar, V (Corresponding Author), IKG Punjab Tech Univ, Kapurthala, India. Khullar, V (Corresponding Author), CT Inst Engn Management \& Technol, Comp Sci \& Engn, Jalandhar, Punjab, India. Khullar, Vikas; Bala, Manju; Singh, Harjit Pal, IKG Punjab Tech Univ, Kapurthala, India. Khullar, Vikas, CT Inst Engn Management \& Technol, Comp Sci \& Engn, Jalandhar, Punjab, India. Bala, Manju, Khalsa Coll Engn \& Technol, Amritsar, Punjab, India. Singh, Harjit Pal, CT Inst Engn Management \& Technol, Jalandhar, Punjab, India.},
  author-email               = {vikas.khullar@gmail.com},
  da                         = {2022-09-28},
  doc-delivery-number        = {LD2HU},
  doi                        = {10.1108/AIA-05-2019-0014},
  earlyaccessdate            = {DEC 2019},
  funding-acknowledgement    = {I.K.G. Punjab Technical University, Kapurthala (India); CT Institute of Engineering, Management, and Technology, Jalandhar (India)},
  funding-text               = {The authors are thankful to I.K.G. Punjab Technical University, Kapurthala (India) and CT Institute of Engineering, Management, and Technology, Jalandhar (India), for supporting us to making this research possible.},
  journal-iso                = {Adv. Autism},
  keywords                   = {Autism spectrum disorder; Convolutional neural network; Behavioral improvement; Facial emotion; Social smile; Video player},
  keywords-plus              = {YOUNG-CHILDREN; RECOGNITION; SYSTEM; IMPAIRMENTS; SKILLS},
  language                   = {English},
  number-of-cited-references = {52},
  orcid-numbers              = {Khullar, Vikas/0000-0002-0404-3652},
  priority                   = {prio2},
  publisher                  = {EMERALD GROUP PUBLISHING LTD},
  research-areas             = {Psychology},
  researcherid-numbers       = {Singh, Dr Harjit Pal/GPW-8215-2022 Khullar, Vikas/AAB-9123-2019},
  times-cited                = {2},
  type                       = {Article},
  unique-id                  = {WOS:000501385900001},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {6},
  web-of-science-categories  = {Psychology, Developmental},
  web-of-science-index       = {Emerging Sources Citation Index (ESCI)},
}

@Article{WOS:000451598900403,
  author                     = {Leo, Marco and Carcagni, Pierluigi and Distante, Cosimo and Spagnolo, Paolo and Mazzeo, Pier Luigi and Rosato, Anna Chiara and Petrocchi, Serena and Pellegrino, Chiara and Levante, Annalisa and De Lume, Filomena and Lecciso, Flavia},
  journal                    = {SENSORS},
  title                      = {Computational Assessment of Facial Expression Production in ASD Children},
  year                       = {2018},
  month                      = {NOV},
  number                     = {11},
  volume                     = {18},
  status                     = {Aceito},
  abstract                   = {In this paper, a computational approach is proposed and put into
   practice to assess the capability of children having had diagnosed
   Autism Spectrum Disorders (ASD) to produce facial expressions. The
   proposed approach is based on computer vision components working on
   sequence of images acquired by an off-the-shelf camera in unconstrained
   conditions. Action unit intensities are estimated by analyzing local
   appearance and then both temporal and geometrical relationships, learned
   by Convolutional Neural Networks, are exploited to regularize gathered
   estimates. To cope with stereotyped movements and to highlight even
   subtle voluntary movements of facial muscles, a personalized and
   contextual statistical modeling of non-emotional face is formulated and
   used as a reference. Experimental results demonstrate how the proposed
   pipeline can improve the analysis of facial expressions produced by ASD
   children. A comparison of system's outputs with the evaluations
   performed by psychologists, on the same group of ASD children, makes
   evident how the performed quantitative analysis of children's abilities
   helps to go beyond the traditional qualitative ASD assessment/diagnosis
   protocols, whose outcomes are affected by human limitations in observing
   and understanding multi-cues behaviors such as facial expressions.},
  address                    = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  affiliation                = {Leo, M (Corresponding Author), Natl Res Council Italy, Inst Appl Sci \& Intelligent Syst, Via Monteroni, I-73100 Lecce, Italy. Leo, Marco; Carcagni, Pierluigi; Distante, Cosimo; Spagnolo, Paolo; Mazzeo, Pier Luigi, Natl Res Council Italy, Inst Appl Sci \& Intelligent Syst, Via Monteroni, I-73100 Lecce, Italy. Rosato, Anna Chiara, Amici Nico Onlus, Via Campania 6, I-73046 Lecce, Italy. Petrocchi, Serena, USI, Inst Commun \& Hlth, Via Buffi 6, CH-6900 Lugano, Switzerland. Pellegrino, Chiara, L Adelfia Onlus, Via S Sangiovanni 115, I-73031 Lecce, Italy. Levante, Annalisa; De Lume, Filomena; Lecciso, Flavia, Univ Salento, Dipartimento Storia, Soc \& Studi Sull Uomo, Studium 2000,Edificio 5,Via Valesio, I-73100 Lecce, Italy.},
  article-number             = {3993},
  author-email               = {marco.leo@cnr.it pierluigi.carcagni@cnr.it cosimo.distante@cnr.it paolo.spagnolo@cnr.it pierluigi.mazzeo@cnr.it annachiara.rosato@libero.it serena.petrocchi@usi.ch chiara.pellegrino@yahoo.it annalisa.levante@unisalento.it filomena.delume@unisalento.it flavia.lecciso@unisalento.it},
  da                         = {2022-09-28},
  doc-delivery-number        = {HC1YJ},
  doi                        = {10.3390/s18113993},
  eissn                      = {1424-8220},
  funding-acknowledgement    = {project ``MUSA-Metodologie Ubiquitarie di inclusione sociale per l'Autismo{''} {[}1536, VZC4TI4]},
  funding-text               = {This work was partially supported by the project ``MUSA-Metodologie Ubiquitarie di inclusione sociale per l'Autismo{''} codice pratica VZC4TI4-{''}Aiuti a sostegno dei Cluster Tecnologici regionali per l'Innovazione{''} deliberazione della giunta regionale No. 1536 del 24/07/2014.},
  journal-iso                = {Sensors},
  keywords                   = {quantitative facial expression analysis; geometrical and temporal regularization of facial action units; ASD diagnosis and assessment},
  keywords-plus              = {RECOGNITION; EMOTION; 3D},
  language                   = {English},
  number-of-cited-references = {68},
  oa                         = {Green Submitted, Green Published, gold},
  orcid-numbers              = {Spagnolo, Paolo/0000-0001-9129-9375 Mazzeo, Pier Luigi/0000-0002-7552-2394 Leo, Marco/0000-0001-5636-6130 Levante, Annalisa/0000-0002-3250-3839 Petrocchi, Serena/0000-0002-7223-8240 Lecciso, Flavia/0000-0003-1846-7402 Distante, Cosimo/0000-0002-1073-2390},
  priority                   = {prio3},
  publisher                  = {MDPI},
  research-areas             = {Chemistry; Engineering; Instruments \& Instrumentation},
  researcherid-numbers       = {Carcagni, Pierluigi/AAY-6373-2020 Spagnolo, Paolo/AAX-7082-2020 Mazzeo, Pier Luigi/AAA-5053-2020 Leo, Marco/AAG-6296-2019 Petrocchi, Serena/AAA-2221-2021 Distante, Cosimo/M-7996-2013},
  times-cited                = {28},
  type                       = {Article},
  unique-id                  = {WOS:000451598900403},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {19},
  web-of-science-categories  = {Chemistry, Analytical; Engineering, Electrical \& Electronic; Instruments \& Instrumentation},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000721036500001,
  author                     = {Dai, Zhongpeng and Shao, Junneng and Zhou, Hongliang and Chen, Zhilu and Zhang, Siqi and Wang, Huan and Jiang, Haiteng and Yao, Zhijian and Lu, Qing},
  journal                    = {PROGRESS IN NEURO-PSYCHOPHARMACOLOGY \& BIOLOGICAL PSYCHIATRY},
  title                      = {Disrupted fronto-parietal network and default-mode network gamma interactions distinguishing suicidal ideation and suicide attempt in depression},
  year                       = {2022},
  issn                       = {0278-5846},
  month                      = {MAR 8},
  volume                     = {113},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Background: Precise suicide risk evaluation struggled in Major
   depressive disorder (MDD), especially for patients with only
   suicidal-ideation (SI) but without suicide attempt (SA). MDD patients
   have deficits in negative emotion processing, which is associated with
   the generation of SI and SA. Given the critical role of gamma
   oscillations in negative emotion processing, we hypothesize that the
   transition from SI to SA in MDD could be characterized by abnormal gamma
   interactions. Methods: We recruited 162 participants containing 106 MDD
   patients and 56 healthy controls (HCs). Participants performed facial
   recognition tasks while magnetoencephalography data were recorded.
   Time-frequencyrepresentation (TFR) analysis was conducted to identify
   the dominant spectra differences between MDD and HCs, and then source
   analysis was applied to localize the region of interests. Furthermore,
   frequency-specific functional connectivity network were constructed and
   a semi-supervised clustering algorithm was utilized to predict potential
   suicide risk. Results: Gamma (50-70 Hz) power was found significantly
   increased in MDD, mainly residing in regions from
   fronto-parietal-control-network (FPN), visual-network (VN),
   default-mode-network (DMN) and salience-network (SN). Based on impaired
   gamma functional connectivity network between well-established SA group
   and non-SI group, semi-supervised algorithm clustered patients with only
   SI into two groups with different suicide risks. Moreover, Inter-network
   gamma connectivity between FPN and DMN significantly negatively
   correlated with suicide risk and not confounded by depression severity.
   Conclusion: Inter-network gamma connectivity with FPN and DMN might be
   the key neuropathological interactions underling the progression from SI
   to SA. By applying semi-supervised clustering to electrophysiological
   data, it is possible to predict individual suicide risk.},
  address                    = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
  affiliation                = {Yao, ZJ; Lu, Q (Corresponding Author), Southeast Univ, Sch Biol Sci \& Med Engn, 2 Sipailou, Nanjing 210096, Jiangsu, Peoples R China. Dai, Zhongpeng; Shao, Junneng; Zhang, Siqi; Wang, Huan; Jiang, Haiteng; Yao, Zhijian; Lu, Qing, Southeast Univ, Sch Biol Sci \& Med Engn, 2 Sipailou, Nanjing 210096, Jiangsu, Peoples R China. Dai, Zhongpeng; Shao, Junneng; Zhang, Siqi; Wang, Huan; Jiang, Haiteng; Lu, Qing, Minist Educ, Key Lab, Child Dev \& Learning Sci, Nanjing, Peoples R China. Zhou, Hongliang; Chen, Zhilu; Yao, Zhijian, Nanjing Med Univ, Affiliated Brain Hosp, Dept Psychiat, Nanjing 210029, Peoples R China. Zhou, Hongliang; Chen, Zhilu; Yao, Zhijian, Nanjing Univ, Med Sch, Nanjing Brain Hosp, Nanjing 210093, Peoples R China. Jiang, Haiteng, Carnegie Mellon Univ, Dept Biomed Engn, Pittsburgh, PA 15213 USA.},
  article-number             = {110475},
  author-email               = {zjyao@nju.edu.cn luq@seu.edu.cn},
  da                         = {2022-09-28},
  doc-delivery-number        = {XB0OP},
  doi                        = {10.1016/j.pnpbp.2021.110475},
  earlyaccessdate            = {NOV 2021},
  eissn                      = {1878-4216},
  funding-acknowledgement    = {National Natural Science Foundation of China {[}81871066, 81571639, 81701784]; Jiangsu Pro-vincial Medical Innovation Team of the Project of Invigorating Health Care through Science, Technology and Education {[}CXTDC2016004]; Jiangsu Provincial key research and development program {[}BE2018609]; Fundamental Research Funds for the Central Uni-versities {[}2242021k30014, 2242021k30059]},
  funding-text               = {This work was partly supported by the National Natural Science Foundation of China (81871066, 81571639, 81701784) ; Jiangsu Pro-vincial Medical Innovation Team of the Project of Invigorating Health Care through Science, Technology and Education (CXTDC2016004) ; Jiangsu Provincial key research and development program (BE2018609) ; The Fundamental Research Funds for the Central Uni-versities (2242021k30014, 2242021k30059).},
  journal-iso                = {Prog. Neuro-Psychopharmacol. Biol. Psychiatry},
  keywords                   = {Suicide attempt; Suicidal ideation; Major depression; Gamma band; Magnetoencephalography (MEG); Semi-supervised clustering},
  keywords-plus              = {POSTERIOR CINGULATE CORTEX; EMOTION REGULATION; MAJOR DEPRESSION; DECISION-MAKING; RISK-FACTORS; CONNECTIVITY; OSCILLATION},
  language                   = {English},
  number-of-cited-references = {60},
  orcid-numbers              = {Dai, Zhongpeng/0000-0001-9645-5824},
  publisher                  = {PERGAMON-ELSEVIER SCIENCE LTD},
  research-areas             = {Neurosciences \& Neurology; Pharmacology \& Pharmacy; Psychiatry},
  times-cited                = {1},
  type                       = {Article},
  unique-id                  = {WOS:000721036500001},
  usage-count-last-180-days  = {7},
  usage-count-since-2013     = {29},
  web-of-science-categories  = {Clinical Neurology; Neurosciences; Pharmacology \& Pharmacy; Psychiatry},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{ WOS:000450540800010,
Author = {Bolinger, Elaina and Born, Jan and Zinke, Katharina},
Title = {Sleep divergently affects cognitive and automatic emotional response in
   children},
Journal = {NEUROPSYCHOLOGIA},
Year = {2018},
Volume = {117},
Pages = {84-91},
Month = {AUG},
Abstract = {Sleep enhances memory for emotional experiences, but its influence on
   the emotional response associated with memories is elusive. Here, we
   compared the influence of nocturnal sleep on memory for negative and
   neutral pictures and the associated emotional response in 8-11-year-old
   children, i.e., an age group with heightened levels of emotional
   memory-related sleep features. During all sessions, emotional responses
   as measured by subjective ratings, the late positive potential of the
   EEG (LPP) and heart rate deceleration (HRD) were recorded. Sleep
   enhanced picture memory. Compared to dynamics across wakefulness, sleep
   decreased the emotional response in ratings and the LPP, while
   increasing the emotional response in HRD. We conclude that sleep
   consolidates immediate emotional meaning by enhancing more automatic
   emotional responses while concurrently promoting top-down control of
   emotional responses, perhaps through strengthening respective
   neocortical representations.},
Publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Born, J (Corresponding Author), Univ Tubingen, Inst Med Psychol \& Behav Neurobiol, Otfried Muller Str 25, D-72076 Tubingen, Germany.
   Zinke, K (Corresponding Author), Univ Tubingen, Inst Med Psychol \& Behav Neurobiol, Silcherstr 5, D-72076 Tubingen, Germany.
   Bolinger, Elaina; Born, Jan; Zinke, Katharina, Univ Tubingen, Inst Med Psychol \& Behav Neurobiol, Otfried Muller Str 25, D-72076 Tubingen, Germany.
   Born, Jan, Univ Tubingen, Ctr Integrat Neurosci, D-72076 Tubingen, Germany.},
DOI = {10.1016/j.neuropsychologia.2018.05.015},
ISSN = {0028-3932},
EISSN = {1873-3514},
Keywords = {Development; Children; Sleep; Emotion; EEG; Memory},
Keywords-Plus = {REM-SLEEP; MEMORY CONSOLIDATION; REACTIVITY; AROUSAL; POTENTIALS;
   SPINDLES; AMYGDALA},
Research-Areas = {Behavioral Sciences; Neurosciences \& Neurology; Psychology},
Web-of-Science-Categories  = {Behavioral Sciences; Neurosciences; Psychology, Experimental},
Author-Email = {jan.born@uni-tuebingen.de
   katharina.zinke@uni-tuebingen.de},
ResearcherID-Numbers = {Zinke, Katharina/A-3678-2019
   Born, Jan/K-2596-2016},
ORCID-Numbers = {Zinke, Katharina/0000-0003-2595-2668
   Born, Jan/0000-0002-1847-6248},
Funding-Acknowledgement = {Deutsche Forschungsgemeinschaft SFB 654},
Funding-Text = {This work was supported by the Deutsche Forschungsgemeinschaft SFB 654
   (Plasticity and Sleep). We would like to thank Alexander
   Prehn-Kristensen and Christian Wiesner for preparing the task and
   analyzing the results of a pilot study, Hong-Viet Ngo for providing the
   spindle-detection algorithm code, and Cristin Clar and Lilliam Griselda
   Hernandez-Reyes for their help with data collection.},
Number-of-Cited-References = {49},
Times-Cited = {22},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Neuropsychologia},
Doc-Delivery-Number = {HA8LL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000450540800010},
OA = {hybrid},
DA = {2022-09-28},
}

@Article{WOS:000629509400015,
  author                     = {Xie, Wanze and Leppanen, Jukka M. and Kane-Grade, Finola E. and Nelson, Charles A.},
  journal                    = {NEUROIMAGE},
  title                      = {Converging neural and behavioral evidence for a rapid, generalized response to threat-related facial expressions in 3-year-old children},
  year                       = {2021},
  issn                       = {1053-8119},
  month                      = {APR 1},
  volume                     = {229},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Electrophysiological studies on adults suggest that humans are efficient
   at detecting threat from facial information and tend to grant these
   signals a priority in access to attention, awareness, and action. The
   developmental origins of this bias are poorly understood, partly because
   few studies have examined the emergence of a generalized neural and
   behavioral response to distinct categories of threat in early childhood.
   We used event-related potential (ERP) and eye-tracking measures to
   examine children's early visual responses and overt attentional biases
   towards multiple exemplars of angry and fearful vs. other (e.g., happy
   and neutral) faces. A large group of children was assessed
   longitudinally in infancy (5, 7, or 12 months) and at 3 years of age.
   The final ERP dataset included 148 infants and 132 3-year-old children;
   and the final eye-tracking dataset included 272 infants and 334
   3-year-olds. We demonstrate that 1) neural and behavioral responses to
   facial expressions converge on an enhanced response to fearful and angry
   faces at 3 years of age, with no differentiation between or bias towards
   one or the other of these expressions, and 2) a support vector machine
   learning model using data on the early-stage neural responses to threat
   reliably predicts the duration of overt attentional dwell time for
   threat-related faces at 3 years. However, we found little within-subject
   correlation between threat-bias attention in infancy and at 3 years of
   age. These results provide unique evidence for the early development of
   a rapid, unified response to two distinct categories of facial
   expressions with different physical characteristics, but shared
   threat-related meaning.},
  address                    = {525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA},
  affiliation                = {Xie, WZ; Nelson, CA (Corresponding Author), Boston Childrens Hosp, Boston, MA 02115 USA. Xie, Wanze; Kane-Grade, Finola E.; Nelson, Charles A., Boston Childrens Hosp, Boston, MA 02115 USA. Xie, Wanze; Nelson, Charles A., Harvard Med Sch, Boston, MA 02115 USA. Leppanen, Jukka M., Univ Turku, Dept Psychol \& Speech Language Pathol, Turku, Finland. Nelson, Charles A., Harvard Grad Sch Educ, Cambridge, MA USA.},
  article-number             = {117732},
  author-email               = {xiew1202@gmail.com charles\_nelson@harvard.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {QX7FD},
  doi                        = {10.1016/j.neuroimage.2021.117732},
  earlyaccessdate            = {JAN 2021},
  eissn                      = {1095-9572},
  funding-acknowledgement    = {National Institute of Mental Health {[}MH078829]},
  funding-text               = {The current work is support by a grant from National Institute of Mental Health, \#MH078829. We would like to thank all the families for their participation in this study and the entire Emotion Project. We would also like to acknowledge the Emotion Project staff past and present for their assistance in data acquisition, data processing, and relevant discussion.},
  journal-iso                = {Neuroimage},
  keywords-plus              = {EVENT-RELATED POTENTIALS; SOCIAL ATTENTION; FEARFUL FACES; EYE TRACKING; INFANTS; BRAIN; RECOGNITION; EMOTION; BIAS; EMERGENCE},
  language                   = {English},
  number-of-cited-references = {63},
  oa                         = {gold, Green Accepted},
  publisher                  = {ACADEMIC PRESS INC ELSEVIER SCIENCE},
  research-areas             = {Neurosciences \& Neurology; Radiology, Nuclear Medicine \& Medical Imaging},
  researcherid-numbers       = {Xie, Wanze/ABE-2153-2022},
  times-cited                = {6},
  type                       = {Article},
  unique-id                  = {WOS:000629509400015},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {6},
  web-of-science-categories  = {Neurosciences; Neuroimaging; Radiology, Nuclear Medicine \& Medical Imaging},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@InProceedings{WOS:000772239600019,
  author                     = {Abou el-Seoud, Samir and Ahmed, Samaa A.},
  booktitle                  = {CHALLENGES OF THE DIGITAL TRANSFORMATION IN EDUCATION, ICL2018, VOL 2},
  title                      = {IQ and EQ Enhancement for People with Mental Illness},
  year                       = {2019},
  address                    = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
  editor                     = {Auer, ME and Tsiatsos, T},
  note                       = {21st International Conference on Interactive Collaborative Learning (ICL) / 47th IGIP International Conference on Engineering Pedagogy - Teaching and Learning in a Digital World, GREECE, SEP 25-28, 2018},
  organization               = {IGIP; Aristotle Univ Thessaloniki},
  pages                      = {197-209},
  publisher                  = {SPRINGER INTERNATIONAL PUBLISHING AG},
  series                     = {Advances in Intelligent Systems and Computing},
  volume                     = {917},
  status                     = {Rejeitado - Escopo},
  abstract                   = {This paper focuses on IQ and EQ enhancement for people with mental
   challenges such as Autism, Asparagus syndrome and Down syndrome. People
   with autism and asparagus syndrome develop a relatively high IQ but
   below average EQ. On the other hand, people with Down syndrome and
   intellectual disability have low IQ and relatively high EQ. The system
   uses speech recognition and detection to listen and understand the user.
   The application uses text to speech (TTS) in addition to an artificial
   intelligent chat bot to maintain conversations with the user.
   Furthermore the system uses image processing to detect users emotions
   and facial expression. The application uses the methodologies used in
   several medical institutes and psychiatrist. In addition to IQ and EQ
   tests that are performed to detect the enhancement of the users IQ and
   EQ. There are two types of Intellectual disability previously known as
   mental retardation, namely: mildly mentally retarded (MIMR) and severely
   mentally retarded (MOMR). Intellectual disability according to IQ is
   rated by scores. An IQ of 130 and above means Very Intelligent, an IQ of
   120-129 indicates intelligent, and an IQ 110-119 indicates High Average,
   an IQ of 90-109 indicates Average, and an IQ of 80-89 indicates Low
   Average. An IQ of 70-79 means MIMR and an IQ 69 and lower means MOMR
   {[}1]. In Emotional intelligence, the average human rate is from 90 to
   110. A person with an IQ below 90 is considered to have low EQ. The main
   goal of this paper is to help users with below average IQ and EQ to seek
   normal rates. The developed program uses machine learning, speech
   recognition, image processing, and best IQ methodologies available to
   perform an EQ test repeatedly until the user reaches the normal EQ
   rates. That will allow users with autism to participate in their society
   and have a normal life. Many parents cannot afford to take their
   children to a specialist or send them to special schools. Consequently,
   parents should no longer suffer from self-taking care for their disable
   kids. In developing countries, children in Orphanages would not be
   diagnosed for their disability nor would have support from the staff.
   There are many programs and therapy institutions exist that is
   specialized in helping those disable people. However, mostly there are
   no computerized programs nor enough personal to support such children,
   adults or even parents. The combination of methodologies and the
   repetitive EQ and IQ tests ensures the effectiveness and efficiency of
   the product.},
  affiliation                = {Abou el-Seoud, S (Corresponding Author), British Univ Egypt BUE, Fac Informat \& Comp Sci Comp Sci, Cairo Governorate, Egypt. Abou el-Seoud, Samir, British Univ Egypt BUE, Fac Informat \& Comp Sci Comp Sci, Cairo Governorate, Egypt. Ahmed, Samaa A., British Univ Egypt BUE, Fac Informat \& Comp Sci Software Engn, Cairo Governorate, Egypt.},
  author-email               = {samir.elseoud@bue.edu.eg samaa131048@bue.edu.eg},
  da                         = {2022-09-28},
  doc-delivery-number        = {BS8HL},
  doi                        = {10.1007/978-3-030-11935-5\_19},
  eissn                      = {2194-5365},
  isbn                       = {978-3-030-11935-5; 978-3-030-11934-8},
  issn                       = {2194-5357},
  keywords                   = {Autism; IQ improvement; EQ improvement; AI; Intellectual disabilities},
  language                   = {English},
  number-of-cited-references = {7},
  orcid-numbers              = {Abou El-Seoud, M. Samir/0000-0002-5534-5861},
  research-areas             = {Computer Science},
  times-cited                = {0},
  type                       = {Proceedings Paper},
  unique-id                  = {WOS:000772239600019},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {2},
  web-of-science-categories  = {Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory \& Methods},
  web-of-science-index       = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@Article{WOS:000473769800001,
  author                     = {Dawood, Amina and Turner, Scott and Perepa, Prithvi},
  journal                    = {IEEE ACCESS},
  title                      = {Natural-Spontaneous Affective-Cognitive Dataset for Adult Students With and Without Asperger Syndrome},
  year                       = {2019},
  issn                       = {2169-3536},
  pages                      = {77990-77999},
  volume                     = {7},
  abstract                   = {Any viable algorithm to infer affective states of individuals with
   autism requires natural and reliable data in real time and in an
   uncontrolled environment. For this purpose, this study provides a new
   natural-spontaneous affective-cognitive dataset based on facial
   expressions, eye gaze, and head movements for adult students with and
   without Asperger syndrome (AS). The data gathering and collecting in a
   computer-based learning environment is one of the significant areas,
   which has attracted researchers' attention in affective computing
   applications. Due to the important impact of emotions on students
   learning outcome and their performance, the dataset included a range of
   affective-cognitive states which goes beyond basic emotions. This study
   reports the methodology that was used in data collection and annotation.
   Description and comparison of other available datasets were summarized,
   and also the study presents the results that were concluded in more
   details. In addition, some challenges were inherent to this study.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Dawood, A (Corresponding Author), Univ Northampton, Dept Comp, Northampton NN1 5PH, England. Dawood, Amina; Turner, Scott, Univ Northampton, Dept Comp, Northampton NN1 5PH, England. Perepa, Prithvi, Univ Northampton, Dept Educ, Northampton NN1 5PH, England.},
  author-email               = {amina.dawood@northampton.ac.uk},
  da                         = {2022-09-28},
  doc-delivery-number        = {IG4JL},
  doi                        = {10.1109/ACCESS.2019.2921914},
  journal-iso                = {IEEE Access},
  keywords                   = {Emotional dataset; autism; Asperger syndrome; spontaneous; natural; facial expressions; affective computing; affective-cognitive states},
  keywords-plus              = {FACIAL EXPRESSION; RECOGNITION SYSTEM; CHILDREN; EMOTIONS; MODEL; INTERVENTIONS; VALIDATION; TIME; FACE},
  language                   = {English},
  number-of-cited-references = {84},
  oa                         = {gold},
  orcid-numbers              = {Perepa, Prithvi/0000-0003-3130-9193 Turner, Scott/0000-0003-2735-3220 Dawood, Amina/0000-0002-0085-6314},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Computer Science; Engineering; Telecommunications},
  researcherid-numbers       = {Perepa, Prithvi/AAM-4658-2020 Turner, Scott/AFZ-5119-2022},
  times-cited                = {0},
  type                       = {Article},
  unique-id                  = {WOS:000473769800001},
  usage-count-last-180-days  = {1},
  usage-count-since-2013     = {8},
  web-of-science-categories  = {Computer Science, Information Systems; Engineering, Electrical \& Electronic; Telecommunications},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000445816800001,
  author                     = {Mencattini, Arianna and Mosciano, Francesco and Comes, Maria Colomba and Di Gregorio, Tania and Raguso, Grazia and Daprati, Elena and Ringeval, Fabien and Schuller, Bjorn and Di Natale, Corrado and Martinelli, Eugenio},
  journal                    = {SCIENTIFIC REPORTS},
  title                      = {An emotional modulation model as signature for the identification of children developmental disorders},
  year                       = {2018},
  issn                       = {2045-2322},
  month                      = {SEP 27},
  volume                     = {8},
  status                     = {Aceito},
  abstract                   = {In recent years, applications like Apple's Sid or Microsoft's Cortana
   have created the illusion that one can actually ``chat{''} with a
   machine. However, a perfectly natural human-machine interaction is far
   from real as none of these tools can empathize. This issue has raised an
   increasing interest in speech emotion recognition systems, as the
   possibility to detect the emotional state of the speaker. This
   possibility seems relevant to a broad number of domains, ranging from
   man-machine interfaces to those of diagnostics. With this in mind, in
   the present work, we explored the possibility of applying a precision
   approach to the development of a statistical learning algorithm aimed at
   classifying samples of speech produced by children with developmental
   disorders(DD) and typically developing(TD) children. Under the
   assumption that acoustic features of vocal production could not be
   efficiently used as a direct marker of DD, we propose to apply the
   Emotional Modulation function(EMF) concept, rather than running analyses
   on acoustic features per se to identify the different classes. The novel
   paradigm was applied to the French Child Pathological \& Emotional
   Speech Database obtaining a final accuracy of 0.79, with maximum
   performance reached in recognizing language impairment (0.92) and autism
   disorder (0.82).},
  address                    = {MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
  affiliation                = {Martinelli, E (Corresponding Author), Univ Roma Tor Vergata, Dept Elect Engn, Via Politecn 1, I-00133 Rome, Italy. Mencattini, Arianna; Mosciano, Francesco; Comes, Maria Colomba; Di Natale, Corrado; Martinelli, Eugenio, Univ Roma Tor Vergata, Dept Elect Engn, Via Politecn 1, I-00133 Rome, Italy. Di Gregorio, Tania; Raguso, Grazia, Univ Bari, Fac Sci MMFFNN, Univ Campus Ernesto Quagliariello, I-70126 Bari, Italy. Daprati, Elena, Univ Roma Tor Vergata, CBMS, Dept Syst Med, Via Montpellier 1, I-00133 Rome, Italy. Ringeval, Fabien, Univ Grenoble Alpes, Lab Informat Grenoble, F-38401 St Martin Dheres, France. Schuller, Bjorn, Imperial Coll London, GLAM, London SW7 2AZ, England. Schuller, Bjorn, Univ Augsburg, Chair Embedded Intelligence Hlth Care \& Wellbeing, D-86159 Augsburg, Germany.},
  article-number             = {14487},
  author-email               = {martinelli@ing.uniroma2.it},
  da                         = {2022-09-28},
  doc-delivery-number        = {GV1FU},
  doi                        = {10.1038/s41598-018-32454-7},
  journal-iso                = {Sci Rep},
  keywords-plus              = {AUTISM SPECTRUM DISORDER; FUTURE; SPEECH; CLASSIFICATION; INTONATION; TECHNOLOGY; PSYCHIATRY; PRECISION; PATTERNS; SENSORS},
  language                   = {English},
  number-of-cited-references = {44},
  oa                         = {Green Published, gold},
  orcid-numbers              = {Di Natale, Corrado/0000-0002-0543-4348 Comes, Maria Colomba/0000-0003-2903-4389 Martinelli, Eugenio/0000-0002-6673-2066 Mencattini, Arianna/0000-0002-3753-0457 Schuller, Bjorn/0000-0002-6478-8699},
  priority                   = {prio2},
  publisher                  = {NATURE PUBLISHING GROUP},
  research-areas             = {Science \& Technology - Other Topics},
  researcherid-numbers       = {Di Natale, Corrado/M-8701-2014 Comes, Maria Colomba/AAE-4797-2021 Martinelli, Eugenio/AAA-9105-2019 daprati, elena/AAA-2224-2019 Mencattini, Arianna/K-7910-2015},
  times-cited                = {7},
  type                       = {Article},
  unique-id                  = {WOS:000445816800001},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {14},
  web-of-science-categories  = {Multidisciplinary Sciences},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@inproceedings{ WOS:000530893804122,
Author = {Rahman, Jessica Sharmin and Gedeon, Tom and Caldwell, Sabrina and Jones,
   Richard and Hossain, Md Zakir and Zhu, Xuanying},
Book-Group-Author = {IEEE},
Title = {Melodious Micro-frissons: Detecting Music Genres From Skin Response},
Booktitle = {2019 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)},
Series = {IEEE International Joint Conference on Neural Networks (IJCNN)},
Year = {2019},
Note = {International Joint Conference on Neural Networks (IJCNN), Budapest,
   HUNGARY, JUL 14-19, 2019},
Abstract = {The relationship between music and human physiological signals has been
   a topic of interest among researchers for many years. Understanding this
   relationship can not only lead to more enhanced music therapy methods,
   but it may also help in finding a cure to mental disorders and epileptic
   seizures that are triggered by certain music. In this paper, we
   investigate the effects of 3 different genres of music in participants'
   Electrodermal Activity (EDA). Signals were recorded from 24 participants
   while they listened to 12 music stimuli. Various feature selection
   methods were applied to a number of features which were extracted from
   the signals. A simple neural network using Genetic Algorithm (GA)
   feature selection can reach as high as 96.8\% accuracy in classifying 3
   different music genres. Classification based on participants' subjective
   rating of emotion reaches 98.3\% accuracy with the Statistical
   Dependency (SD) / Minimal Redundancy Maximum Relevance (MRMR) feature
   selection technique. This shows that human emotion has a strong
   correlation with different types of music. In the future this system can
   be used to distinguish music based on their positive of negative effect
   on human mental health.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Rahman, JS (Corresponding Author), Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT, Australia.
   Rahman, Jessica Sharmin; Gedeon, Tom; Caldwell, Sabrina; Jones, Richard; Hossain, Md Zakir; Zhu, Xuanying, Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT, Australia.},
ISSN = {2161-4393},
ISBN = {978-1-7281-1985-4},
Keywords = {Music Therapy; Physiological Signals; Electrodermal Activity;
   Classification},
Keywords-Plus = {EMOTION RECOGNITION; MOZART MUSIC; EPILEPSY; SEIZURES; CLASSIFICATION;
   ADULTS; CHILDREN; QUALITY; BRAIN},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Hardware \&
   Architecture; Engineering, Electrical \& Electronic},
Author-Email = {jessica.rahman@anu.edu.au
   tom@cs.anu.edu.au
   sabrina.caldwell@anu.edu.au
   richard.jones@anu.edu.au
   zakir.hossain@anu.edu.au
   xuanying.zhu@anu.edu.au},
ResearcherID-Numbers = {Zhu, Xuanying/AAX-4284-2021
   Hossain, Zakir/Y-1537-2019},
ORCID-Numbers = {Hossain, Zakir/0000-0003-1892-831X},
Number-of-Cited-References = {39},
Times-Cited = {4},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BO9HA},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000530893804122},
DA = {2022-09-28},
}

@article{ WOS:000585482500001,
Author = {Halfon, Sibel and Doyran, Metehan and Turkmen, Batikan and Oktay, Eda
   Aydin and Salah, Ali Albert},
Title = {Multimodal affect analysis of psychodynamic play therapy},
Journal = {PSYCHOTHERAPY RESEARCH},
Year = {2021},
Volume = {31},
Number = {3},
Pages = {402-417},
Month = {APR 3},
Abstract = {Objective: We explore state of the art machine learning based tools for
   automatic facial and linguistic affect analysis to allow easier, faster,
   and more precise quantification and annotation of children's verbal and
   non-verbal affective expressions in psychodynamic child psychotherapy.
   Method: The sample included 53 Turkish children: 41 with internalizing,
   externalizing and comorbid problems; 12 in the non-clinical range. We
   collected audio and video recordings of 148 sessions, which were
   manually transcribed. Independent raters coded children's expressions of
   pleasure, anger, sadness and anxiety using the Children's Play Therapy
   Instrument (CPTI). Automatic facial and linguistic affect analysis
   modalities were adapted, developed, and combined in a system that
   predicts affect. Statistical regression methods (linear and polynomial
   regression) and machine learning techniques (deep learning, support
   vector regression and extreme learning machine) were used for predicting
   CPTI affect dimensions. Results: Experimental results show significant
   associations between automated affect predictions and CPTI affect
   dimensions with small to medium effect sizes. Fusion of facial and
   linguistic features work best for pleasure predictions; however, for
   other affect predictions linguistic analyses outperform facial analyses.
   External validity analyses partially support anger and pleasure
   predictions. Discussion: The system enables retrieving affective
   expressions of children, but needs improvement for precision.},
Publisher = {ROUTLEDGE JOURNALS, TAYLOR \& FRANCIS LTD},
Address = {2-4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Halfon, S (Corresponding Author), Istanbul Bilgi Univ, Kazim Karabekir Cad 2-13, TR-34060 Istanbul, Turkey.
   Halfon, Sibel, Istanbul Bilgi Univ, Kazim Karabekir Cad 2-13, TR-34060 Istanbul, Turkey.
   Doyran, Metehan; Salah, Ali Albert, Univ Utrecht, Dept Informat \& Comp Sci, Utrecht, Netherlands.
   Turkmen, Batikan; Salah, Ali Albert, Bogazici Univ, Comp Engn Dept, Bebek, Turkey.
   Oktay, Eda Aydin, Northeastern Univ, Khoury Coll Comp Sci, Boston, MA 02115 USA.},
DOI = {10.1080/10503307.2020.1839141},
EarlyAccessDate = {NOV 2020},
ISSN = {1050-3307},
EISSN = {1468-4381},
Keywords = {multimodal affect analysis; face analysis; text analysis; psychodynamic
   play therapy},
Keywords-Plus = {EXTREME LEARNING-MACHINE; FACIAL EXPRESSIONS; EMOTION; PSYCHOTHERAPY;
   COREGULATION; CHILDREN; BEHAVIOR; ASSOCIATIONS; RECOGNITION; REGRESSION},
Research-Areas = {Psychology},
Web-of-Science-Categories  = {Psychology, Clinical},
Author-Email = {sibel.halfon@bilgi.edu.tr},
Funding-Acknowledgement = {Scientific and Technological Research Council of Turkey (TUBITAK)
   {[}215K180]},
Funding-Text = {This study was partially supported by the Scientific and Technological
   Research Council of Turkey (TUBITAK) Project No: 215K180.},
Number-of-Cited-References = {84},
Times-Cited = {6},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {12},
Journal-ISO = {Psychother. Res.},
Doc-Delivery-Number = {QG3DO},
Web-of-Science-Index = {Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000585482500001},
DA = {2022-09-28},
}

@Article{WOS:000736965200002,
  author                     = {Tooley, Ursula A. and Bassett, Danielle S. and Mackey, Allyson P.},
  journal                    = {NEUROIMAGE},
  title                      = {Functional brain network community structure in childhood: Unfinished territories and fuzzy boundaries},
  year                       = {2022},
  issn                       = {1053-8119},
  month                      = {FEB 15},
  volume                     = {247},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Adult cortex is organized into distributed functional communities. Yet,
   little is known about community architecture of children's brains. Here,
   we uncovered the community structure of cortex in childhood using fMRI
   data from 670 children aged 9-11 years (48\% female, replication sample=
   544 , 56\% female) from the Adolescent Brain and Cognitive Development
   study. We first applied a data-driven community detection approach to
   cluster cortical regions into communities, then employed a generative
   model-based approach called the weighted stochastic block model to
   further probe community interactions. Children showed similar community
   structure to adults, as defined by Yeo and colleagues in 2011, in
   early-developing sensory and motor communities, but differences emerged
   in transmodal areas. Children have more cortical territory in the limbic
   community, which is involved in emotion processing, than adults. Regions
   in association cortex interact more flexibly across communities,
   creating uncertainty for the model-based assignment algorithm, and
   perhaps reflecting cortical boundaries that are not yet solidified.
   Uncertainty was highest for cingulo-opercular areas involved in flexible
   deployment of cognitive control. Activation and deactivation patterns
   during a working memory task showed that both the data driven approach
   and a set of adult communities statistically capture functional
   organization in middle childhood. Collectively, our findings suggest
   that community boundaries are not solidified by middle childhood.},
  address                    = {525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA},
  affiliation                = {Mackey, AP (Corresponding Author), Univ Penn, Sch Arts \& Sci, Dept Psychol, Philadelphia, PA 19104 USA. Tooley, Ursula A., Univ Penn, Perelman Sch Med, Neurosci Grad Grp, Philadelphia, PA 19104 USA. Tooley, Ursula A.; Mackey, Allyson P., Univ Penn, Sch Arts \& Sci, Dept Psychol, Philadelphia, PA 19104 USA. Bassett, Danielle S., Univ Penn, Sch Engn \& Appl Sci, Dept Bioengn, Philadelphia, PA 19104 USA. Bassett, Danielle S., Univ Penn, Sch Engn \& Appl Sci, Dept Elect \& Syst Engn, Philadelphia, PA 19104 USA. Bassett, Danielle S., Univ Penn, Sch Arts \& Sci, Dept Phys \& Astron, Philadelphia, PA 19104 USA. Bassett, Danielle S., Univ Penn, Perelman Sch Med, Dept Neurol, Philadelphia, PA 19104 USA. Bassett, Danielle S., Univ Penn, Perelman Sch Med, Dept Psychiat, Philadelphia, PA 19104 USA. Bassett, Danielle S., Santa Fe Inst, Santa Fe, NM 87501 USA.},
  article-number             = {118843},
  author-email               = {mackeya@upenn.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {XY4SV},
  doi                        = {10.1016/j.neuroimage.2021.118843},
  earlyaccessdate            = {DEC 2021},
  eissn                      = {1095-9572},
  funding-acknowledgement    = {National Science Foundation Graduate Research Fellowship; Jacobs Foundation Early Career Research Fellowship; National Institute on Drug Abuse {[}1R34DA050297-01]; Army Research Office {[}Grafton-W911NF-16-1-0474]; National Science Foundation {[}NSF PHY-1554488, BCS-1631550, IIS-1926757]},
  funding-text               = {The authors would like to acknowledge Dr. Richard Watts, who provided the task activation maps used in this study, and thank Dr. Linden Parkes for his helpful comments on an earlier version of this manuscript. U.A.T. was supported by the National Science Foundation Graduate Research Fellowship. A.P.M. was supported by a Jacobs Foundation Early Career Research Fellowship and the National In-stitute on Drug Abuse (1R34DA050297-01) . D.S.B. was supported by the Army Research Office (Grafton-W911NF-16-1-0474) and the National Science Foundation (NSF PHY-1554488, BCS-1631550, and IIS-1926757) . Data used in the preparation of this article were ob-tained from the Adolescent Brain Cognitive Development (ABCD)},
  journal-iso                = {Neuroimage},
  keywords                   = {Development; Community structure; Networks; Graph theory; Network neuroscience},
  keywords-plus              = {RESTING-STATE FMRI; MOTION ARTIFACT; CONFOUND REGRESSION; PREFRONTAL CORTEX; COGNITIVE CONTROL; MOTOR CORTEX; CONNECTIVITY; ORGANIZATION; EMERGENCE; VISUALIZATION},
  language                   = {English},
  number-of-cited-references = {116},
  oa                         = {gold, Green Submitted, Green Accepted},
  publisher                  = {ACADEMIC PRESS INC ELSEVIER SCIENCE},
  research-areas             = {Neurosciences \& Neurology; Radiology, Nuclear Medicine \& Medical Imaging},
  times-cited                = {1},
  type                       = {Article},
  unique-id                  = {WOS:000736965200002},
  usage-count-last-180-days  = {5},
  usage-count-since-2013     = {17},
  web-of-science-categories  = {Neurosciences; Neuroimaging; Radiology, Nuclear Medicine \& Medical Imaging},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Article{WOS:000711695600001,
  author                     = {Singh, Tripti and Mohadikar, Mohan and Gite, Shilpa and Patil, Shruti and Pradhan, Biswajeet and Alamri, Abdullah},
  journal                    = {IEEE ACCESS},
  title                      = {Attention Span Prediction Using Head-Pose Estimation With Deep Neural Networks},
  year                       = {2021},
  issn                       = {2169-3536},
  pages                      = {142632-142643},
  volume                     = {9},
  status                     = {Aceito},
  abstract                   = {Automated human pose estimation is evolving as an exciting research area
   in human activity detection. It includes sophisticated applications such
   as malpractice detection in the examination, distracted driving, gesture
   detection, etc., and requires robust and reliable pose estimation
   techniques. These applications help to map the attention of the user
   with head pose estimation (HPE) metrics supported by emotion and gaze
   analysis. This paper solves the problem of attention score estimation
   with HPE. The proposed method ensures ease of implementation while
   addressing head pose estimation using 68 facial features. Further, to
   attain reliability and precision, head pose estimation has been
   implemented as a regression task. The coordinate pair angle method
   (CPAM) with deep neural network (DNN) regression and elastic net
   regression is carried out. The use of DNN ensures precision on low
   lighting, distorted or occluded images. CPAM methodology leverages
   facial landmark detection and angular difference to estimate head pose.
   Experimentation results showed that the proposed model could handle
   large datasets, real-time data processing, significant pose variations,
   partial occlusions, and diverse facial expressions with a mean absolute
   error (MAE) of 3 degrees and less. The proposed system was evaluated on
   three standard databases: the 300W across large poses (300W-LP) dataset,
   annotated facial landmarks in the wild (AFLW2000) dataset, and the
   national institute of mental health child emotional faces picture set
   (NIMH-ChEFS) dataset. The results achieved are on par with recent
   state-of-the-art methodologies such as anisotropic angle distribution
   learning (AADL), joint head pose estimation and face alignment algorithm
   (JFA), rotation axis focused attention network (RAFA-Net), and propose
   an MAE ranging up to 6 degrees. The paper could achieve remarkable
   results for attention span prediction using head pose estimation and for
   many possible future applications.},
  address                    = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  affiliation                = {Gite, S (Corresponding Author), Symbiosis Int Deemed Univ, Symbiosis Ctr Appl Artificial Intelligence, Symbiosis Inst Technol, Comp Sci Dept, Pune 412115, Maharashtra, India. Pradhan, B (Corresponding Author), Univ Technol Sydney, Fac Engn \& IT, Ctr Adv Modelling \& Geospatial Informat Syst CAMG, Sydney, NSW 2007, Australia. Pradhan, B (Corresponding Author), Univ Kebangsaan Malaysia, Earth Observat Ctr, Inst Climate Change, Bangi 43600, Selangor, Malaysia. Singh, Tripti; Mohadikar, Mohan, Symbiosis Int Deemed Univ, Symbiosis Ctr Appl Artificial Intelligence, Symbiosis Inst Technol, Pune 412115, Maharashtra, India. Gite, Shilpa; Patil, Shruti, Symbiosis Int Deemed Univ, Symbiosis Ctr Appl Artificial Intelligence, Symbiosis Inst Technol, Comp Sci Dept, Pune 412115, Maharashtra, India. Pradhan, Biswajeet, Univ Technol Sydney, Fac Engn \& IT, Ctr Adv Modelling \& Geospatial Informat Syst CAMG, Sydney, NSW 2007, Australia. Pradhan, Biswajeet, Univ Kebangsaan Malaysia, Earth Observat Ctr, Inst Climate Change, Bangi 43600, Selangor, Malaysia. Alamri, Abdullah, King Saud Univ, Coll Sci, Dept Geol \& Geophys, Riyadh 11451, Saudi Arabia.},
  author-email               = {shilpa.gite@sitpune.edu.in biswajeet.pradhan@uts.edu.au},
  da                         = {2022-09-28},
  doc-delivery-number        = {WN3TY},
  doi                        = {10.1109/ACCESS.2021.3120098},
  funding-acknowledgement    = {Symbiosis Centre of Applied Artificial Intelligence (SCAAI); Centre for Advanced Modelling and Geospatial Information Systems (CAMGIS), Faculty of Engineering and Information Technology, University of Technology Sydney, Australia; King Saud University, Riyadh, Saudi Arabia {[}RSP-2021/14]},
  funding-text               = {This work was supported in part by the Symbiosis Centre of Applied Arti\~{}cial Intelligence (SCAAI); in part by the Centre for Advanced Modelling and Geospatial Information Systems (CAMGIS), Faculty of Engineering and Information Technology, University of Technology Sydney, Australia; and in part by the Researchers Supporting Project through King Saud University, Riyadh, Saudi Arabia, under Grant RSP-2021/14.},
  journal-iso                = {IEEE Access},
  keywords                   = {Head; Pose estimation; Faces; Three-dimensional displays; Magnetic heads; Convolutional neural networks; Training; Head pose estimation; CPAM; convolutional neural network; regression; attention span prediction},
  keywords-plus              = {GAUSSIAN-LABEL DISTRIBUTION; VISION},
  language                   = {English},
  number-of-cited-references = {62},
  oa                         = {gold},
  orcid-numbers              = {Pradhan, Biswajeet/0000-0001-9863-2054 Patil, Shruti G/0000-0002-4903-1540 Mohadikar, Mohan/0000-0002-6768-6698 Singh, T.P./0000-0002-9375-3495},
  priority                   = {prio1},
  publisher                  = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  research-areas             = {Computer Science; Engineering; Telecommunications},
  researcherid-numbers       = {Pradhan, Biswajeet/E-8226-2010},
  times-cited                = {3},
  type                       = {Article},
  unique-id                  = {WOS:000711695600001},
  usage-count-last-180-days  = {8},
  usage-count-since-2013     = {15},
  web-of-science-categories  = {Computer Science, Information Systems; Engineering, Electrical \& Electronic; Telecommunications},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@Article{WOS:000424804400016,
  author                     = {Gibbard, Clare R. and Ren, Juejing and Skuse, David H. and Clayden, Jonathan D. and Clark, Chris A.},
  journal                    = {HUMAN BRAIN MAPPING},
  title                      = {Structural connectivity of the amygdala in young adults with autism spectrum disorder},
  year                       = {2018},
  issn                       = {1065-9471},
  month                      = {MAR},
  number                     = {3},
  pages                      = {1270-1282},
  volume                     = {39},
  abstract                   = {Autism spectrum disorder (ASD) is characterized by impairments in social
   cognition, a function associated with the amygdala. Subdivisions of the
   amygdala have been identified which show specificity of structure,
   connectivity, and function. Little is known about amygdala connectivity
   in ASD. The aim of this study was to investigate the microstructural
   properties of amygdalacortical connections and their association with
   ASD behaviours, and whether connectivity of specific amygdala subregions
   is associated with particular ASD traits. The brains of 51
   high-functioning young adults (25 with ASD; 26 controls) were scanned
   using MRI. Amygdala volume was measured, and amygdalacortical
   connectivity estimated using probabilistic tractography. An iterative
   winner takes all' algorithm was used to parcellate the amygdala based on
   its primary cortical connections. Measures of amygdala connectivity were
   correlated with clinical scores. In comparison with controls, amygdala
   volume was greater in ASD (F(1,94) = 4.19; p = .04). In white matter
   (WM) tracts connecting the right amygdala to the right cortex, ASD
   subjects showed increased mean diffusivity (t = 2.35; p = .05), which
   correlated with the severity of emotion recognition deficits (rho =
   -0.53; p = .01). Following amygdala parcellation, in ASD subjects
   reduced fractional anisotropy in WM connecting the left amygdala to the
   temporal cortex was associated with with greater attention switching
   impairment (rho = -0.61; p = .02). This study demonstrates that both
   amygdala volume and the microstructure of connections between the
   amygdala and the cortex are altered in ASD. Findings indicate that the
   microstructure of right amygdala WM tracts are associated with overall
   ASD severity, but that investigation of amygdala subregions can identify
   more specific associations.},
  address                    = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
  affiliation                = {Gibbard, CR (Corresponding Author), UCL Great Ormond St Inst Child Hlth, Dev Imaging \& Biophys Sect, 30 Guilford St, London WC1N 1EH, England. Gibbard, Clare R.; Clayden, Jonathan D.; Clark, Chris A., UCL Great Ormond St Inst Child Hlth, Dev Imaging \& Biophys Sect, 30 Guilford St, London WC1N 1EH, England. Ren, Juejing; Skuse, David H., UCL Great Ormond St Inst Child Hlth, Behav Sci Unit, 30 Guilford St, London WC1N 1EH, England.},
  author-email               = {c.gibbard@ucl.ac.uk},
  da                         = {2022-09-28},
  doc-delivery-number        = {FV7ZL},
  doi                        = {10.1002/hbm.23915},
  eissn                      = {1097-0193},
  funding-acknowledgement    = {European Union {[}FP7-2009-C 238292]; UCL Grand Challenge Studentship; Erasmus Mundus Chinese Collaborative PhD Studentship; Medical Research Council {[}G1002276, G0300117] Funding Source: researchfish; MRC {[}G1002276, G0300117] Funding Source: UKRI},
  funding-text               = {European Union Grant, Grant Number: FP7-2009-C 238292; UCL Grand Challenge Studentship; Erasmus Mundus Chinese Collaborative PhD Studentship},
  journal-iso                = {Hum. Brain Mapp.},
  keywords                   = {amygdaloid nuclear complex; autism spectrum disorders; diffusion tensor imaging; diffusion tractography},
  keywords-plus              = {DEFICIT HYPERACTIVITY DISORDER; MATTER FRACTIONAL ANISOTROPY; HIGH-FUNCTIONING ADULTS; IN-VIVO; ASPERGER-SYNDROME; SOCIAL IMPAIRMENT; TURNER-SYNDROME; BRAIN CHANGE; CHILDREN; DIFFUSION},
  language                   = {English},
  number-of-cited-references = {91},
  oa                         = {hybrid, Green Published},
  orcid-numbers              = {Clayden, Jon/0000-0002-6608-0619 Hales, Clare/0000-0003-4502-8018},
  publisher                  = {WILEY},
  research-areas             = {Neurosciences \& Neurology; Radiology, Nuclear Medicine \& Medical Imaging},
  researcherid-numbers       = {Clayden, Jon/C-1894-2008},
  times-cited                = {24},
  type                       = {Article},
  unique-id                  = {WOS:000424804400016},
  usage-count-last-180-days  = {2},
  usage-count-since-2013     = {14},
  web-of-science-categories  = {Neurosciences; Neuroimaging; Radiology, Nuclear Medicine \& Medical Imaging},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@article{ WOS:000650601100001,
Author = {Kalabikhina, Irina Evgenievna and Banin, Evgeniy Petrovich and
   Abduselimova, Imiliya Abduselimovna and Klimenko, German Andreevich and
   Kolotusha, Anton Vasilyevich},
Title = {The Measurement of Demographic Temperature Using the Sentiment Analysis
   of Data from the Social Network VKontakte},
Journal = {MATHEMATICS},
Year = {2021},
Volume = {9},
Number = {9},
Month = {MAY},
Abstract = {Social networks have a huge potential for the reflection of public
   opinion, values, and attitudes. In this study, the presented approach
   can allow to continuously measure how cold ``the demographic
   temperature{''} is based on data taken from the Russian social network
   VKontakte. This is the first attempt to analyze the sentiment of
   Russian-language comments on social networks to determine the
   demographic temperature (ratio of positive and negative comments) in
   certain socio-demographic groups of social network users. The authors
   use generated data from the comments to posts from 314 pro-natalist
   groups (with child-born reproductive attitudes) and eight anti-natalist
   groups (with child-free reproductive attitudes) on the demographic
   topic, which have 9 million of users from all over Russia. The algorithm
   of the sentiment analysis for demographic tasks is presented in the
   article. In particularly, it was found that comments under posts are
   more suitable for analyzing the sentiment of statements than the texts
   of posts. Using the available data in two types of groups since 2014, we
   find an asynchronous structural shift in comments of the corpuses of
   pro-natalist and anti-natalist thematic groups. Interpretations of the
   evidences are offered in the discussion part of the article. An
   additional result of our work is two open Russian-language datasets of
   comments on social networks.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Banin, EP (Corresponding Author), Bauman Moscow State Tech Univ, Fac Robot \& Complex Automat, Dept Appl Mech, Baumanskaya 2 Ya St 5-1, Moscow 105005, Russia.
   Kalabikhina, Irina Evgenievna; Abduselimova, Imiliya Abduselimovna; Klimenko, German Andreevich; Kolotusha, Anton Vasilyevich, Lomonosov Moscow State Univ, Fac Econ, Populat Dept, GSP 1, Moscow 119991, Russia.
   Banin, Evgeniy Petrovich, Bauman Moscow State Tech Univ, Fac Robot \& Complex Automat, Dept Appl Mech, Baumanskaya 2 Ya St 5-1, Moscow 105005, Russia.},
DOI = {10.3390/math9090987},
Article-Number = {987},
EISSN = {2227-7390},
Keywords = {reproductive attitudes; demographic temperature; sentiment analysis; big
   data; Latent Dirichlet Allocation; VKontakte},
Keywords-Plus = {STRENGTH DETECTION; NEGATIVE EMOTIONS; MEDIA},
Research-Areas = {Mathematics},
Web-of-Science-Categories  = {Mathematics},
Author-Email = {kalabikhina@econ.msu.ru
   banzay@bmtsu.ru
   abia15a@econ.msu.ru
   klga19a@econ.msu.ru
   koav13a@econ.msu.ru},
ResearcherID-Numbers = {BANIN, EVGENY/L-2074-2015},
ORCID-Numbers = {BANIN, EVGENY/0000-0002-7006-2990},
Funding-Acknowledgement = {Economic Faculty at Lomonosov Moscow State University},
Funding-Text = {The manuscript was prepared with the financial support of the Economic
   Faculty at Lomonosov Moscow State University on research on the topic
   ``Reproduction of the population in a digital society{''}.},
Number-of-Cited-References = {35},
Times-Cited = {1},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {7},
Journal-ISO = {Mathematics},
Doc-Delivery-Number = {SC3UR},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000650601100001},
OA = {gold},
DA = {2022-09-28},
}

@Article{WOS:000527294800005,
  author                     = {Bassell, Julia and Srivastava, Siddharth and Prohl, Anna K. and Scherrer, Benoit and Kapur, Kush and Filip-Dhima, Rajna and Berry-Kravis, Elizabeth and Soorya, Latha and Thurm, Audrey and Powell, Craig M. and Bernstein, Jonathan A. and Buxbaum, Joseph D. and Kolevzon, Alexander and Warfield, Simon K. and Sahin, Mustafa and Dev Synaptopathies Consortium},
  journal                    = {PEDIATRIC NEUROLOGY},
  title                      = {Diffusion Tensor Imaging Abnormalities in the Uncinate Fasciculus and Inferior Longitudinal Fasciculus in Phelan-McDermid Syndrome},
  year                       = {2020},
  issn                       = {0887-8994},
  month                      = {MAY},
  pages                      = {24-31},
  volume                     = {106},
  status                     = {Rejeitado - Escopo},
  abstract                   = {Background: This cohort study utilized diffusion tensor imaging
   tractography to compare the uncinate fasciculus and inferior
   longitudinal fasciculus in children with Phelan-McDermid syndrome with
   age-matched controls and investigated trends between autism spectrum
   diagnosis and the integrity of the uncinate fasciculus and inferior
   longitudinal fasciculus white matter tracts.
   Methods: This research was conducted under a longitudinal study that
   aims to map the genotype, phenotype, and natural history of
   Phelan-McDermid syndrome and identify biomarkers using neuroimaging
   (ClinicalTrial NCT02461420). Patients were aged three to 21 years and
   underwent longitudinal neuropsychologic assessment over 24 months. MRI
   processing and analyses were completed using previously validated image
   analysis software distributed as the Computational Radiology Kit
   (http://crl.med.harvard.edu/). Whole-brain connectivity was generated
   for each subject using a stochastic streamline tractography algorithm,
   and automatically defined regions of interest were used to map the
   uncinate fasciculus and inferior longitudinal fasciculus.
   Results: There were 10 participants (50\% male; mean age 11.17 years)
   with Phelan-McDermid syndrome (n = 8 with autism). Age-matched controls,
   enrolled in a separate longitudinal study (NIH R01 NS079788), underwent
   the same neuroimaging protocol. There was a statistically significant
   decrease in the uncinate fasciculus fractional anisotropy measure and a
   statistically significant increase in uncinate fasciculus mean
   diffusivity measure, in the patient group versus controls in both right
   and left tracts (P <= 0.024).
   Conclusion: Because the uncinate fasciculus plays a critical role in
   social and emotional interaction, this tract may underlie some deficits
   seen in the Phelan-McDermid syndrome population. These findings need to
   be replicated in a larger cohort. (C) 2020 Elsevier Inc. All rights
   reserved.},
  address                    = {STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA},
  affiliation                = {Sahin, M (Corresponding Author), Boston Childrens Hosp, Dept Neurol, 300 Longwood Ave, Boston, MA 02115 USA. Bassell, Julia, Brown Univ, Warren Alpert Med Sch, Providence, RI 02912 USA. Srivastava, Siddharth; Kapur, Kush; Sahin, Mustafa, Harvard Med Sch, Boston Childrens Hosp, Dept Neurol, Boston, MA 02115 USA. Prohl, Anna K.; Scherrer, Benoit; Warfield, Simon K., Harvard Med Sch, Boston Childrens Hosp, Dept Radiol, Computat Radiol Lab, Boston, MA 02115 USA. Filip-Dhima, Rajna; Sahin, Mustafa, Harvard Med Sch, Boston Childrens Hosp, FM Kirby Neurobiol Ctr, Boston, MA 02115 USA. Berry-Kravis, Elizabeth, Rush Univ, Med Ctr, Dept Pediat, Chicago, IL 60612 USA. Berry-Kravis, Elizabeth, Rush Univ, Med Ctr, Dept Neurol Sci, Chicago, IL 60612 USA. Berry-Kravis, Elizabeth, Rush Univ, Med Ctr, Dept Biochem, Chicago, IL 60612 USA. Soorya, Latha, Rush Univ, Med Ctr, Dept Psychiat, Chicago, IL 60612 USA. Thurm, Audrey, NIMH, Pediat \& Dev Neurosci Branch, NIH, Bethesda, MD 20892 USA. Powell, Craig M., Univ Alabama Birmingham, Sch Med, Dept Neurobiol, Birmingham, AL USA. Powell, Craig M., Univ Alabama Birmingham, Sch Med, Civitan Int Res Ctr, Birmingham, AL USA. Bernstein, Jonathan A., Stanford Univ, Dept Pediat, Sch Med, Stanford, CA 94305 USA. Buxbaum, Joseph D.; Kolevzon, Alexander, Mt Sinai Sch Med, Seaver Autism Ctr Res \& Treatment, New York, NY USA. Buxbaum, Joseph D.; Kolevzon, Alexander, Icahn Sch Med Mt Sinai, Dept Psychiat, New York, NY 10029 USA. Buxbaum, Joseph D., Mt Sinai Sch Med, Dept Genet \& Genom Sci, New York, NY USA. Buxbaum, Joseph D., Mt Sinai Sch Med, Dept Neurosci, New York, NY USA.},
  author-email               = {mustafa.sahin@childrens.harvard.edu},
  da                         = {2022-09-28},
  doc-delivery-number        = {LF3AX},
  doi                        = {10.1016/j.pediatrneurol.2020.01.006},
  eissn                      = {1873-5150},
  funding-acknowledgement    = {Developmental Synaptopathies Consortium, National Center for Advancing Translational Sciences Rare Diseases Clinical Research Network {[}U54NS092090]; National Center for Advancing Translational Sciences; National Institute of Mental Health; National Institute of Neurological Disorders and Stroke; Eunice Kennedy Shriver National Institute of Child Health and Human Development; Intramural Research Program of the NIMH {[}ZICMH002961]},
  funding-text               = {This study is supported by the Developmental Synaptopathies Consortium (U54NS092090), which is a part of the National Center for Advancing Translational Sciences Rare Diseases Clinical Research Network. The Rare Diseases Clinical Research Network is an initiative of the Office of Rare Diseases Research of the National Center for Advancing Translational Sciences, and Developmental Synaptopathies Consortium is funded through collaboration between the National Center for Advancing Translational Sciences, the National Institute of Mental Health, the National Institute of Neurological Disorders and Stroke, and the Eunice Kennedy Shriver National Institute of Child Health and Human Development. This research was also supported (in part) by the Intramural Research Program of the NIMH ZICMH002961.},
  journal-iso                = {Pediatr. Neurol.},
  keywords                   = {22q13.3 deletion; SHANK3; DTI; Autism},
  keywords-plus              = {AUTISM SPECTRUM DISORDERS; HIGH-FUNCTIONING AUTISM; FIBER TRACTS; SHANK3; CONNECTIVITY; RECOGNITION; MEMORY; CHILDREN; EMOTION; CORTEX},
  language                   = {English},
  number-of-cited-references = {68},
  oa                         = {Green Accepted},
  orcid-numbers              = {Kapur, Kush/0000-0001-9022-913X White, Stormi/0000-0002-9850-3858 SAHIN, MUSTAFA/0000-0001-7044-2953 Powell, Craig/0000-0001-5451-2165 Phillips, Jennifer/0000-0002-6360-2346 Buxbaum, Joseph/0000-0001-8898-8313},
  publisher                  = {ELSEVIER SCIENCE INC},
  research-areas             = {Neurosciences \& Neurology; Pediatrics},
  researcherid-numbers       = {Buxbaum, Joseph D/G-6001-2010 Kapur, Kush/AAQ-3437-2020},
  times-cited                = {6},
  type                       = {Article},
  unique-id                  = {WOS:000527294800005},
  usage-count-last-180-days  = {0},
  usage-count-since-2013     = {1},
  web-of-science-categories  = {Clinical Neurology; Pediatrics},
  web-of-science-index       = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
}

@Comment{jabref-meta: databaseType:bibtex;}
