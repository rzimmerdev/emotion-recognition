@Misc{Yannakakis2015,
  author    = {Yannakakis, Georgios N. and Paiva, Ana},
  title     = {Emotion in games.},
  year      = {2015},
  abstract  = {Emotion has been investigated from various perspectives and across several domains within human-computer interaction (HCI) including intelligent tutoring systems, interactive web applications, social media, and human-robot interaction. One of the most promising and also challenging applications of affective computing research is within computer games. This chapter focuses on the study of emotion in the computer games domain, reviews seminal work at the crossroads of game technology, game design, and affective computing, and details the key phases for efficient affect-based interaction in games. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {New York, NY, US},
  doi       = {10.1109/TAFFC.2014.2313816},
  issn      = {978-0-19-994223-7 (Hardcover)},
  journal   = {The Oxford handbook of affective computing.},
  keywords  = {*Computers, *Games, *Human Computer Interaction, *Robotics, Emotions},
  pages     = {459--471},
  publisher = {Oxford University Press},
  refid     = {2014-43075-034},
  series    = {Oxford library of psychology.},
}

@Misc{Montoya2013,
  author    = {Montoya, Daniel},
  title     = {Robotic emotions: Cognitive models and the search for the upset robot.},
  year      = {2013},
  abstract  = {The study of emotions in humans has been greatly advanced by new developments in robotic applications. In a previous chapter (Montoya, Baker-Oglesbee, & Bhattacharya, 2011) we reviewed the evidence regarding human emotional response to robots, a response that included behavioral and neural elements. In this chapter, we look at the other side of the equation by describing some of the current models of emotion-generation in artificial systems and detail some of the challenges and the solutions that have been proposed. We succinctly review some of the recent cognitive models utilizing humanoid robots such as iCub and Nao with the underlying assumption that applications based on human development and embodied agents will have the greater chances to succeed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Porto, Portugal},
  issn      = {978-989-643-117-4 (Digital (undefined format))},
  journal   = {Emotional expression: The brain and the face, Vol. 4},
  keywords  = {*Cognition, *Emotions, *Models, Robotics},
  pages     = {197--213},
  publisher = {Edições Universidade Fernando Pessoa},
  refid     = {2014-04196-006},
  series    = {Studies in brain, face and emotion.},
  url       = {https://www.researchgate.net/publication/259231727_Robotic_Emotions_Cognitive_Models_and_the_Search_for_the_Upset_Robot},
}

@Misc{Su2022,
  author    = {Su, Shanshan and Wang, Yuan and Jiang, Wenhui and Zhao, Wenqing and Gao, Rui and Wu, Yanru and Tao, Jing and Su, Yousong and Zhang, Jie and Li, Kangzheng and Zhang, Zhuojun and Zhao, Min and Wang, Zhen and Luo, Yanli and Huang, Xiao and Wang, Lanlan and Wang, Xiaoping and Li, Yi and Jia, Qiufang and Wang, Lianzi and Li, Huafang and Huang, Jingjing and Qiu, Jianyin and Xu, Yifeng},
  title     = {Efficacy of artificial intelligence-assisted psychotherapy in patients with anxiety disorders: A prospective, national multicenter randomized controlled trial protocol.},
  year      = {2022},
  abstract  = {Background: Anxiety disorders have the highest prevalence of all psychiatric disorders in China. Medication and psychotherapy are two main treatment approaches for this group of disorders, and when used in combinations are significantly more beneficial than medication alone. The resources are insufficient. The availability of psychotherapy is low due to the limited resources. Artificial intelligence (AI)-assisted psychotherapy offers an opportunity to develop an efficient and standardized psychotherapy model and improve the availability of psychotherapy, which is key to improve the clinical efficacy of anxiety disorder treatments. Objectives: The present protocol aims to determine whether medication plus AI-assisted psychotherapy has greater efficacy than medication alone in the treatment of anxiety disorders. Methods: We will recruit patients in eight hospitals in China. Seven hundred and eight patients with anxiety disorders will be randomly allocated on a 1:1 basis to either medication plus AI-assisted psychotherapy group, or medication alone group. We have built an AI psychotherapy robot named XIAO AN. In this study we will deliver psychotherapy to patients in the medication plus AI-assisted psychotherapy group. Patients will be assessed at baseline and at the end of week 2, 4, 8, and 12. Follow-up assessments will be conducted at 3 and 6 months posttreatment. The primary outcome is change of Hamilton Anxiety Rating Scale (HAMA) score from baseline the end of 12-week treatment. A secondary efficacy outcome will be improvement in treatment at an early stage (score reduction in HAMA ≥25% after 2 weeks of treatment). Other measurements include Hamilton Depression Scale, Clinical Global Impression, Treatment Emergent Symptom Scale, Social Disability Screening Schedule, Insomnia Severity Index and so on. Scales will be assessed by independent raters who are blind to treatment allocation and analyses will be conducted by a statistician who is also blind to treatment allocation. Discussion: This will be the first multicentered randomized controlled single-blind trial in China to assess the efficacy of medication plus AI-assisted psychotherapy compared with medication alone for anxiety disorders. The study has the potential to address the limitations of the limited availability of psychotherapy, and to augment the efficacy of the treatment of anxiety disorders in China. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Qiu, Jianyin: jianyin_qiu@163.com},
  doi       = {10.3389/fpsyt.2021.799917},
  issn      = {1664-0640(Electronic)},
  journal   = {Frontiers in Psychiatry},
  keywords  = {*Anxiety Disorders, *Artificial Intelligence, *Drug Therapy, Psychotherapy},
  publisher = {Frontiers Media S.A.},
  refid     = {2022-30251-001},
  volume    = {12},
}

@Misc{AlOmair2022,
  author    = {Al-Omair, Osamah M.},
  title     = {Incorporating emotion recognition in co-adaptive systems.},
  year      = {2022},
  abstract  = {The collaboration between human and computer systems has grown astronomically over the past few years. The ability of software systems adapting to human's input is critical in the symbiosis of human-system co-adaptation, where human and software-based systems work together in a close partnership to achieve synergetic goals. However, it is not always clear what kinds of human's input should be considered to enhance the effectiveness of human and system co-adaptation. To address this issue, this research describes an approach that focuses on incorporating human emotion to improve human-computer co-adaption. The key idea is to provide a formal framework that incorporates human emotions as a foundation for explainability into co-adaptive systems, especially, how software systems recognize human emotions and adapt the system's behaviors accordingly. Detecting and recognizing optimum human emotion is a first step towards human and computer symbiosis. As the first step of this research, we conduct a comparative review for a number of technologies and methods for emotion recognition. Specifically, testing the detection accuracy of facial expression recognition of different cloud-services, algorithms, and methods.Secondly, we study the application of emotion recognition within the areas of e-learning, robotics, and explainable artificial intelligence (XAI). We propose a formal framework that incorporates human emotions into an adaptive e-learning system, to create a more personalized learning experience for higher quality of learning outcomes. In addition, we propose a framework for a co-adaptive Emotional Support Robot. This human-centric framework adopts a reinforced learning approach where the system assesses its own emotional re-actions.Finally, we present a formal probabilistic framework that incorporates emotion recognition for explanations and predicting human performance in a co-adaptive scenario. We illustrate the operability of our framework using a Decision Support System with a human operator supervising the system's decisions. We model our approach using a Stock Prediction Engine that was developed in our research lab to predict the price direction of a stock. We use probabilistic model checking to determine how complex an explanation needs to be based on how confused the human is for the purpose of improving the system's overall utility. In addition, we conduct a web-based human experiment to measure the effectiveness of incorporating emotions in improving the outcome of a co-adaptive system. Our study shows that considering human emotions in co-adaptive systems' explanation is one of the important factors for improving the overall systems performance and utility functions. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4217(Print)},
  keywords  = {*Computers, *Facial Expressions, *Human Computer Interaction, *Emotion Recognition, *Affective Computing, Probability, Robotics, Electronic Learning},
  pages     = {No Pagination Specified--No Pagination Specified},
  publisher = {ProQuest Information & Learning},
  refid     = {2022-70650-282},
  url       = {https://dl.acm.org/doi/10.1145/3425329.3425343},
  volume    = {83},
}

@Misc{Singh2016,
  author    = {Singh, Saurabh K. and Jha, Shashi Shekhar and Nair, Shivashankar B.},
  title     = {On realizing emotional memories.},
  year      = {2016},
  abstract  = {Emotion and memory have been two intermingled areas in psychological research. Although researchers are still fairly clueless on how human emotions or memory work, several attempts have been made to copy the dynamics of these two entities in the realm of robotics. This chapter describes one such attempt to capture the dynamics of human emotional memories and model the same for use in a real robot. Emotional memories are created at extreme emotional states, namely, very positive or happy events or very negative ones. The positive ones result in the formation of positive memories while the negative ones form the negative counterparts. The robotic system seeks the positive ones while it tries to avoid the negative ones. Such memories aid the system in making the right decisions, especially when situations similar to the one which caused their generation, repeat in the future. This chapter introduces the manner in which a multi-agent emotion engine churns out the emotions which in turn generate emotional memories. Results obtained from simulations and those from using a real situated robot described herein, validate the working of these memories. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Hershey, PA, US},
  doi       = {10.4018/978-1-5225-0159-6.ch072},
  issn      = {1-522-50159-2 (Hardcover); 978-1-522-50159-6 (Hardcover); 978-1-522-50160-2 (Digital (undefined format))},
  journal   = {Psychology and mental health: Concepts, methodologies, tools, and applications, Vols. 1-3},
  keywords  = {*Decision Making, *Robotics, *Positive Emotions, Memory, Negative Emotions},
  pages     = {1652--1689},
  publisher = {Information Science Reference/IGI Global},
  refid     = {2017-00889-072},
}

@Misc{Mewafy2022,
  author    = {Mewafy, Sherif El Sayed Mohamed Mohamed},
  title     = {Investigation into the creation of an ambient intelligent physiology measurement environment to facilitate modelling of the human wellbeing.},
  year      = {2022},
  status    = {Rejeitado: Inacessivel},
  abstract  = {The elderly population worldwide has an increasing expectation of wellbeing and life expectancy. The monitoring of the majority of elderly people on an individual basis, in a medical sense, will not be a viable proposition in the future due to the projected numbers of individuals requiring such activity. The expectation is that the infrastructure available will not be adequate to meet all the anticipated requirements and subsequently people will have to live at home with inadequate care. A new global objective that aims towards enhancing the quality of life of the elderly is being supported by extensive research. This research has been taking place in the field of ambient intelligence (AmI), considering factors including more comfort, improved health, enhanced security for the elderly, and facilitating the living in their homes longer. Prior research has shown a need for accelerated expansion in the ambient intelligence domain. To that end this work presents a novel learning technique for intelligent agents that can be used in Ambient Intelligent Environments (AIEs).The main objective of this work is to add knowledge to the AmI domain and to explore the practical applications within this research field. The added knowledge is accomplished through the development of an ambient intelligent health care environment that allows a practical assessment of the human well-being to take place. This is achieved by transforming the elderly living environment into an intelligent pseudo robot within which they reside to better understand the human wellbeing.The system developed aims to provide evidence that a level of automated care is both possible and practical. This care is for those with chronic physical or mental disabilities who have difficulty in their interactions with standardised living spaces. The novel integrated hardware and software architecture provides personalised environmental monitoring. It also provides control facilities based on the patient's physical and emotional wellness in their home.Entitled Health Adaptive Online Emotion Fuzzy Agent (HAOEFA), the system provides a non-invasive, self-learning, intelligent controlling system that constantly adapts to the requirements of an individual. The system has the ability to model and learn the user behaviour in order to control the environment on their behalf. This is achieved with respect to the changing environmental conditions as well as the user's health and emotional states being detected. A change of emotion can have a direct impact on the system's control taking place in the environment. Thus HAOEFA combines an emotion recognition system within a fuzzy logic learning and adaptation based controller. The emotion recogniser detects the occupant's emotions upon the changes of the physiological data being monitored. In addition to acting as an output to the occupant's physiological changes, the detected emotion also acts as input to the whole situation being observed by HAOEFA. This allows HAOEFA to control the Glam i-HomeCare on the user's behalf with respect to their emotional status.The system developed incorporates real-time, continuous adaptations to facilitate any changes to the occupant's behaviour within the environment. It also allows the rules to be adapted and extended online, assisting a life-long learning technique as the environmental conditions change and the user behaviour adjusts with it. HAOEFA uses the fuzzy c-means clustering methodology for extracting membership functions (MFs) before building its set of fuzzy rules. These MFs together with the rules base constitute a major part of the proposed system. It has the ability to learn and model the individual human behaviour with respect to their emotional status.Following the provided literature review and the presentation of Fuzzy logic MFs (see section 3.3). (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4217(Print)},
  keywords  = {*Adaptive Behavior, *Learning, *Measurement, *Physiology, *Well Being, Adaptation, Adjustment, Intelligence},
  pages     = {No Pagination Specified--No Pagination Specified},
  publisher = {ProQuest Information & Learning},
  refid     = {2022-56507-121},
  volume    = {83},
}

@Misc{Montoya2011,
  author    = {Montoya, Daniel and Baker-Oglesbee, Alissa and Bhattacharya, Sambit},
  title     = {What the robot sees, what the human feels: Robotic face detection and the human emotional response.},
  year      = {2011},
  status    = {Rejeitado: Inacessivel},
  abstract  = {New developments in robotics have brought humans and robots interacting in human environments. Research has focused its attention on the development of human-like virtual displays and robotics, while parallel lines of research have focused on the study of human responses to robotic agents with special emphasis in human’s emotional reaction. This chapter explores the intersection between robotics and neurosciences with special emphasis in human-robot interactions (HRI). We briefly present recent innovations in the context of robotic face detection and recognition as well as human physiological and cognitive response to the presence of artificial agents. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Montoya, Daniel: dmontoya@uncfsu.edu},
  issn      = {978-989-643-084-9 (Paperback)},
  journal   = {Emotional expression: The brain and the face, Vol. 3},
  keywords  = {*Emotional Responses, *Face Perception, *Neurosciences, Robotics},
  pages     = {43--71},
  publisher = {Edições Universidade Fernando Pessoa},
  refid     = {2011-30587-002},
  series    = {Studies in brain, face, and emotion.},
}

@Misc{Shen2016,
  author    = {Shen, Solace},
  title     = {Children's conceptions of the moral standing of a humanoid robot of the here and now.},
  year      = {2016},
  status    = {Rejeitado: Inacessivel},
  abstract  = {Sophisticated humanoid robots have recently moved from the laboratory and research settings to the home environment. Some models are now marketed to families with children, and are designed to engage children in increasingly social and potentially moral interactions. The purpose of this study is to investigate---when children interact with a commercially available humanoid robot and witness a human causing a harm to the robot---whether children conceptualize the robot as being an entity that deserves moral consideration: what is referred to in this study as having moral standing. Participants included 120 children in 2 age groups (8-9 and 14-15). To assess the effects of the robot's physical embodiment on participants' conceptions of the robot's moral standing, 30 participants from each age group (gender balanced) were randomly assigned to 1 of 2 conditions and interacted with either a humanoid robot or an analogous virtual agent. In each condition, the interaction culminated in a confederate hitting the robot/virtual agent with a book. Each participant was then engaged in a semi-structured interview that ascertained their judgments and reasoning regarding the robot/virtual agent and three comparison entities. Results show that the majority of the participants judged the confederate hitting the robot as a violation of moral obligation, and many brought the concept of artificial emotion to bear in their reasoning. Participants were significantly more likely to judge it a violation of moral obligation to hit the robot than the virtual agent. Moreover, the 8- to 9-year-olds were significantly more likely than the 14- to 15-year-olds to judge it a violation of moral obligation to hit either the robot or the virtual agent. Finally, participants' conceptions of the robot's moral standing largely showed a unique composition of moral features when compared to those of the comparison entities. Discussion addresses the broader implications of these findings and future directions for research are offered. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4217(Print)},
  keywords  = {*Childhood Development, *Judgment, *Morality, Robotics},
  pages     = {No Pagination Specified--No Pagination Specified},
  publisher = {ProQuest Information & Learning},
  refid     = {2016-42148-015},
  volume    = {77},
}

@Misc{Hsieh2022,
  author    = {Hsieh, Te-Yi and Cross, Emily S.},
  title     = {People’s dispositional cooperative tendencies towards robots are unaffected by robots’ negative emotional displays in prisoner’s dilemma games.},
  year      = {2022},
  abstract  = {Abstract The study explores the impact of robots’ emotional displays on people’s tendency to cooperate with a robot opponent in prisoner’s dilemma games. Participants played iterated prisoner’s dilemma games with a non-expressive robot (as a measure of cooperative baseline), followed by an angry, and a sad robot, in turn. Based on the Emotion as Social Information model, we expected participants with higher cooperative predispositions to cooperate less when a robot displayed anger, and cooperate more when the robot displayed sadness. Contrarily, according to this model, participants with lower cooperative predispositions should cooperate more with an angry robot and less with a sad robot. The results of 60 participants failed to support the predictions. Only the participants’ cooperative predispositions significantly predicted their cooperative tendencies during gameplay. Participants who cooperated more in the baseline measure also cooperated more with the robots displaying sadness and anger. In exploratory analyses, we found that participants who accurately recognised the robots’ sad and angry displays tended to cooperate less with them overall. The study highlights the impact of personal factors in human-robot cooperation, and how these factors might surpass the influence of bottom-up emotional displays by the robots in the present experimental scenario. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Cross, Emily S.: e.cross@westernsydney.edu.au},
  doi       = {10.1080/02699931.2022.2054781},
  issn      = {1464-0600(Electronic),0269-9931(Print)},
  journal   = {Cognition and Emotion},
  pages     = {No Pagination Specified--No Pagination Specified},
  publisher = {Taylor & Francis},
  refid     = {2022-53720-001},
}

@Misc{Marchetti2020,
  author    = {Marchetti, Antonella and Di Dio, Cinzia and Massaro, Davide and Manzi, Federico},
  title     = {The psychosocial fuzziness of fear in the coronavirus (COVID-19) era and the role of robots.},
  year      = {2020},
  abstract  = {The article reflects on the psychosocial fuzziness of fear in the coronavirus (COVID-19) era and the role of robots. In reading the new relational dynamics hypothesized in the present work, from which the robot is spared, COVID-19 pandemics added novelty to the physiognomy of fear, which (unlike anxiety) is an emotion linked to objects and situational antecedents, and which may therefore be affected by the nature of its objects at the level of subjective experiences, behavioral reactions, as well as coping strategies. These theoretical suggestions may enrich knowledge from an interdisciplinary perspective, such as robotics and psychology, providing important starting points for future research by emphasizing which psychological components should be investigated in people interacting with robots. An example is the perception of in-group/out-group, as well as the components of fear that, in our opinion, are mitigated toward robots in the specific COVID-19 situation, which forces us to adapt to the inclusion of new social agents devoted to care assistance. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Manzi, Federico: federico.manzi@unicatt.it},
  doi       = {10.3389/fpsyg.2020.02245},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Fear, *Pandemics, *Psychosocial Factors, *Robotics, *Coronavirus, COVID-19, Physical Distancing},
  publisher = {Frontiers Media S.A.},
  refid     = {2020-76187-001},
  volume    = {11},
}

@Misc{Maiese2014,
  author    = {Maiese, Michelle},
  title     = {Body and emotion.},
  year      = {2014},
  status    = {Rejeitado: Inacessivel},
  abstract  = {My claim that emotions are essentially embodied, enactive appraisals has important implications for artificial intelligence (AI). Much of the work being done in AI begins with the notion that it is possible to formulate "explicitly specifiable rules of thought" that govern the move from one cognitive state to another and then program these rules into a computer. Early work in AI and robotics often involved robots operating in mostly static environments that to some extent had been designed specifically for them. In the real world, however, things change. To be truly "intelligent," a robot must be able to cope and adjust its plan of action, and thus be truly dynamic and immediately sensitive to change. However, the robot should not have to replan as a result of every change to its surroundings, but instead only in response to changes that are relevant to its goals. It seems clear that this ability to detect relevance is a crucial aspect of human intelligence, but it is unclear how to get a robot to accomplish this simply by following an algorithm. Programming many thousands of facts into the computer hardly helps, since effective agency requires that the computer determine which facts are relevant to its proposed action. Even if the computer had a straight forward set of relevance rules, it is unclear that it could apply these rules successfully in any efficient way. What is relevant is constantly changing, based on the interplay between various aspects of the environment, situational factors, and the robot's particular abilities. Changing any one factor can change the relevance of some other factor. Carried out by a computer system manipulating formal symbols, all this rule-following would take too long and would be too cognitively "expensive". Moreover, it seems that "in order to identify the possibly relevant facts in the current situation one would need a frame for recognizing," but this would result in a "regress of frames for recognizing relevant frames for recognizing relevant facts". Without an immediate, intuitive means of detecting relevance, robots could respond only to fixed features of their surroundings. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {New York, NY, US},
  issn      = {978-0-415-62361-2 (Hardcover); 978-1-315-77584-5 (PDF)},
  journal   = {The Routledge handbook of embodied cognition.},
  keywords  = {*Cognitive Ability, *Cognitive Processes, *Emotions, Intelligence},
  pages     = {231--239},
  publisher = {Routledge/Taylor & Francis Group},
  refid     = {2014-19965-022},
  series    = {Routledge handbooks in philosophy.},
}

@Misc{Pak2020,
  author    = {Pak, Richard and de Visser, Ewart J. and Rovira, Ericka},
  title     = {Living with robots: Emerging issues on the psychological and social implications of robotics.},
  year      = {2020},
  status    = {Rejeitado: Inacessivel},
  abstract  = {This book focuses on the issues that come to bear when humans interact and collaborate with robots. It dives deeply into critical factors that impact how individuals interact with robots at home, work and play. It includes topics ranging from robot anthropomorphic design, degree of autonomy, trust, individual differences and machine learning. While other books focus on engineering capabilities or the highly conceptual, philosophical issues of human-robot interaction, this resource tackles the human elements at play in these interactions, which are essential if humans and robots are to coexist and collaborate effectively. Authored by key psychology robotics researchers, the book limits its focus to specifically those robots who are intended to interact with people, including technology such as drones, self-driving cars, and humanoid robots. Forward-looking, the book examines robots not as the novelty they used to be, but rather the practical idea of robots participating in our everyday lives. It explores how individual differences in cognitive abilities and personality influence human-robot interaction and examines the human response to robot autonomy Includes tools and methods for the measurement of social emotion with robots. The book also delves into a broad range of domains--military, caregiving, toys, surgery, and more. It anticipates the issues we will encountering with robots in the next ten years. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {San Diego, CA, US},
  issn      = {9780128153673 (Paperback)},
  journal   = {Living with robots: Emerging issues on the psychological and social implications of robotics.},
  keywords  = {*Robotics, *Technology, *Human Robot Interaction, Caregivers, Cognitive Ability, Emotions, Machine Learning, Surgery, Autonomy, Caregiving},
  pages     = {xiv, 204--xiv, 204},
  publisher = {Elsevier Academic Press},
  refid     = {2019-72311-000},
}

@Misc{Nadel2015,
  author    = {Nadel, Jacqueline and Han, Bora},
  title     = {Emotion on board.},
  year      = {2015},
  abstract  = {Emotion is on board from first. Fetal facial expressions change strikingly fast, thus indicating that they are actively practiced before birth. What for? Are we wired to express our emotions and to resonate to others’? Blank faces or non contingent faces cause disturbance early on life. When mother does not respond contingently to her 2-month-old, the infant mood turns to negative expression. Children with autism, even low-functioning ones, try to reconnect with a still person. Children track preferentially core expressive features, even if those features are generated by a robot. Our EEG studies with adults indicate that emotion is captured regardless of the vehicle (human or robotic stimuli). This suggests that emotional development leads to decipher emotional signals beyond faces. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Amsterdam, Netherlands},
  doi       = {10.1075/ceb.10.03nad},
  issn      = {978-90-272-4160-3 (Hardcover); 978-90-272-6765-8 (Digital (undefined format))},
  journal   = {Emotion in language: Theory - research - application.},
  keywords  = {*Childhood Development, *Emotional Responses, Facial Expressions},
  pages     = {49--67},
  publisher = {John Benjamins Publishing Company},
  refid     = {2016-06464-003},
  series    = {Consciousness and emotion book series.},
}

@Misc{Egbert2015,
  author    = {Egbert, Matthew D. and Barandiaran, Xabier E.},
  title     = {"Modeling habits as self-sustaining patterns of sensorimotor behavior": Corrigendum.},
  year      = {2015},
  abstract  = {Reports an error in "Modeling habits as self-sustaining patterns of sensorimotor behavior" by Matthew D. Egbert and Xabier E. Barandiaran (Frontiers in Human Neuroscience, 2014[Aug][8], Vol 8[590]). In the original article, Equation 8 is incorrectly written. The correction equation is present in the erratum. (The following abstract of the original article appeared in record 2014-52023-001). In the recent history of psychology and cognitive neuroscience, the notion of habit has been reduced to a stimulus-triggered response probability correlation. In this paper we use a computational model to present an alternative theoretical view (with some philosophical implications), where habits are seen as self-maintaining patterns of behavior that share properties in common with self-maintaining biological processes, and that inhabit a complex ecological context, including the presence and influence of other habits. Far from mechanical automatisms, this organismic and self-organizing concept of habit can overcome the dominating atomistic and statistical conceptions, and the high temporal resolution effects of situatedness, embodiment and sensorimotor loops emerge as playing a more central, subtle and complex role in the organization of behavior. The model is based on a novel “iterant deformable sensorimotor medium (IDSM),” designed such that trajectories taken through sensorimotor-space increase the likelihood that in the future, similar trajectories will be taken. We couple the IDSM to sensors and motors of a simulated robot, and show that under certain conditions, the IDSM conditions, the IDSM forms self-maintaining patterns of activity that operate across the IDSM, the robot’s body, and the environment. We present various environments and the resulting habits that form in them. The model acts as an abstraction of habits at a much needed sensorimotor “meso-scale” between microscopic neuron-based models and macroscopic descriptions of behavior. Finally, we discuss how this model and extensions of it can help us understand aspects of behavioral self-organization, historicity and autonomy that remain out of the scope of contemporary representationalist frameworks. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Egbert, Matthew D.: Embodied Emotion, Cognition and (Inter-)Action Lab, School of Computer Science, University of Hertfordshire, College Lane, Hatfield, HRT, United Kingdom, AL10 9AB, mde@matthewegbert.com},
  doi       = {10.3389/fnhum.2015.00209},
  issn      = {1662-5161(Electronic)},
  journal   = {Frontiers in Human Neuroscience},
  keywords  = {*Habits, *Mathematical Modeling, *Perceptual Motor Processes, Simulation},
  publisher = {Frontiers Media S.A.},
  refid     = {2015-35773-001},
  volume    = {9},
}

@Misc{Chu2022,
  author    = {Chu, Li},
  title     = {Do we become more or less curious with age? Examining the roles of age and personal relevance.},
  year      = {2022},
  status    = {Rejeitado: Inacessivel},
  abstract  = {According to socioemotional selectivity theory (Carstensen, 1995), people tend to prioritize emotionally meaningful goals over information-seeking goals in later adulthood. However, being curious and motivated to learn is becoming more essential for older adults today than ever before because there are more situations in which older adults have to adapt to novelties (e.g., new life style and novel technologies). Thus, it would be important to understand the association between age and different types of curiosity (e.g., trait versus state) and potential factors that contribute to this association. Through three manuscripts, this thesis aimed to provide more in-depth understanding of the association between age and trait curiosity (Manuscript 1), examine the moderating role of personal relevance in the association between age and state curiosity toward novelties (Manuscript 2) and evaluate the affective experience of state curiosity for younger and older adults (Manuscript 3). In Manuscript 1 (Chu, Tsai, & Fung, 2020), the association between age and trait curiosity was examined. Specifically, this study investigated the underlying mechanisms for age-related declines in intellectual curiosity. A moderated serial multiple mediation model revealed that the association between age and intellectual curiosity was mediated by future time perspective and perceived importance of curiosity. Manuscript 2 utilized a pre- and post-test experimental design and a survey to investigate the relationship between age and different types of curiosity, and the role of personal relevance in this relationship. In general, older adults showed a lower level of trait curiosity than did younger adults, but patterns of age differences in state curiosity were less consistent across the two studies reported in this manuscript. Moreover, older adults who perceived increased personal relevance after interacting with a robot were significantly more curious than older adults who did not perceived increased relevance, but this pattern was not found in younger adults. Finally, Manuscript 3 included a survey study and a time-sampling study to investigate the affective experience of being curious among younger and older adults. The multilevel modelling results illustrated a positive relationship between state curiosity and momentary happiness for both younger and older adults. Yet, momentary anxiousness was positively associated with curiosity of younger adults only but not for older adults. Moreover, a significant three-way interaction between age group, happiness and anxiousness was found, suggesting that older adults were more likely to perceive curiosity as a mixed emotional experience and younger adults were more likely to perceive curiosity as an either-happy-oranxious experience. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4217(Print)},
  keywords  = {*Curiosity, *Emotions, *Learning, *Openness to Experience, *Older Adulthood, Age Differences, Happiness, Motivation},
  pages     = {No Pagination Specified--No Pagination Specified},
  publisher = {ProQuest Information & Learning},
  refid     = {2022-34092-052},
  volume    = {83},
}

@Misc{Schirmer2022,
  author    = {Schirmer, Annett and Cham, Clare and Zhao, Zihao and Lai, Oscar and Lo, Clive and Croy, Ilona},
  title     = {Understanding sex differences in affective touch: Sensory pleasantness, social comfort, and precursive experiences.},
  year      = {2022},
  abstract  = {Although previous research revealed sex differences in affective touch, the implicated processes and the manner in which men and women differ have been left uncertain. Here we addressed this issue in two studies examining sensory pleasure, interpersonal comfort, and touch motivators. Study 1 comprised a series of lab-based experiments in which a robot stroked 214 participants (half female) at five different velocities modulating the activity of C-tactile afferents thought to support tactile pleasantness. Average pleasantness ratings followed velocity with the typical inverted u-shape similarly in both sexes. In Study 2, 260 participants (half female) completed an online survey. Here, women were more likely than men to express touch comfort with less familiar or unknown individuals, had a greater preference for touch with other women, and felt more comfortable giving and receiving touch to the forearm. Additionally, when describing how their own experiences might motivate others to touch them affectively, women produced more negative descriptions than men. Together, these results show that, while the sexes compare in a touch's sensory pleasantness, they differ in their preceding affective experiences and how they value touch at a higher-order social level. This agrees with extant research on negative affect and stress and suggests that affective touch may be a more relevant coping mechanism for women than for men. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Schirmer, Annett: dr.annettschirmer@gmail.com},
  doi       = {10.1016/j.physbeh.2022.113797},
  issn      = {1873-507X(Electronic),0031-9384(Print)},
  journal   = {Physiology & Behavior},
  keywords  = {*Human Sex Differences, *Pleasure, *Tactual Perception, *Negative Emotions, Animal Sex Differences, Experimental Methods, Surveys, Velocity},
  pages     = {1--11},
  publisher = {Elsevier Science},
  refid     = {2022-55224-001},
  volume    = {250},
}

@Misc{Korcsok2018,
  author    = {Korcsok, Beáta and Konok, Veronika and Persa, György and Faragó, Tamás and Niitsuma, Mihoko and Miklósi, Ádám and Korondi, Péter and Baranyi, Péter and Gácsi, Márta},
  title     = {Biologically inspired emotional expressions for artificial agents.},
  year      = {2018},
  abstract  = {A special area of human-machine interaction, the expression of emotions gains importance with the continuous development of artificial agents such as social robots or interactive mobile applications. We developed a prototype version of an abstract emotion visualization agent to express five basic emotions and a neutral state. In contrast to well-known symbolic characters (e.g., smileys) these displays follow general biological and ethological rules. We conducted a multiple questionnaire study on the assessment of the displays with Hungarian and Japanese subjects. In most cases participants were successful in recognizing the displayed emotions. Fear and sadness were most easily confused with each other while both the Hungarian and Japanese participants recognized the anger display most correctly. We suggest that the implemented biological approach can be a viable complement to the emotion expressions of some artificial agents, for example mobile devices. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  address   = {Korcsok, Beáta: korcsok@mogi.bme.hu},
  doi       = {10.3389/fpsyg.2018.01191},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Artificial Intelligence, *Human Computer Interaction, Emotional Responses},
  publisher = {Frontiers Media S.A.},
  refid     = {2018-36037-001},
  volume    = {9},
}

@Misc{Eimler2011,
  author    = {Eimler, Sabrina C. and Krämer, Nicole C. and von der Pütten, Astrid M.},
  title     = {Empirical results on determinants of acceptance and emotion attribution in confrontation with a robot rabbit.},
  year      = {2011},
  abstract  = {As robots increasingly enter people’s everyday lives, it becomes ever more important to explore the conditions and determinants of acceptance of human interactions with these devices. Moreover, a positive feeling associated with the interaction with robots is a precondition for the user’s willingness to engage in further interactions and establish long-term relationships. This article presents empirical results from two studies that focus on the user’s perception of the robot rabbit Nabaztag, a small WiFi-enabled device with movable ears, integrated RFID reader functionality, and speech-synthesis capability. In the first study, 53 participants were confronted with a range of the Nabaztag’s functionality, and, using RFID cards, they interacted with the rabbit. The analysis of people’s answers concerning Perceived Ease of Use and Perceived Usefulness as well as hedonic and pragmatic aspects, showed gender-accorded differences regarding the evaluation of the device. Neither the degree of familiarity with computers nor the fact of whether technical disfunctionality occurred during the trial influenced the evaluation of the robot, while ownership of a robotic toy let people evaluate the Nabaztag more positively. The second study took a more detailed observance of the effect of the rabbit’s expression by its ears. In a within-subjects setting, a German (N = 100) and a U.S. American sample (N = 111) were asked to rate the rabbit’s current emotional status from pictures that showed the rabbit with a variety of six different ear positions. Results indicate that people infer specific emotional states from the robot rabbit’s different ear positions. Also illustrated is that observers’ attribution of feelings to the rabbit depends on their cultural backgrounds. Implications and questions for future research are discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Eimler, Sabrina C.: University of Duisburg-Essen, Forsthausweg 2, Duisburg, Germany, 47048, sabrina.elmer@uni-due.de},
  doi       = {10.1080/08839514.2011.587154},
  issn      = {1087-6545(Electronic),0883-9514(Print)},
  journal   = {Applied Artificial Intelligence},
  keywords  = {*Attribution, *Robotics, *Social Interaction, *Human Robot Interaction, Emotions, Rabbits},
  pages     = {503--529},
  publisher = {Taylor & Francis},
  refid     = {2011-14806-004},
  volume    = {25},
}

@Misc{Parisi2011,
  author    = {Parisi, Domenico},
  title     = {The other half of the embodied mind.},
  year      = {2011},
  abstract  = {Embodied theories of mind tend to be theories of the cognitive half of the mind and to ignore its emotional half while a complete theory of the mind should account for both halves. Robots are a new way of expressing theories of the mind which are less ambiguous and more capable to generate specific and non-controversial predictions than verbally expressed theories. We outline a simple robotic model of emotional states as states of a sub-part of the neural network controlling the robot's behavior which has specific properties and which allows the robot to make faster and more correct motivational decisions, and we describe possible extensions of the model to account for social emotional states and for the expression of emotions that, unlike those of current “emotional” robots, are really “felt” by the robot in that they play a well-identified functional role in the robot's behavior. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  address   = {Parisi, Domenico: Laboratory of Artificial Life and Robotics, Institute of Cognitive Sciences and Technologies, National Research Council, Viale Marx 15, Rome, Italy, 00137, domenico.parisi@istc.cnr.it},
  doi       = {10.3389/fpsyg.2011.00069},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Emotional Control, *Mind, Robotics},
  publisher = {Frontiers Media S.A.},
  refid     = {2017-41550-001},
  volume    = {2},
}

@Misc{Barros2015,
  author    = {Barros, Pablo and Jirak, Doreen and Weber, Cornelius and Wermter, Stefan},
  title     = {Multimodal emotional state recognition using sequence-dependent deep hierarchical features.},
  year      = {2015},
  abstract  = {Emotional state recognition has become an important topic for human-robot interaction in the past years. By determining emotion expressions, robots can identify important variables of human behavior and use these to communicate in a more human-like fashion and thereby extend the interaction possibilities. Human emotions are multimodal and spontaneous, which makes them hard to be recognized by robots. Each modality has its own restrictions and constraints which, together with the non-structured behavior of spontaneous expressions, create several difficulties for the approaches present in the literature, which are based on several explicit feature extraction techniques and manual modality fusion. Our model uses a hierarchical feature representation to deal with spontaneous emotions, and learns how to integrate multiple modalities for non-verbal emotion recognition, making it suitable to be used in an HRI scenario. Our experiments show that a significant improvement of recognition accuracy is achieved when we use hierarchical features and multimodal information, and our model improves the accuracy of state-of-the-art approaches from 82.5% reported in the literature to 91.3% for a benchmark dataset on spontaneous emotion expressions. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Barros, Pablo: Department of Informatics, University of Hamburg, Vogt-Koelln-Strasse 30, Hamburg, Germany, 22527, barros@informatik.uni-hamburg.de},
  doi       = {10.1016/j.neunet.2015.09.009},
  issn      = {1879-2782(Electronic),0893-6080(Print)},
  journal   = {Neural Networks},
  keywords  = {*Emotional States, *Mathematical Modeling, *Neural Networks, *Robotics, *Human Robot Interaction, Facial Expressions, Learning},
  pages     = {140--151},
  publisher = {Elsevier Science},
  refid     = {2015-51027-001},
  volume    = {72},
}

@Misc{Vallverdu2015,
  author    = {Vallverdú, Jordi},
  title     = {Handbook of research on synthesizing human emotion in intelligent systems and robotics.},
  year      = {2015},
  abstract  = {The emotional flavour of all the spheres of human activities is a matter of fact, beyond any reasonable doubt for everyone interested on human cognition. Besides, emotional aspects can be also found as non-linguistic utterances that define the social domain of human life. From our bodies to social groups, emotions act as ways to regulate and model these exchanges. Emotions and feelings are basic regulators of human activity. In fact, they are the base of our interaction with the world: through pleasure, pain, hunger or fear, we create intentional dispositions, acting like homeostatic controls over our actions. From the basic emotions to complex ones, humans share a common nature to give sense to the world. Recent decades of scientific research on neurophysiology have shown how emotions are not simply a part of human activity, but a fundamental one. Emotional states had been historically banned in the territory of human rationality (Descartes and his “res cogitans” was the last and most convincing proponent of this approach). And evolutionary approaches to consciousness or studies of emotions have shown that the origin of consciousness, lying in the structure of the nervous system (which enables the data feedback loops, the cause of the emergence of consciousness) might be emotion (rather than perception) and that experienced sensations (i.e. qualia) inherently require someone to experience them. On the other hand, there are several kinds of studies of emotions in synthetic environments, such as affective computing or sociable robots. Some authors have also tried to develop computational models of artificial emotions or have drawn attention to the interesting phenomenon of emotions within artificial environments. For the previous reason, this book covers an important area: how to recognize, model and implement emotions, from natural domains to artificial ones. Human-Robot Interaction (henceforth, HRI), for example, is one of the hottest topics in contemporary research: there is a necessity for a better understanding of how both can collaborate and share spaces and actions. The potential readers of this publication can be classified under two main groups: active researchers (those implied actually into artificial emotions as well as related fields like AI, computing, robotics, philosophy, psychology, and so forth) and university students on these or general grades, because this book will not only provide an excellent frame for future researchers but tries at the same time to stimulate young and brilliant students towards the field of synthetic emotions. This publication also provides a solid conceptual frame for scientists and an opportunity for thinkers to develop new research lines for the future of the topic. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Hershey, PA, US},
  doi       = {10.4018/978-1-4666-7278-9},
  issn      = {978-1-4666-7278-9 (Hardcover); 978-1-4666-7279-6 (Digital (undefined format))},
  journal   = {Handbook of research on synthesizing human emotion in intelligent systems and robotics.},
  keywords  = {*Artificial Intelligence, *Expert Systems, *Robotics, Cognition, Doubt},
  pages     = {xxv, 469--xxv, 469},
  publisher = {Information Science Reference/IGI Global},
  refid     = {2015-24110-000},
  series    = {Advances in computational intelligence and robotics (ACIR) book series.},
}

@Misc{Singh2015,
  author    = {Singh, Saurabh K. and Jha, Shashi Shekhar and Nair, Shivashankar B.},
  title     = {On realizing emotional memories.},
  year      = {2015},
  abstract  = {Emotion and memory have been two intermingled areas in psychological research. Although researchers are still fairly clueless on how human emotions or memory work, several attempts have been made to copy the dynamics of these two entities in the realm of robotics. This chapter describes one such attempt to capture the dynamics of human emotional memories and model the same for use in a real robot. Emotional memories are created at extreme emotional states, namely, very positive or happy events or very negative ones. The positive ones result in the formation of positive memories while the negative ones form the negative counterparts. The robotic system seeks the positive ones while it tries to avoid the negative ones. Such memories aid the system in making the right decisions, especially when situations similar to the one which caused their generation, repeat in the future. This chapter introduces the manner in which a multi-agent emotion engine churns out the emotions which in turn generate emotional memories. Results obtained from simulations and those from using a real situated robot described herein, validate the working of these memories. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Hershey, PA, US},
  doi       = {10.4018/978-1-4666-7278-9.ch005},
  issn      = {978-1-4666-7278-9 (Hardcover); 978-1-4666-7279-6 (Digital (undefined format))},
  journal   = {Handbook of research on synthesizing human emotion in intelligent systems and robotics.},
  keywords  = {*Decision Making, *Emotional Control, *Intelligent Agents, Memory},
  pages     = {116--151},
  publisher = {Information Science Reference/IGI Global},
  refid     = {2015-24110-005},
  series    = {Advances in computational intelligence and robotics (ACIR) book series.},
}

@Misc{Lindblom2015,
  author    = {Lindblom, J. and Alenljung, B.},
  title     = {Socially embodied human-robot interaction: Addressing human emotions with theories of embodied cognition.},
  year      = {2015},
  abstract  = {A fundamental challenge of human interaction with socially interactive robots, compared to other interactive products, comes from them being embodied. The embodied nature of social robots questions to what degree humans can interact ‘naturally’ with robots, and what impact the interaction quality has on the user experience (UX). UX is fundamentally about emotions that arise and form in humans through the use of technology in a particular situation. This chapter aims to contribute to the field of human-robot interaction (HRI) by addressing, in further detail, the role and relevance of embodied cognition for human social interaction, and consequently what role embodiment can play in HRI, especially for socially interactive robots. Furthermore, some challenges for socially embodied interaction between humans and socially interactive robots are outlined and possible directions for future research are presented. It is concluded that the body is of crucial importance in understanding emotion and cognition in general, and, in particular, for a positive user experience to emerge when interacting with socially interactive robots. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Hershey, PA, US},
  doi       = {10.4018/978-1-4666-7278-9.ch007},
  issn      = {978-1-4666-7278-9 (Hardcover); 978-1-4666-7279-6 (Digital (undefined format))},
  journal   = {Handbook of research on synthesizing human emotion in intelligent systems and robotics.},
  keywords  = {*Cognition, *Emotions, *Robotics, *Social Interaction, Human Robot Interaction},
  pages     = {169--190},
  publisher = {Information Science Reference/IGI Global},
  refid     = {2015-24110-007},
  series    = {Advances in computational intelligence and robotics (ACIR) book series.},
}

@Misc{Goeruer2015,
  author    = {Görür, O. Can and Erkmen, Aydan M.},
  title     = {Intention and body-mood engineering via proactive robot moves in HRI.},
  year      = {2015},
  abstract  = {This chapter focuses on emotion and intention engineering by socially interacting robots that induce desired emotions/intentions in humans. The authors provide all phases that pave this road, supported by overviews of leading works in the literature. The chapter is partitioned into intention estimation, human body-mood detection through external-focused attention, path planning through mood induction and reshaping intention. Moreover, the authors present their novel concept, with implementation, of reshaping current human intention into a desired one, using contextual motions of mobile robots. Current human intention has to be deviated towards the new desired one by destabilizing the obstinance of human intention, inducing positive mood and making the “robot gain curiosity of human”. Deviations are generated as sequences of transient intentions tracing intention trajectories. The authors use elastic networks to generate, in two modes of body mood: “confident” and “suspicious”, transient intentions directed towards the desired one, choosing among intentional robot moves previously learned by HMM. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Hershey, PA, US},
  doi       = {10.4018/978-1-4666-7278-9.ch012},
  issn      = {978-1-4666-7278-9 (Hardcover); 978-1-4666-7279-6 (Digital (undefined format))},
  journal   = {Handbook of research on synthesizing human emotion in intelligent systems and robotics.},
  keywords  = {*Emotional States, *Intention, *Robotics, Engineering},
  pages     = {255--282},
  publisher = {Information Science Reference/IGI Global},
  refid     = {2015-24110-012},
  series    = {Advances in computational intelligence and robotics (ACIR) book series.},
}

@Misc{Lim2015,
  author    = {Lim, Angelica and Okuno, Hiroshi G.},
  title     = {Developing robot emotions through interaction with caregivers.},
  year      = {2015},
  abstract  = {In this chapter, the authors explore social constructivist theories of emotion, which suggest that emotional behaviors are developed through experience, rather than innate. The authors’ approach to artificial emotions follows this paradigm, stemming from a relatively young field called developmental or ‘epigenetic’ robotics. The chapter describes the design and implementation of a robot called MEI (multimodal emotional intelligence) with an emotion development system. MEI synchronizes to humans through voice and movement dynamics, based on mirror mechanism-like entrainment. Via typical caregiver interactions, MEI associates these dynamics with its physical feeling, e.g. distress (low battery or excessive motor heat) or flourishing (homeostasis). Our experimental results show that emotion clusters developed through robot-directed motherese (“baby talk”) are similar to adult happiness and sadness, giving evidence to constructivist theories. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Hershey, PA, US},
  doi       = {10.4018/978-1-4666-7278-9.ch015},
  issn      = {978-1-4666-7278-9 (Hardcover); 978-1-4666-7279-6 (Digital (undefined format))},
  journal   = {Handbook of research on synthesizing human emotion in intelligent systems and robotics.},
  keywords  = {*Caregivers, *Constructivism, *Emotional Intelligence, Robotics},
  pages     = {316--337},
  publisher = {Information Science Reference/IGI Global},
  refid     = {2015-24110-015},
  series    = {Advances in computational intelligence and robotics (ACIR) book series.},
}

@Misc{Alenljung2015,
  author    = {Alenljung, B. and Lindblom, J.},
  title     = {User experience of socially interactive robots: Its role and relevance.},
  year      = {2015},
  abstract  = {Socially interactive robots are expected to have an increasing importance in everyday life for a growing number of people, but negative user experience (UX) can entail reluctance to use robots. Positive user experience underpins proliferation of socially interactive robots. Therefore, it is essential for robot developers to put serious efforts to attain social robots that the users experience as positive. In current human-robot interaction (HRI) research, user experience is reckoned to be important and is used as an argument for stating that something is positive. However, the notion of user experience is noticeably often taken for granted and is neither described nor problematized. By recognizing the complexity of user experience the intended contributions can be even more valuable. Another trend in HRI research is to focus on user experience evaluation and examination of user experience. The current research paths of user experience of socially interactive robots are not enough. This chapter suggests that additional research directions are needed in order accomplish long-term, wide-spread success of socially interactive robots. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Hershey, PA, US},
  doi       = {10.4018/978-1-4666-7278-9.ch017},
  issn      = {978-1-4666-7278-9 (Hardcover); 978-1-4666-7279-6 (Digital (undefined format))},
  journal   = {Handbook of research on synthesizing human emotion in intelligent systems and robotics.},
  keywords  = {*Experience Level, *Robotics, Social Interaction},
  pages     = {352--364},
  publisher = {Information Science Reference/IGI Global},
  refid     = {2015-24110-017},
  series    = {Advances in computational intelligence and robotics (ACIR) book series.},
}

@Misc{Carpenter2013,
  author    = {Carpenter, Julie},
  title     = {Just doesn’t look right: Exploring the impact of humanoid robot integration into Explosive Ordnance Disposal teams.},
  year      = {2013},
  abstract  = {This chapter provides a critical analysis of the potential short- and long-term cultural, emotional, and ethical outcomes facing Explosive Ordnance Disposal (EOD) specialists working closely with anthropomorphic robots in daily team situations as viewed through the interdisciplinary lens of Human-Robot Interaction (HRI) research. Effective small group communication and decision-making is especially critical for EOD teams. Communication failures cause immediate safety concerns, potential physical and psychological harm to EOD team members, and similar repercussions for any individuals in close physical proximity of the Unexploded Ordnance (UXO). The complexity of EOD Team duties, coupled with the inherent limitations of human performance, make it gravely important that technicians have tools that aid rather than hamper team goals. The U.S. Military is seeking a refinement of EOD robot design, including the incorporation of some humanlike characteristics such as biped design, upright walking ability, and responsiveness to human voice and gesture commands. These characteristics can be arguably useful for robots to move in human spaces, learn in a humanlike way, dexterously disarm munitions, and communicate efficiently with human users. But while humanoid design may move the role of the robot to one that becomes potentially more effective in some environments, it may complicate emotional and ethical issues in terms of how human team members view the robot - as an extension of self, an external tool, a team member, a pet, or other entity. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Hershey, PA, US},
  doi       = {10.4018/978-1-4666-2211-1.ch032},
  issn      = {978-1-4666-2211-1 (Hardcover); 978-1-4666-2212-8 (Digital (undefined format))},
  journal   = {Handbook of research on technoself: Identity in a technological society, Volumes 1-2},
  keywords  = {*Apparatus, *Human Computer Interaction, *Robotics, *Human Robot Interaction, Collaboration, Decision Making},
  pages     = {609--636},
  publisher = {Information Science Reference/IGI Global},
  refid     = {2014-35586-032},
}

@Misc{Nijssen2021,
  author    = {Nijssen, Sari R. R. and Müller, Barbara C. N. and Bosse, Tibor and Paulus, Markus},
  title     = {You, robot? The role of anthropomorphic emotion attributions in children’s sharing with a robot.},
  year      = {2021},
  abstract  = {Sharing helps children form and maintain relationships with other children. Yet, children born today interact not only with other children, but increasingly with robots as well. Little is known on whether and how children treat robots as recipients of prosocial acts. We thus investigated children’s sharing behavior towards robots. Specifically, we assessed the effect of anthropomorphic appearance and affective state attributions. Children (4-9 years old; n = 120) were introduced to robots that varied in the extent to which they looked human-like. Children’s perceptions of the robots’ affective states were manipulated by explicitly demonstrating one robot as having feelings and the other one not. Subsequently, children’s sharing behavior towards and feelings about sharing with these robots were measured. Results indicate that there was no effect of anthropomorphic appearance on sharing behavior. However, importantly, children in both age groups shared more resources with a robot that they attributed with affective states, and expressed more positive emotional judgments about sharing with that robot as well. An exploratory mediation analysis further revealed that children’s positive feelings about sharing guided their actual sharing behavior with robots. In sum, children show more pro-social behavior when they believe a robot can feel. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Nijssen, Sari R. R.: Behavioural Science Institute, Radboud University Nijmegen, P.O. Box 9104, Nijmegen, Netherlands, 6500 HE, S.Nijssen@psych.ru.nl},
  doi       = {10.1016/j.ijcci.2021.100319},
  issn      = {2212-8697(Electronic),2212-8689(Print)},
  journal   = {International Journal of Child-Computer Interaction},
  keywords  = {*Attribution, *Prosocial Behavior, *Robotics, *Sharing (Social Behavior), *Anthropomorphism, Emotional States, Emotions, Positive Emotions},
  publisher = {Elsevier Science},
  refid     = {2022-56137-001},
  volume    = {30},
}

@Misc{Morency2014,
  author    = {Morency, Louis-Philippe},
  title     = {The role of context in affective behavior understanding.},
  year      = {2014},
  status    = {Rejeitado: Inacessivel},
  abstract  = {This chapter argues that it is possible to significantly improve state-of-the art recognition techniques by exploiting regularities in how people communicate. People do not provide affective feedback at random. Rather, they react to the current topic, previous utterances, and the speaker's current verbal and nonverbal behavior. For example, listeners are far more likely to nod or shake if the speaker has just asked them a question, and incorporating such dialogue context can improve recognition performance during human-robot interaction. More generally, speakers and listeners coproduce a range of lexical, prosodic, and nonverbal patterns. Our goal is to automatically discover these patterns using only easily observable features of human face-to-face interaction (e.g., prosodic features and eye gaze), and exploit them to improve recognition accuracy. This chapter shows that the recognition of affective gestures can be improved by considering the behaviors of other participants in the conversation. Specifically, it shows that the multimodal context from the current speaker can improve the visual recognition of listener gestures. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {New York, NY, US},
  issn      = {978-0-19-538764-3 (Hardcover)},
  journal   = {Social emotions in nature and artifact.},
  keywords  = {*Human Computer Interaction, *Nonverbal Communication, *Verbal Communication, *Emotion Recognition, *Human Robot Interaction, Gestures},
  pages     = {128--142},
  publisher = {Oxford University Press},
  refid     = {2013-44492-008},
  series    = {Oxford series on cognitive models and architecture.},
}

@Article{Ahmad2021,
  author    = {Ahmad, Muneeb Imtiaz and Gao, Yuan and Alnajjar, Fady and Shahid, Suleman and Mubin, Omar},
  journal   = {Behaviour & Information Technology},
  title     = {Emotion and memory model for social robots: A reinforcement learning based behaviour selection.},
  year      = {2021},
  issn      = {1362-3001(Electronic),0144-929X(Print)},
  pages     = {No Pagination Specified--No Pagination Specified},
  abstract  = {In this paper, we propose a reinforcement learning (RL) mechanism for social robots to select an action based on users’ learning performance and social engagement. We applied this behavior selection mechanism to extend the emotion and memory model, which allows a robot to create a memory account of the user’s emotional events and adapt its behavior based on the developed memory. We evaluated the model in a vocabulary-learning task at a school during a children’s game involving robot interaction to see if the model results in maintaining engagement and improving vocabulary learning across the four different interaction sessions. Generally, we observed positive findings based on child vocabulary learning and sustaining social engagement during all sessions. Compared to the trends of a previous study, we observed a higher level of social engagement across sessions in terms of the duration of the user gaze toward the robot. For vocabulary retention, we saw similar trends in general but also showing high vocabulary retention across some sessions. The findings indicate the benefits of applying RL techniques that have a reward system based on multi-modal user signals or cues. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Ahmad, Muneeb Imtiaz: muneeb.ahmad@westernsydney.edu.au},
  doi       = {10.1080/0144929X.2021.1977389},
  publisher = {Taylor & Francis},
  refid     = {2021-90181-001},
}

@Misc{Hoorn2015,
  author    = {Hoorn, Johan F.},
  title     = {Psychological aspects of technology interacting with humans.},
  year      = {2015},
  abstract  = {This chapter describes a large number of sometimes complex studies that in unison have led to the production of a reliable, serviceable sometimes stern but fair care robot. The initial summary of studies is necessary to understand what the smart, sensitive, creative, and moral humanoid that interacts with its human user is actually based upon: Its core is driven by the model called Perceiving and Experiencing Fictional Characters (PEFiC), the interactive variant of which (I-PEFiC) received a module to make affective decisions (ADM). An integration of I-PEFICADM with prevailing emotion models led to the Silicon Coppélia model. Silicon Coppélia, then, was extended by a system for moral reasoning (Moral Coppélia) and is now in the process to become creative (ACASIA) and have an understanding of reality in contrast to fiction (Epistemics of the Virtual). (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Hoboken, NJ, US},
  doi       = {10.1002/9781118426456.ch8},
  issn      = {978-1-118-41336-4 (Hardcover); 978-1-118-42652-4 (Digital (undefined format))},
  journal   = {The handbook of the psychology of communication technology.},
  keywords  = {*Human Computer Interaction, *Psychology, *Technology, Decision Making, Emotions, Robotics},
  pages     = {176--201},
  publisher = {Wiley Blackwell},
  refid     = {2014-32529-008},
  series    = {Handbooks in communication and media.},
}

@Misc{Watanabe2013,
  author    = {Watanabe, Shigeru and Kuczaj, Stan},
  title     = {Emotions of animals and humans: Comparative perspectives.},
  year      = {2013},
  status    = {Rejeitado: Inacessivel},
  abstract  = {Emotions of Animals and Humans: Comparative Perspectives takes a multidisciplinary approach to emotion, with contributions from biologists, psychologists, neuroscientists, robot engineers, and artists. A wide range of emotional phenomena is discussed, including the notion that humans' sophisticated sensibility, as evidenced by our aesthetic appreciation of the arts, is based at least in part on a basic emotional sensibility that is found in young children and perhaps even some non-human animal species. As a result, this book comprises a unique comparative perspective on the study of emotion. A number of chapters consider emotions in a variety of animal groups, including fish, birds, and mammals. Other chapters expand the scope of the book to humans and robots. Specific topics covered in these chapters run the gamut from lower-level emotional activity, such as emotional expression, to higher-level emotional activity, such as altruism, love, and aesthetics. Taken as a whole, the book presents manifold perspectives on emotion and provides a solid foundation for future multidisciplinary research on the nature of emotions. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Watanabe, Shigeru: Department of Psychology, Keio University, 2-15-45 Mita, Minato-ku, Tokyo, Japan, 108-8345},
  issn      = {978-4-431-54123-3 (PDF); 978-4-431-54122-6 (Hardcover)},
  journal   = {Emotions of animals and humans: Comparative perspectives.},
  keywords  = {*Emotions, *Interdisciplinary Research, Species Differences},
  pages     = {xiii, 283--xiii, 283},
  publisher = {Springer Science + Business Media},
  refid     = {2012-29710-000},
  series    = {The science of the mind.},
}

@Misc{Itakura2013,
  author    = {Itakura, Shoji and Moriguchi, Yusuke and Morita, Tomoyo},
  title     = {The development of mentalizing and emotion in human children.},
  year      = {2013},
  status    = {Rejeitado: Inacessivel},
  abstract  = {For human infants, agents--defined as other humans--are the fundamental units of their social world. Agents provide very special stimuli to infants. Researchers of object-person differentiation have proposed a set of rules that infants probably use during their interaction with people as opposed to objects. This chapter reviews investigations into how children understand and detect both human and nonhuman agents and communicate with them, starting with a definition of mentalizing and summary of the course of its development. A study is presented on infant imitation of a robot's action and a false-belief task with robots, proposing a new research domain called "developmental cybernetics," which studies the interaction between children and robots. It has been predicted that in ordinary twenty-first-century households, robotics technology will be as common as refrigerators and dishwashers. Therefore, exploring developmental cybernetics is important. Finally, two more studies from the perspective of cognitive neuroscience are presented, with a discussion on the usefulness of the neurocognitive approach in understanding the development of mentalizing, alongside two studies concerned with this issue. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Itakura, Shoji: Department of Psychology, Graduate School of Letters, Kyoto University, Yoshida-honmachi, Sakyo-Ku, Kyoto, Japan, 6068501, sitakura@bun.kyoto-u.ac.jp},
  issn      = {978-4-431-54123-3 (PDF); 978-4-431-54122-6 (Hardcover)},
  journal   = {Emotions of animals and humans: Comparative perspectives.},
  keywords  = {*Childhood Development, *Cybernetics, *Theory of Mind, *Cognitive Neuroscience, Emotional Development, Robotics, Social Cognition, Mentalization},
  pages     = {207--222},
  publisher = {Springer Science + Business Media},
  refid     = {2012-29710-009},
  series    = {The science of the mind.},
}

@Misc{Oosthuizen2021,
  author    = {Oosthuizen, Rudolf M.},
  title     = {Artificial shame in the fourth industrial revolution.},
  year      = {2021},
  abstract  = {Futurists predict that a third of jobs that exist today could be taken by smart technology, artificial intelligence, human-robot interaction and algorithms. Robots will handle 52% of current work tasks by 2025--almost twice as many as they currently do. According to the World Economic Forum (2018), rapid changes in machines and algorithms, or computer processes, could create 133 million new roles in place of the 75 million that will be displaced between now and 2022. The Fourth Industrial Revolution is coming, and it is changing almost all aspects of human life, including the cultural context. The objective of the chapter is to present a critical review of shame for artificial cognitive systems, which must or may not be able to interact with human beings in the context of the Fourth Industrial Revolution. However, emotional architectures have been focused on more basic models of emotions, and shame has been neglected in most of research cases by researchers of robotics, computer sciences or artificial intelligence (Vallverdú, 2014). Shame in artificial devices, as well as the culture of future human-machine interactions, is investigated (Brougham & Haar, 2018; Samani, Koh, Saadatian, & Polydorou, 2012). (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Oosthuizen, Rudolf M.: Department of Industrial and Organisational Psychology, University of South Africa, Pretoria, South Africa, oosthrm@unisa.ac.za},
  doi       = {10.1007/978-3-030-59527-2_25},
  issn      = {978-3-030-59526-5 (Hardcover); 978-3-030-59527-2 (Digital (undefined format))},
  journal   = {Shame 4.0: Investigating an emotion in digital worlds and the Fourth Industrial Revolution.},
  keywords  = {*Artificial Intelligence, *Emotions, *Human Machine Systems, *Industrial and Organizational Psychology, *Shame, Cognitive Ability, Morality},
  pages     = {537--554},
  publisher = {Springer Nature Switzerland AG},
  refid     = {2022-06586-025},
}

@Article{Namba2022,
  author    = {Namba, Shushi and Sato, Wataru and Matsui, Hiroshi},
  journal   = {Journal of Nonverbal Behavior},
  title     = {Spatio-temporal properties of amused, embarrassed, and pained smiles.},
  year      = {2022},
  issn      = {1573-3653(Electronic),0191-5886(Print)},
  pages     = {No Pagination Specified--No Pagination Specified},
  abstract  = {Smiles are universal but nuanced facial expressions that are most frequently used in face-to-face communications, typically indicating amusement but sometimes conveying negative emotions such as embarrassment and pain. Although previous studies have suggested that spatial and temporal properties could differ among these various types of smiles, no study has thoroughly analyzed these properties. This study aimed to clarify the spatiotemporal properties of smiles conveying amusement, embarrassment, and pain using a spontaneous facial behavior database. The results regarding spatial patterns revealed that pained smiles showed less eye constriction and more overall facial tension than amused smiles; no spatial differences were identified between embarrassed and amused smiles. Regarding temporal properties, embarrassed and pained smiles remained in a state of higher facial tension than amused smiles. Moreover, embarrassed smiles showed a more gradual change from tension states to the smile state than amused smiles, and pained smiles had lower probabilities of staying in or transitioning to the smile state compared to amused smiles. By comparing the spatiotemporal properties of these three smile types, this study revealed that the probability of transitioning between discrete states could help distinguish amused, embarrassed, and pained smiles. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Namba, Shushi: shushi.namba@riken.jp},
  doi       = {10.1007/s10919-022-00404-7},
  publisher = {Springer},
  refid     = {2022-79315-001},
}

@Misc{Kim2016,
  author    = {Kim, Jeansok J. and Choi, June-Seek and Lee, Hongjoo J.},
  title     = {Foraging in the face of fear: Novel strategies for evaluating amygdala functions in rats.},
  year      = {2016},
  status    = {Rejeitado: Inacessivel},
  abstract  = {Fear is a defensive mechanism that plays an important role in our lives: It activates organized bodily-behavioral responses that help minimize our exposure to risks. This chapter presents a novel ethobehavioral paradigm to explore foraging behavior in laboratory rats in quantifiable "approach food-avoid predator" situations that simulate the environments in which the adaptive functions of fear evolved. Specifically, animals seeking food in a seminaturalistic apparatus, consisting of a nest and an open area, encountered a "predatory" robot executing a programmed set of threatening actions. All rats instinctively and robustly reacted to the looming robot by fleeing into the safety of the nest and freezing (fear responses). Afterward, the animals emerged from the nest and cautiously approached the food until the surging robot reevoked fear responses. With repeated encounters, however, the success of seizing the food correlated positively with the food-to-robot distance, suggesting that rats use a spatial (or distance) gradient of fear from the locus of the threat. Further experiments revealed that the amygdala bidirectionally regulates rats' foraging behavior in risky environments. Researching fear from functional, mechanistic, and phylogenetic perspectives will likely provide a deeper understanding of this fundamental emotion. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {New York, NY, US},
  issn      = {978-1-4625-2594-2 (Hardcover); 978-1-4625-2595-9 (PDF)},
  journal   = {Living without an amygdala.},
  keywords  = {*Amygdala, *Animal Foraging Behavior, *Fear, Rats},
  pages     = {129--148},
  publisher = {The Guilford Press},
  refid     = {2016-38564-005},
}

@Article{David2022,
  author    = {David, Oana A. and David, Daniel},
  journal   = {Journal of Rational-Emotive & Cognitive-Behavior Therapy},
  title     = {How can we best use technology to teach children to regulate emotions? Efficacy of the cognitive reappraisal strategy based on robot versus cartoons versus written statements in regulating test anxiety.},
  year      = {2022},
  issn      = {1573-6563(Electronic),0894-9085(Print)},
  pages     = {No Pagination Specified--No Pagination Specified},
  abstract  = {Test anxiety has a high prevalence in children and is associated with lower academic performance. The main purpose of the current paper was to investigate the efficacy of using technology based tools, in the form of a robotic agent and cartoons, for teaching school aged children functional cognitive reappraisal emotion-regulation strategies for managing test anxiety. Sixty-nine elementary school aged children participated in the current study. Test anxiety was induced and then the children were allocated to the roboRETMAN, the PsyPills (written reappraisal statements), and the wait-list conditions. In the second stage, children in the wait-list received the RETmagic cartoons. Children reported on their anxiety, positive emotions, rational and irrational beliefs. Results show a higher efficacy for the roboRETMAN compared to wait-list in helping children manage their test anxiety, improving their positive emotions and reducing irrational cognitions. This study brings important contributions to the field, given that there is no research done so far investigating most effective means for delivering emotion-regulation strategies in children. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {David, Oana A.: oanadavid@psychology.ro},
  doi       = {10.1007/s10942-021-00440-0},
  publisher = {Springer},
  refid     = {2022-31114-001},
}

@Misc{Tisseron2015,
  author    = {Tisseron, Serge and Tordo, Frédéric and Baddoura, Ritta},
  title     = {Testing empathy with robots: A model in four dimensions and sixteen items.},
  year      = {2015},
  abstract  = {The four-dimensional model of empathy presented in this paper addresses human-human, human-avatar and human-robot interaction, and aims at better understanding the specificities of the empathy that humans might develop towards robots. Its first dimension is auto-empathy and refers to an empathetic relationship with oneself: how can a human directing a robot expand the various components of empathy he feels for himself to this robot? The second is direct empathy: what does a human attribute to a robot in terms of thoughts, emotions, action potentials or even altruism, on the model of what he imagines and attributes to himself? The third dimension is reciprocal empathy that consists of thinking that a robot is able to identify with me, feel or guess my emotions and thoughts, anticipate my actions and wear me assistance if necessary. Finally, the fourth dimension, intersubjective empathy, is about thinking and imagining that a robot can inform me of things--emotions, thoughts that I am likely to experience- that I do not know about myself. Each of these four dimensions includes four different components: (1) Action (empathy of action), (2) Emotion (emotional empathy), (3) Cognition (cognitive empathy) and (4) Assistance (empathy of assistance). This theoretical model of empathy in four dimensions and four components defines sixteen items whose relevance will be tested in the near future through comparative experimental research involving human-human and human-robot interaction. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Baddoura, Ritta: INSERM U846, Stem-Cell and Brain Research Institute, 18 avenue du Doyen Lepine, Bron, France, 69500, rittabaddoura@yahoo.fr},
  doi       = {10.1007/s12369-014-0268-5},
  issn      = {1875-4805(Electronic),1875-4791(Print)},
  journal   = {International Journal of Social Robotics},
  keywords  = {*Empathy, *Robotics, *Human Robot Interaction, Testing, Action Potentials},
  pages     = {97--102},
  publisher = {Springer},
  refid     = {2014-51455-001},
  volume    = {7},
}

@Article{Spekman2020,
  author    = {Spekman, Marloes L. C. and Konijn, Elly A. and Hoorn, Johan F.},
  journal   = {International Journal of Social Robotics},
  title     = {How physical presence overrides emotional (coping) effects in hri: Testing the transfer of emotions and emotional coping in interaction with a humanoid social robot.},
  year      = {2020},
  issn      = {1875-4805(Electronic),1875-4791(Print)},
  pages     = {No Pagination Specified--No Pagination Specified},
  abstract  = {AbstractThe increasing pressure on healthcare systems calls for innovative solutions, such as social robots. However, healthcare situations often are highly emotional while little is known about how people’s prior emotional state may affect the perception and acceptance of such robots. Following appraisal theories of emotion, the appraisal of coping potential related to one’s emotions was found to be important in acting as mediator between emotional state and perceptions of a robot (Spekman et al. in Comput Hum Behav 85:308-318, 2018. 10.1016/j.chb.2018.03.043; in Belief in emotional coping ability affects what you see in a robot, not the emotions as such, Dissertation, Vrije Universiteit Amsterdam, Amsterdam, 2018), though this has not yet been tested in relation to actual emotional coping nor in an actual encounter with a robot. Hence, the current study focused on how actual emotional coping influences subsequent robot perceptions in two experiments. In Study 1 (N = 101) and Study 2 (N = 110) participants encountered a real humanoid robot after a manipulation to induce various emotions and coping potential. Manipulations in both studies were effective, yet the results in Study 1 were potentially confounded by a novelty effect of participants’ first encounter with a real robot that talked to them. Therefore, in Study 2, participants interacted briefly with the robot before the actual experiment. Results showed an interaction effect of prior emotions and (manipulated) coping potential on robot perceptions, but not the effects expected based on previous studies. An actual interaction with a robot thus seems to provoke different reactions to the robot, thereby overruling any emotional effects. These findings are discussed in light of the healthcare context in which these social robots might be deployed. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Spekman, Marloes L. C.: mlcspekman@gmail.com},
  doi       = {10.1007/s12369-020-00649-6},
  publisher = {Springer},
  refid     = {2020-29671-001},
}

@Misc{Raffard2016,
  author    = {Raffard, Stéphane and Bortolon, Catherine and Khoramshahi, Mahdi and Salesse, Robin N. and Burca, Marianna and Marin, Ludovic and Bardy, Benoit G. and Billard, Aude and Macioce, Valérie and Capdevielle, Delphine},
  title     = {Humanoid robots versus humans: How is emotional valence of facial expressions recognized by individuals with schizophrenia? An exploratory study.},
  year      = {2016},
  abstract  = {Background: The use of humanoid robots to play a therapeutic role in helping individuals with social disorders such as autism is a newly emerging field, but remains unexplored in schizophrenia. As the ability for robots to convey emotion appear of fundamental importance for human-robot interactions, we aimed to evaluate how schizophrenia patients recognize positive and negative facial emotions displayed by a humanoid robot. Methods: We included 21 schizophrenia outpatients and 17 healthy participants. In a reaction time task, they were shown photographs of human faces and of a humanoid robot (iCub) expressing either positive or negative emotions, as well as a non-social stimulus. Patients' symptomatology, mind perception, reaction time and number of correct answers were evaluated. Results: Results indicated that patients and controls recognized better and faster the emotional valence of facial expressions expressed by humans than by the robot. Participants were faster when responding to positive compared to negative human faces and inversely were faster for negative compared to positive robot faces. Importantly, participants performed worse when they perceived iCub as being capable of experiencing things (experience subscale of the mind perception questionnaire). In schizophrenia patients, negative correlations emerged between negative symptoms and both robot's and human's negative face accuracy. Conclusions: Individuals do not respond similarly to human facial emotion and to non-anthropomorphic emotional signals. Humanoid robots have the potential to convey emotions to patients with schizophrenia, but their appearance seems of major importance for human-robot interactions. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Raffard, Stéphane: Laboratory Epsylon, EA 4556, Montpellier University 3, 1 University Department of Adult Psychiatry, 39 Avenue Charles Flahault, Montpellier, France, 34295, Cedex 5},
  doi       = {10.1016/j.schres.2016.06.001},
  issn      = {1573-2509(Electronic),0920-9964(Print)},
  journal   = {Schizophrenia Research},
  keywords  = {*Facial Expressions, *Robotics, *Schizophrenia, Affective Valence},
  pages     = {506--513},
  publisher = {Elsevier Science},
  refid     = {2016-29511-001},
  volume    = {176},
}

@Misc{Morgavi2012,
  author    = {Morgavi, Giovanna and Marconi, Lucia and Morando, Mauro and Cutugno, Paola},
  title     = {From human creative cognitive processes to adaptable artificial system design.},
  year      = {2012},
  status    = {Rejeitado: Inacessivel},
  abstract  = {Since the year 2000, a number of researchers have suggested a developmental perspective on artificial intelligence (AI) and robotics. One of the fundamental methodological assumptions is that cognition is embodied, which means that it arises from bodily interactions with the world and that it is continuously meshed with them. In other words, thinking emerges from real-life experiences, from sensorimotor coordinated interactions, and from exploration of the surrounding environment. Interdisciplinary theory and empirical evidence are used to inform epigenetic robotic models, and these models can be used as theoretical tools to make experimental predictions in developmental psychology and other disciplines studying cognitive development in living systems. Let us look at a metaphor as an example of abstraction in the linguistic process. It consists of a relationship among meanings that take the context from one world to another. Abstractions not only are grounded in perceptual experience, retaining something of the "perceptual character" of the experiences from which they were derived, but also are a product or reasoning or creative thought. When we speak about "a sea of troubles," we are interested only in a part of the "sea," its dimension, while all the other meanings and images are discarded. With the above in mind, our research questions are as follows: How does the process of metaphor understanding happen in nature? Which procedures allow people to bring out the understanding of the metaphor? What mechanisms are triggered? We studied the modalities through which preschool children (4-5 years old) bring out the understanding of metaphor hidden in common idiomatic sentences in their native language. We chose children of this age because of their level of cognitive development. The cognitive development of children of this age does not include the ability of abstraction, but they are able to explain the process they are thinking. We carried out this experiment in an infant school in the Genoese Municipality, Italy, which had a "Sea Laboratory" where children were accustomed to some sea animal names and some of their characteristics, but they were not in touch with the meaning of the idiomatic sentences requiring an abstract act to be understood. Forty-two phrases were proposed to about eight work groups of 9 or 10 children each; the children were asked to provide abstracted meanings for the phrases. The experimenter read a new sentence and asked for help to find its meaning. Children were allowed to talk, to draw, or to move around the classroom as they liked. The experimenter spurred on the meaning search through neutral communication. The process was recorded, and then we analyzed the answers. Collective speech was analyzed to compensate for individual differences. We collected 6,838 answers and 290 drawings. This output has been analyzed and classified by 4 people. If an answer could be associated with more than one subprocess, each involved subprocess was counted, resulting on 7,230 path instances. Only answers with unanimous classification were accepted; this involved 28 rejections, corresponding to 0.39% of the total instances. Our experiment resulted in some consideration for the architecture of a robot. As in human beings, an efficient developmental architecture shows a hierarchy of basic parallel paths for processing different aspects of knowledge, to let the growing-up phase emerge; an artificial architecture should be able to parallel process a combination of different paths in an internal representation. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {New York, NY, US},
  issn      = {978-1-84872-973-5 (Hardcover)},
  journal   = {Attention, representation, and human performance: Integration of cognition, emotion, and motivation.},
  keywords  = {*Artificial Intelligence, *Cognitive Development, *Cognitive Processes, *Robotics, *Systems Design, Abstraction, Creativity},
  pages     = {133--145},
  publisher = {Psychology Press},
  refid     = {2011-27461-008},
}

@Misc{LaFrance2008,
  author    = {LaFrance, Marianne},
  title     = {What's in a robot's smile? The many meanings of positive facial display.},
  year      = {2008},
  abstract  = {Offers a valuable corrective to simplistic accounts of the relationship between facial expression and affective state by discussing the example of smiling. The author shows that smiles appear in a variety of forms in order to express a variety of emotions, with only one, the Duchenne smile, unambiguously associated with happiness. Smiles (and other emotion-related facial displays) are not necessarily indicators of internal states, but often act as social messages, e.g. to show others our disposition towards an interaction episode. From the perspective of smiles as volitional social messages, the problem of distinguishing "true" from "fake" smiles holds less relevance than it has traditionally been given. Instead, understanding the meaning of a smile requires understanding the social context in which that smile occurs, and this chapter examines some of the social dimensions that are related to different types of smiles. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {LaFrance, Marianne: Yale University, Department of Psychology, P.O. Box 208205, New Haven, CT, US, 06520-8205, marianne.lafrance@yale.edu},
  doi       = {10.1075/aicr.74.06laf},
  issn      = {978-90-272-5210-4 (Hardcover)},
  journal   = {Animating expressive characters for social interaction.},
  keywords  = {*Emotional States, *Robotics, *Smiles, *Taxonomies, Deception, Facial Expressions, Interpersonal Interaction, Messages, Social Environments},
  pages     = {37--51},
  publisher = {John Benjamins Publishing Company},
  refid     = {2008-14110-003},
  series    = {Advances in consciousness research.},
}

@Misc{Reuten2018,
  author    = {Reuten, Anne and van Dam, Maureen and Naber, Marnix},
  title     = {Pupillary responses to robotic and human emotions: The uncanny valley and media equation confirmed.},
  year      = {2018},
  abstract  = {Physiological responses during human-robots interaction are useful alternatives to subjective measures of uncanny feelings for nearly humanlike robots (uncanny valley) and comparable emotional responses between humans and robots (media equation). However, no studies have employed the easily accessible measure of pupillometry to confirm the uncanny valley and media equation hypotheses, evidence in favor of the existence of these hypotheses in interaction with emotional robots is scarce, and previous studies have not controlled for low level image statistics across robot appearances. We therefore recorded pupil size of 40 participants that viewed and rated pictures of robotic and human faces that expressed a variety of basic emotions. The robotic faces varied along the dimension of human likeness from cartoonish to humanlike. We strictly controlled for confounding factors by removing backgrounds, hair, and color, and by equalizing low level image statistics. After the presentation phase, participants indicated to what extent the robots appeared uncanny and humanlike, and whether they could imagine social interaction with the robots in real life situations. The results show that robots rated as nearly humanlike scored higher on uncanniness, scored lower on imagined social interaction, evoked weaker pupil dilations, and their emotional expressions were more difficult to recognize. Pupils dilated most strongly to negative expressions and the pattern of pupil responses across emotions was highly similar between robot and human stimuli. These results highlight the usefulness of pupillometry in emotion studies and robot design by confirming the uncanny valley and media equation hypotheses. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  address   = {Naber, Marnix: marnixnaber@gmail.com},
  doi       = {10.3389/fpsyg.2018.00774},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Emotions, *Human Computer Interaction, *Pupil Dilation, Robotics},
  publisher = {Frontiers Media S.A.},
  refid     = {2018-25792-001},
  volume    = {9},
}

@Misc{Egbert2014,
  author    = {Egbert, Matthew D. and Barandiaran, Xabier E.},
  title     = {Modeling habits as self-sustaining patterns of sensorimotor behavior.},
  year      = {2014},
  abstract  = {[Correction Notice: An Erratum for this article was reported in Vol 9[209] of Frontiers in Human Neuroscience (see record 2015-35773-001). In the original article, Equation 8 is incorrectly written. The correction equation is present in the erratum.] In the recent history of psychology and cognitive neuroscience, the notion of habit has been reduced to a stimulus-triggered response probability correlation. In this paper we use a computational model to present an alternative theoretical view (with some philosophical implications), where habits are seen as self-maintaining patterns of behavior that share properties in common with self-maintaining biological processes, and that inhabit a complex ecological context, including the presence and influence of other habits. Far from mechanical automatisms, this organismic and self-organizing concept of habit can overcome the dominating atomistic and statistical conceptions, and the high temporal resolution effects of situatedness, embodiment and sensorimotor loops emerge as playing a more central, subtle and complex role in the organization of behavior. The model is based on a novel “iterant deformable sensorimotor medium (IDSM),” designed such that trajectories taken through sensorimotor-space increase the likelihood that in the future, similar trajectories will be taken. We couple the IDSM to sensors and motors of a simulated robot, and show that under certain conditions, the IDSM conditions, the IDSM forms self-maintaining patterns of activity that operate across the IDSM, the robot’s body, and the environment. We present various environments and the resulting habits that form in them. The model acts as an abstraction of habits at a much needed sensorimotor “meso-scale” between microscopic neuron-based models and macroscopic descriptions of behavior. Finally, we discuss how this model and extensions of it can help us understand aspects of behavioral self-organization, historicity and autonomy that remain out of the scope of contemporary representationalist frameworks. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Egbert, Matthew D.: Embodied Emotion, Cognition and (Inter-)Action Lab, School of Computer Science, University of Hertfordshire, College Lane, Hatfield, HRT, United Kingdom, AL10 9AB, mde@matthewegbert.com},
  doi       = {10.3389/fnhum.2014.00590},
  issn      = {1662-5161(Electronic)},
  journal   = {Frontiers in Human Neuroscience},
  keywords  = {*Habits, *Mathematical Modeling, *Perceptual Motor Processes, Simulation},
  publisher = {Frontiers Media S.A.},
  refid     = {2014-52023-001},
  volume    = {8},
}

@Article{Huang2020,
  author    = {Huang, Tsung-Ren and Liu, Yu-Wei and Hsu, Shin-Min and Goh, Joshua O. S. and Chang, Yu-Ling and Yeh, Su-Ling and Fu, Li-Chen},
  journal   = {International Journal of Social Robotics},
  title     = {asynchronously embedding psychological test questions into human-robot conversations for user profiling.},
  year      = {2020},
  issn      = {1875-4805(Electronic),1875-4791(Print)},
  pages     = {No Pagination Specified--No Pagination Specified},
  abstract  = {Psychological variables of a person (e.g., cognitive abilities, personality traits, emotional states, and preferences) are valuable information that can be utilized by social robots to offer personalized human-robot interaction. These variables are often latent and inferred indirectly from a third-person perspective based on an individual’s behavioral manifestations (e.g., facial emotion expressions), and hence the true values of inferred psychological variables remain unknown to a robot observer. Although earlier studies have employed robot-administered psychological tests to infer psychological variables based on an individual’s first-person responses, these tests were formally presented and could be tedious to some users. To leverage the validity and reliability of well-established psychological tests for user profiling with ease, the present study examined the possibility of asynchronously embedding psychological test questions into casual human-robot conversations. In our experiment using a big-five personality inventory, the verbal responses from users to these asynchronous test questions were then compared with the written responses to the same personality test. The personality measures estimated from the two approaches correlated strongly in a young adult population but only moderately in an older population. These findings demonstrate the validity of the proposed asynchronous method for psychological testing in human-agent interactions and suggest some caveats when this testing method is applied to older adults or other special populations. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Huang, Tsung-Ren: tren@mil.psy.ntu.edu.tw},
  doi       = {10.1007/s12369-020-00716-y},
  publisher = {Springer},
  refid     = {2020-89026-001},
}

@Misc{Paiva2015,
  author    = {Paiva, Ana and Leite, Iolanda and Ribeiro, Tiago},
  title     = {Emotion modeling for social robots.},
  year      = {2015},
  abstract  = {This chapter describes current advances in emotion modeling for social robots. It begins by contextualizing the role of emotions in social robots, considering the concept of the affective loop. It describes a number of elements for the synthesis and expression of emotions through robotic embodiments and provides an overview of emotional adaptation and empathy in social robots. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {New York, NY, US},
  doi       = {10.1093/oxfordhb/9780199942237.013.029},
  issn      = {978-0-19-994223-7 (Hardcover)},
  journal   = {The Oxford handbook of affective computing.},
  keywords  = {*Human Computer Interaction, *Robotics, *Human Robot Interaction, *Social Robotics, Emotions, Empathy, Simulation, Computational Modeling},
  pages     = {296--308},
  publisher = {Oxford University Press},
  refid     = {2014-43075-021},
  series    = {Oxford library of psychology.},
}

@Misc{Howard2010,
  author    = {Howard, Lorraine and Vick, Sarah-Jane},
  title     = {Does it bite? The role of stimuli characteristics on preschoolers' interactions with robots, insects and a dog.},
  year      = {2010},
  abstract  = {While there is increasing interest in the impact of animal interactions upon children's wellbeing and attitudes, there has been less attention paid to the specific characteristics of the animals that attract and engage children. We used a within-subjects design to explore how differences in animal features (such as their animacy, size, and texture) impacted upon pre-school children's social and emotional responses. This study examined pre-schoolers' interactions with two animal-like robots (Teksta and Scoozie), two insect types (stick insects and hissing cockroaches) and a dog (Teasel, a West Highland Terrier). Nineteen preschool participants aged 35-57 months were videoed while interacting with the experimenter, a peer, and each stimulus (presented individually). We used both verbal and nonverbal behaviors to evaluate interactions and emotional responses to the stimuli and found that these two measures could be incongruent, highlighting the need for systematic approaches to evaluating children's interactions with animals. We categorized the content of children's dialogues in relation to psychological and biological attributes of each stimulus and their distinctions between living and non-living stimuli; the majority of comments were biological, with psychological terms largely reserved for the dog and mammal-like robot only. Comments relating to living qualities revealed ambiguity towards attributes that denote differences between living and non-living creatures. We used a range of nonverbal measures, including willingness to approach and touch stimuli, rates of self-touching, facial expressions of emotion, and touch to others. Insects (hissing cockroaches and stick insects) received the most negative verbal and nonverbal responses. The mammal-like robot (rounded, fluffy body shape, large eyes, and sympathetic sounds) was viewed much more positively than its metallic counterpart, as was the real dog. We propose that these interactions provide information on how children perceive animals and a platform for the examination of human socio-emotional and cognitive development more generally. The children engaged in social referencing to the adult experimenter rather than familiar peers when uncertain about the stimuli presented, suggesting that caregivers have a primary role in shaping children's responses to animals. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Howard, Lorraine: School of Psychology and Sports Sciences, Northumbria University, NB155, Newcastle upon Tyne, United Kingdom, NE1 8ST, lorraine.howard@northumbria.ac.uk},
  doi       = {10.2752/175303710X12750451259499},
  issn      = {1753-0377(Electronic),0892-7936(Print)},
  journal   = {Anthrozoös},
  keywords  = {*Emotional Responses, *Facial Expressions, *Interspecies Interaction, *Preschool Students, *Well Being, Dogs, Insects, Robotics},
  pages     = {397--413},
  publisher = {Berg Publishers},
  refid     = {2011-02398-006},
  volume    = {23},
}

@Misc{Soares2019,
  author    = {Soares, Filomena O. and Costa, Sandra C. and Santos, Cristina P. and Pereira, Ana Paula S. and Hiolle, Antoine R. and Silva, Vinícius},
  title     = {Socio-emotional development in high functioning children with autism spectrum disorders using a humanoid robot.},
  year      = {2019},
  abstract  = {The use of robots had already been proven to encourage the promotion of social interaction and skills lacking in children with Autism Spectrum Disorders (ASD), who typically have difficulties in recognizing facial expressions and emotions. The main goal of this research is to study the influence of a humanoid robot to develop socio-emotional skills in children with ASD. The children’s performance in game scenarios aiming to develop facial expressions recognition skills is presented. Along the sessions, children who performed the game scenarios with the robot and the experimenter had a significantly better performance than the children who performed the game scenarios without the robot. The main conclusions of this research support that a humanoid robot is a useful tool to develop socio-emotional skills in the intervention of children with ASD, due to the engagement and positive learning outcome observed. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Soares, Filomena O.: Algoritmi Centre, University of Minho, Guimaraes, Portugal, fsoares@dei.uminho.pt},
  doi       = {10.1075/is.15003.cos},
  issn      = {1572-0381(Electronic),1572-0373(Print)},
  journal   = {Interaction Studies: Social Behaviour and Communication in Biological and Artificial Systems},
  keywords  = {*Autism Spectrum Disorders, *Childhood Development, *Robotics, *Treatment Outcomes, *Socioemotional Functioning, Social Interaction},
  pages     = {205--233},
  publisher = {John Benjamins},
  refid     = {2020-52675-001},
  volume    = {20},
}

@Misc{Wilson2016,
  author    = {Wilson, Jason R. and Scheutz, Matthias and Briggs, Gordon},
  title     = {Reflections on the design challenges prompted by affect-aware socially assistive robots.},
  year      = {2016},
  abstract  = {The rising interest in socially assistive robotics is, at least in part, stemmed by the aging population around the world. A lot of research and interest has gone into insuring the safety of these robots. However, little has been done to consider the necessary role of emotion in these robots and the potential ethical implications of having affect-aware socially assistive robots. In this chapter we address some of the considerations that need to be taken into account in the research and development of robots assisting a vulnerable population. We use two fictional scenarios involving a robot assisting a person with Parkinson's disease to discuss five ethical issues relevant to affect-aware socially assistive robots. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Wilson, Jason R.: Human-Robot Interaction Laboratory, Tufts University, 200 Boston Ave., Medford, MA, US, 02155, wilson@cs.tufts.edu},
  doi       = {10.1007/978-3-319-31413-6_18},
  issn      = {978-3-319-31411-2 (Hardcover); 978-3-319-31413-6 (Digital (undefined format))},
  journal   = {Emotions and personality in personalized services: Models, evaluation and applications.},
  keywords  = {*Emotions, *Ethics, *Human Machine Systems Design, *Parkinson's Disease, *Robotics, Assistive Technology},
  pages     = {377--395},
  publisher = {Springer International Publishing},
  refid     = {2016-40474-018},
  series    = {Human-computer interaction series.},
}

@Misc{Kraemer2013,
  author    = {Krämer, Nicole C. and Klatt, Jennifer and Hoffmann, Laura and Rosenthal-von der Pütten, Astrid},
  title     = {"Emotional" robots and agents--Implementation of emotions in artificial entities.},
  year      = {2013},
  status    = {Rejeitado: Inacessivel},
  abstract  = {The chapter summarizes research on emotions in the context of robot and virtual agent development. First, several motives and reasons for the implementation of emotions in artificial entities are presented. Then, a choice of robot and agent systems that model emotion based on various theories and assumptions are described. However, the common practice to implement emotions is also critically reflected against the background of theories on the relation of emotions and facial expressions. Based on this, alternative approaches are presented which implement a theory of mind module instead of emotions. In a second part, humans' emotional reactions to systems with implemented emotions and corresponding facial expressions are described. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Hauppauge, NY, US},
  issn      = {978-1-62618-820-4 (Hardcover); 978-1-62808-131-2 (PDF)},
  journal   = {Handbook of psychology of emotions (Vol 2): Recent theoretical perspectives and novel empirical findings.},
  keywords  = {*Artificial Intelligence, *Emotions, *Facial Expressions, *Robotics, Theory of Mind},
  pages     = {277--295},
  publisher = {Nova Science Publishers},
  refid     = {2013-39571-014},
  series    = {Psychology of emotions, motivations and actions.},
}

@Misc{Kozima2009,
  author    = {Kozima, Hideki and Michalowski, Marek P. and Nakagawa, Cocoro},
  title     = {Keepon: A playful robot for research, therapy, and entertainment.},
  year      = {2009},
  abstract  = {Keepon is a small creature-like robot designed for simple, natural, nonverbal interaction with children. The minimal design of Keepon's appearance and behavior is meant to intuitively and comfortably convey the robot's expressions of attention and emotion. For the past few years, we have been observing interactions between Keepon and children at various levels of physical, mental, and social development. With typically developing children, we have observed varying styles of play that suggest a progression in ontological understanding of the robot. With children suffering from developmental disorders such as autism, we have observed interactive behaviors that suggest Keepon's design is effective in eliciting a motivation to share mental states. Finally, in developing technology for interpersonal coordination and interactional synchrony, we have observed an important role of rhythm in establishing engagement between people and robots. This paper presents a comprehensive survey of work done with Keepon to date. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Kozima, Hideki: School of Project Design, Miyagi University, Gakuen 1, Taiwa, Kurokawa, Miyagi, Japan, 981-3298, xkozima@myu.ac.jp},
  doi       = {10.1007/s12369-008-0009-8},
  issn      = {1875-4805(Electronic),1875-4791(Print)},
  journal   = {International Journal of Social Robotics},
  keywords  = {*Childhood Development, *Human Computer Interaction, *Recreation, *Robotics, *Human Robot Interaction, Autism Spectrum Disorders, Play Therapy},
  pages     = {3--18},
  publisher = {Springer},
  refid     = {2009-20953-002},
  volume    = {1},
}

@Misc{Cerezo2019,
  author    = {Cerezo, Rebeca and Calderón, Vicente and Romero, Cristóbal},
  title     = {A holographic mobile-based application for practicing pronunciation of basic English vocabulary for Spanish speaking children.},
  year      = {2019},
  abstract  = {This paper describes a holographic mobile-based application designed to help Spanish-speaking children to practice the pronunciation of basic English vocabulary words. The mastery of vocabulary is a fundamental step when learning a language but is often perceived as boring. Producing the correct pronunciation is frequently regarded as the most difficult and complex skill for new learners of English. In order to address these problems this research takes advantage of the power of multi-channel stimuli (sound, image and interaction) in a mobile-based hologram application in order to motivate students and improve their experience of practicing. We adapted the prize-winning HolograFX game and developed a new mobile application to help practice English pronunciation. A 3D holographic robot that acts as a virtual teacher interacts via voice with the children. To test the tool we carried out an experiment with 70 Spanish pre-school children divided into three classes, the control group using traditional methods such as images in books and on the blackboard, and two experimental groups using our drills and practice software. One experimental group used the mobile application without the holographic game and the other experimental group used the application with the holographic game. We performed pre-test and post-test performance assessments, a satisfaction survey and emotion analysis. The results are very promising. They show that the use of the holographic mobile-based application had a significant impact on the children's motivation. It also improved their performance compared to traditional methods used in the classroom. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Romero, Cristóbal: Department of Computer Sciences and Numerical Analysis, University of Cordoba, Cordoba, Spain, 14071, cromero@uco.es},
  doi       = {10.1016/j.ijhcs.2018.11.009},
  issn      = {1095-9300(Electronic),1071-5819(Print)},
  journal   = {International Journal of Human-Computer Studies},
  keywords  = {*Computer Assisted Instruction, *English as Second Language, *Pronunciation, *Vocabulary, *Mobile Devices, Auditory Stimulation, Pictorial Stimuli, Stereoscopic Presentation},
  pages     = {13--25},
  publisher = {Elsevier Science},
  refid     = {2019-05801-003},
  volume    = {124},
}

@Misc{Liles2018,
  author    = {Liles, Karina},
  title     = {Ms. An (meeting students' academic needs): A socially adaptive robot tutor for student engagement in math education.},
  year      = {2018},
  status    = {Rejeitado: Inacessivel},
  abstract  = {This research presents a new, socially adaptive robot tutor, Ms. An ( Meeting Students' Academic N eeds). The goal of this research was to use a decision tree model to develop a socially adaptive robot tutor that predicted and responded to student emotion and performance to actively engage students in mathematics education. The novelty of this multi-disciplinary project is the combination of the fields of HRI, AI, and education. In this study we 1) implemented a decision tree model to classify student emotion and performance for use in adaptive robot tutoring-an approach not applied to educational robotics; 2) presented an intuitive interface for seamless robot operation by novice users; and 3) applied direct human teaching methods (guided practice and progress monitoring) for a robot tutor to engage students in mathematics education. Twenty 4th and 5th grade students in rural South Carolina participated in a between subjects study with two conditions: A) with a non-adaptive robot (control group); and B) with a socially adaptive robot (adaptive group). Students engaged in two one-on-one tutoring sessions to practice multiplication per the South Carolina 4th and 5 th grade mathematics state standards. Although our decision tree models were not very predictive, the results gave answers to our current questions and clarity for future directions. Our adaptive strategies to engage students academically were effective. Further, all students enjoyed working with the robot and we did not see a difference in emotional engagement across the two groups. This study offered insight for developing a socially adaptive robot tutor to engage students academically and emotionally while practicing multiplication. Results from this study will inform the human-robot interaction (HRI) and artificial intelligence (AI) communities on best practices and techniques within the scope of this work. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4217(Print)},
  keywords  = {*Computer Assisted Instruction, *Mathematics Education, *Robotics, *Student Engagement, Tutoring},
  pages     = {No Pagination Specified--No Pagination Specified},
  publisher = {ProQuest Information & Learning},
  refid     = {2018-40527-126},
  volume    = {79},
}

@Misc{Letheren2020,
  author    = {Letheren, Kate and Russell-Bennett, Rebekah and Whittaker, Lucas and Whyte, Stephen and Dulleck, Uwe},
  title     = {The evolution is now: Service robots, behavioral bias and emotions.},
  year      = {2020},
  status    = {Rejeitado: Inacessivel},
  abstract  = {Purpose: The purpose of this chapter is to conduct a critical literature review that examines the origins and development of research on service robots in organizations, as well as the key emotional and cognitive issues between service employees, customers, and robots. This review provides a foundation for future research that leverages the emotional connection between service robots and humans. Design/Methodology/Approach: A critical literature review that examines robotics, artificial intelligence, emotions, approach/avoid behavior, and cognitive biases is conducted. Findings: This research provides six key themes that emerge from the current state of research in the field of service robotics with 14 accompanying research questions forming the basis of a research agenda. The themes presented are as follows: Theme 1: Employees have a forgotten “dual role”; Theme 2: The influence of groups is neglected; Theme 3: Opposing emotions lead to uncertain outcomes; Theme 4: We know how robots influence engagement, but not experience; Theme 5: Trust is necessary but poorly understood; and Theme 6: Bias is contagious: if the human mind is irrational...so too are robot minds. Practical Implications: Practically, this research provides guidance for researchers and practitioners alike regarding the current state of research, gaps, and future directions. Importantly for practitioners, it sheds light on themes in the use of AI and robotics in services, highlighting opportunities to consider the dual role of the employee, examines how incorporating a service robot influences all levels of the organization, addresses motivational conflicts for employees and customers, explores how service robots influence the whole customer experience and how trust is formed, and how we are (often inad- vertently) creating biased robots. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Bingley, United Kingdom},
  issn      = {9781839092602 (Hardcover); 978-1-83909-259-6 (Digital (undefined format)); 978-1-83909-261-9 (EPUB)},
  journal   = {Emotions and service in the digital age.},
  keywords  = {*Artificial Intelligence, *Emotions, Robotics},
  pages     = {27--48},
  publisher = {Emerald Publishing},
  refid     = {2020-78979-003},
  series    = {Research on emotion in organizations.},
}

@Misc{ThimmeschGill2017,
  author    = {Thimmesch-Gill, Zane and Harder, Kathleen A. and Koutstaal, Wilma},
  title     = {Perceiving emotions in robot body language: Acute stress heightens sensitivity to negativity while attenuating sensitivity to arousal.},
  year      = {2017},
  abstract  = {Reliance on socioemotional assistive robots is projected to increase, yet little is known about how our ability to perceive their emotional expression is impacted by psychological factors. In high-risk and high-tension domains such as emergency services and healthcare, how might the cognitive and physiological stress we are experiencing influence how we read a humanoid robot's nonverbally conveyed emotions? Using a novel paradigm, we asked participants under experimentally-induced acute stress vs. low stress to evaluate a set of normed emotional body language poses conveyed by a physically-present vs. virtually-instantiated humanoid robot. Participants rated each pose for emotional valence (negativity/positivity) and arousal (calm/excited). Acute stress increased the perception of negative valence in negative high arousal poses, consistent with stress-induced hypervigilance. Surprisingly, stress diminished the perception of arousal in high arousal poses, whereas repeated presentation of the low arousal poses increased perception of arousal. Participants rated emotion similarly for the physically-present vs. virtually-present robot, although positively-valenced poses conveyed by the physical robot were perceived as more positive and more animate. We propose that perceptions of emotional arousal may be especially vulnerable to context effects and misattribution. These findings have implications for how assistive robots can best be designed for high-risk and high-tension contexts. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Thimmesch-Gill, Zane: Department of Psychology, University of Minnesota, 75 East River Road, S514 Elliott Hall, Minneapolis, MN, US, 55455, thimm009@umn.edu},
  doi       = {10.1016/j.chb.2017.06.036},
  issn      = {1873-7692(Electronic),0747-5632(Print)},
  journal   = {Computers in Human Behavior},
  keywords  = {*Emotions, *Language, *Negative Transfer, *Human Robot Interaction, *Social Robotics, Emergency Services, Acute Stress},
  pages     = {59--67},
  publisher = {Elsevier Science},
  refid     = {2017-40848-010},
  volume    = {76},
}

@Misc{Pepe2008,
  author    = {Pepe, Aaron A.},
  title     = {Applying the appraisal theory of emotion to human-agent interaction.},
  year      = {2008},
  status    = {Rejeitado: Inacessivel},
  abstract  = {Autonomous robots are increasingly being used in everyday life; cleaning our floors, entertaining us and supplementing soldiers in the battlefield. As emotion is a key ingredient in how we interact with others, it is important that our emotional interaction with these new entities be understood. This dissertation proposes using the appraisal theory of emotion (Roseman, Scherer, Schorr, & Johnstone, 2001) to investigate how we understand and evaluate situations involving this new breed of robot. This research involves two studies; in the first study an experimental method was used in which participants interacted with a live dog, a robotic dog or a non-anthropomorphic robot to attempt to accomplish a set of tasks. The appraisals of motive consistent/motive inconsistent (the task was performed correctly/incorrectly) and high/low perceived control (the teammate was well trained/not well trained) were manipulated to show the practicality of using appraisal theory as a basis for human robot interaction studies. Robot form was investigated for its influence on emotions experienced. Finally, the influence of high and low control on the experience of positive emotions caused by another was investigated. Results show that a human - robot live interaction test bed is a valid way to influence participants' appraisals. Manipulation checks of motive consistent/motive inconsistent, high/low perceived control and the proper appraisal of cause were significant. Form was shown to influence both the positive and negative emotions experienced, the more lifelike agents were rated higher in positive emotions and lower in negative emotions. The emotion gratitude was shown to be greater during conditions of low control when the entities performed correctly, suggesting that more experiments should be conducted investigating agent caused motive-conducive events. A second study was performed with participants evaluating their reaction to a hypothetical story. In this story they were interacting with either a human, robotic dog, or robot to complete a task. These three agent types and high/low perceived control were manipulated with all stories ending successfully. Results indicated that gratitude and appreciation are sensitive to the manipulation of agent type. It is suggested that, based on the results of these studies, the emotion gratitude should be added to Roseman et al. (2001) appraisal theory to describe the emotion felt during low-control, motive-consistent, other-caused events. These studies have also shown that the appraisal theory of emotion is useful in the study of human-robot and human-animal interactions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4217(Print)},
  keywords  = {*Cognitive Appraisal, *Emotions, Robotics},
  pages     = {5619--5619},
  publisher = {ProQuest Information & Learning},
  refid     = {2008-99040-597},
  volume    = {68},
}

@Misc{Koch2019,
  author    = {Koch, Sarah A.},
  title     = {Effectiveness and acceptability of a robot-based social skills intervention for children with autism spectrum disorder.},
  year      = {2019},
  status    = {Rejeitado: Inacessivel},
  abstract  = {This study examined the effectiveness, feasibility, and acceptability of a robot- based intervention program designed to improve social-emotional skills in school-age children with Autism Spectrum Disorder (ASD). Twenty-two children with ASD and mild-to-no cognitive impairment were randomized to intervention (n = 11) or waitlist control groups (n = 11) for eight weeks. Participants who completed the robot-based intervention displayed decreased overall engagement from baseline to post-intervention, based on an eye-tracking measure. Nonetheless, they reported high favorability ratings at post-intervention, including consistently high ratings of happiness, increased comfort ratings, and only slightly decreased ratings of desire for future interactions across time. Group comparisons indicated significant improvement in overall accuracy for identifying face drawings and photos corresponding with robotic emotional facial expressions for individuals in the intervention group. There were no group differences for amount of socially directed gaze with the robot during baseline and post-intervention sessions. Similarly, there were no group differences over time for generalized affect recognition and theory of mind skills. Taken together, results support the use of the robot-based intervention within this population as a tool for promoting an enjoyable learning environment conducive to skill development. Improved accuracy within the intervention group for matching robotic facial expressions, along with decreased visual engagement at post-intervention, suggests a shift from effortful processing to more automatic responding as a result of training. However, it is unclear whether this skill improvement resulted from learning of specific facts or the development of more generalized emotion decoding and understanding. Given strong baseline scores on robot-specific and generalized measures of emotion knowledge, results suggest that the information presented in the intervention may have been too simplistic for the sample included in the study, and future research will examine the efficacy and ultimate benefit of this tool within other subsets of children with ASD. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4217(Print)},
  keywords  = {*Autism Spectrum Disorders, *Pediatrics, *Robotics, Social Skills},
  pages     = {No Pagination Specified--No Pagination Specified},
  publisher = {ProQuest Information & Learning},
  refid     = {2018-52508-032},
  volume    = {80},
}

@Misc{Esposito2014,
  author    = {Esposito, Anna and Fortunati, Leopoldina and Lugano, Giuseppe},
  title     = {Modeling emotion, behavior and context in socially believable robots and ICT interfaces.},
  year      = {2014},
  abstract  = {This article discusses about modeling emotion, behavior and context in socially believable robots. The modeling and implementation of sophisticated multimodal software/hardware interfaces is a current scientific challenge of high societal relevance. The main characteristics entailed by these interfaces are being able to interact with people, inferring social, organizational and physical contexts based on sensed data, assisting people with special needs, enhancing elderly health-care assistance, learning and rehabilitation in daily functional activities. The idea to dedicate a special issue of Cognitive Computation to cover the interdisciplinary aspects of human human and human machine interactions was prompted by the author's desire to elicit new guidance in the quest for the implementation of emotionally and socially believable robot and ICT interfaces. The author hopes that this special issue will inspire and stimulate many additional researchers to join us in exploring the implications that socially believable robots and ICT interfaces will have in future societies. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Esposito, Anna: Department of Psychology and IIASS, Seconda Universita di Napoli, Caserta, Italy, anna.esposito@unina2.it},
  doi       = {10.1007/s12559-014-9309-5},
  issn      = {1866-9964(Electronic),1866-9956(Print)},
  journal   = {Cognitive Computation},
  keywords  = {*Human Machine Systems, *Robotics, *Simulation, Emotional States},
  pages     = {623--627},
  publisher = {Springer},
  refid     = {2014-54541-001},
  volume    = {6},
}

@Misc{Grundke2022,
  author    = {Grundke, Andrea and Stein, Jan-Philipp and Appel, Markus},
  title     = {Mind-reading machines: Distinct user responses to thought-detecting and emotion-detecting robots.},
  year      = {2022},
  abstract  = {Human-like robots and other systems with artificial intelligence are increasingly capable of recognizing and interpreting the mental processes of their human users. The present research examines how people evaluate these seemingly mind-reading machines based on the well-established distinction of human mind into agency (i.e., thoughts and plans) and experience (i.e., emotions and desires). Theory and research that applied this distinction to human-robot interaction showed that machines with experience were accepted less and were perceived to be eerier than those with agency. Considering that humans are not yet used to having their thoughts read by other entities and might feel uneasy about this notion, we proposed that thought-detecting robots are perceived to be eerier and are generally evaluated more negatively than emotion-detecting robots. Across two pre-registered experiments (N1 = 335, N2 = 536) based on text vignettes about different kinds of mind-detecting robots, we find support for our hypothesis. Furthermore, the effect remained independent of the six HEXACO personality dimensions, except for an unexpected interaction with conscientiousness. Implications and directions for future research are discussed. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Grundke, Andrea: Psychology of Communication and New Media, Julius-Maximilians-Universität Würzburg, Oswald-Külpe- Weg 82, Würzburg, Germany, 97074, andrea.grundke@uni-wuerzburg.de},
  doi       = {10.1037/tmb0000053},
  issn      = {2689-0208(Electronic)},
  journal   = {Technology, Mind, and Behavior},
  keywords  = {*Emotions, *Human Computer Interaction, *Mind, *Personality, *Robotics, Artificial Intelligence, Conscientiousness, Perception, Desire, Agency, Emotion Recognition},
  pages     = {No Pagination Specified--No Pagination Specified},
  publisher = {American Psychological Association},
  refid     = {2022-30881-001},
  volume    = {3},
}

@Misc{Namba2022a,
  author    = {Namba, Shushi and Nakamura, Koyo and Watanabe, Katsumi},
  title     = {The spatio-temporal features of perceived-as-genuine and deliberate expressions.},
  year      = {2022},
  abstract  = {Reading the genuineness of facial expressions is important for increasing the credibility of information conveyed by faces. However, it remains unclear which spatio-temporal characteristics of facial movements serve as critical cues to the perceived genuineness of facial expressions. This study focused on observable spatio-temporal differences between perceived-as-genuine and deliberate expressions of happiness and anger expressions. In this experiment, 89 Japanese participants were asked to judge the perceived genuineness of faces in videos showing happiness or anger expressions. To identify diagnostic facial cues to the perceived genuineness of the facial expressions, we analyzed a total of 128 face videos using an automated facial action detection system; thereby, moment-to-moment activations in facial action units were annotated, and nonnegative matrix factorization extracted sparse and meaningful components from all action units data. The results showed that genuineness judgments reduced when more spatial patterns were observed in facial expressions. As for the temporal features, the perceived-as-deliberate expressions of happiness generally had faster onsets to the peak than the perceived-as-genuine expressions of happiness. Moreover, opening the mouth negatively contributed to the perceived-as-genuine expressions, irrespective of the type of facial expressions. These findings provide the first evidence for dynamic facial cues to the perceived genuineness of happiness and anger expressions. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Namba, Shushi: sushishushi760@gmail.com},
  doi       = {10.1371/journal.pone.0271047},
  issn      = {1932-6203(Electronic)},
  journal   = {PLoS ONE},
  keywords  = {*Face Perception, *Facial Expressions, *Happiness, *Spatial Orientation (Perception), Anger Expression},
  publisher = {Public Library of Science},
  refid     = {2022-84367-001},
  volume    = {17},
}

@Misc{Brown2010,
  author    = {Brown, Jeff and Fenske, Mark and Neporent, Liz},
  title     = {The winner's brain: Eight strategies great minds use to achieve success.},
  year      = {2010},
  status    = {Rejeitado: Inacessivel},
  abstract  = {What can you learn about success from a robot? Why shouldn't you take a test while wearing red? And how can it be that people who perform poorly on a task often see themselves as being the best? In The Winner's Brain, Harvard-trained brain experts Dr. Jeff Brown and Dr. Mark Fenske explore the surprising science behind motivation, focus, and extraordinary achievement--and why the key to success really is all in your head. Contrary to popular belief, winning in life has little to do with IQ, your circumstances, your financial resources, or even luck. But it has everything to do with creating a failure-resistant brain. Every time you think a thought, feel an emotion, or execute a behavior, your neuro-circuitry changes, and the good news is you can take charge of this process. Throughout the book, dozens of Winners from all walks of life tell their stories. Many are well known, like B. B. King, Laura Linney, and motivational speaker Trisha Meili, the Central Park Jogger. Others are artists and inventors, musicians and business people, an FBI agent, a fighter pilot, even a high-altitude window washer. These insightful interviews further reveal how anyone can change their thinking to improve their life. Fascinating, accessible, and compulsively readable. The Winner's Brain shows you how to unlock your brain's hidden potential to achieve success. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Cambridge, MA, US},
  issn      = {978-07382-1360-6 (Hardcover)},
  journal   = {The winner's brain: Eight strategies great minds use to achieve success.},
  keywords  = {*Achievement, *Brain, *Cognitive Processes, *Mind, Strategies},
  pages     = {viii, 226--viii, 226},
  publisher = {Harvard University Press},
  refid     = {2010-06660-000},
}

@Misc{Lakatos2014,
  author    = {Lakatos, Gabriella and Gácsi, Márta and Konok, Veronika and Brúder, Ildikó and Bereczky, Boróka and Korondi, Péter and Miklósi, Ádám},
  title     = {Emotion attribution to a non-humanoid robot in different social situations.},
  year      = {2014},
  abstract  = {In the last few years there was an increasing interest in building companion robots that interact in a socially acceptable way with humans. In order to interact in a meaningful way a robot has to convey intentionality and emotions of some sort in order to increase believability. We suggest that human-robot interaction should be considered as a specific form of inter-specific interaction and that human-animal interaction can provide a useful biological model for designing social robots. Dogs can provide a promising biological model since during the domestication process dogs were able to adapt to the human environment and to participate in complex social interactions. In this observational study we propose to design emotionally expressive behaviour of robots using the behaviour of dogs as inspiration and to test these dog-inspired robots with humans in inter-specific context. In two experiments (wizard-of-oz scenarios) we examined humans’ ability to recognize two basic and a secondary emotion expressed by a robot. In Experiment 1 we provided our companion robot with two kinds of emotional behaviour ("happiness" and "fear"), and studied whether people attribute the appropriate emotion to the robot, and interact with it accordingly. In Experiment 2 we investigated whether participants tend to attribute guilty behaviour to a robot in a relevant context by examining whether relying on the robot’s greeting behaviour human participants can detect if the robot transgressed a predetermined rule. Results of Experiment 1 showed that people readily attribute emotions to a social robot and interact with it in accordance with the expressed emotional behaviour. Results of Experiment 2 showed that people are able to recognize if the robot transgressed on the basis of its greeting behaviour. In summary, our findings showed that dog-inspired behaviour is a suitable medium for making people attribute emotional states to a non-humanoid robot. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Lakatos, Gabriella: Hungarian Academy of Sciences, Eotvos Lorand University, Comparative Ethology Research Group, Pazmany Peter setany 1/C, Budapest, Hungary, 1117, gabriella.lakatos@gmail.com},
  doi       = {10.1371/journal.pone.0114207},
  issn      = {1932-6203(Electronic)},
  journal   = {PLoS ONE},
  keywords  = {*Attribution, *Emotions, Robotics},
  publisher = {Public Library of Science},
  refid     = {2015-04572-001},
  volume    = {9},
}

@Misc{Breazeal2007,
  author    = {Breazeal, Cynthia and Picard, Rosalind},
  title     = {The role of emotion-inspired abilities in relational robots.},
  year      = {2007},
  status    = {Aceito: Inacessivel},
  abstract  = {This chapter presents our motivation and a snapshot of our work to date in building robots with social-emotional skills, inspired by findings in neuroscience, cognitive science, and human behavior. Our primary motivation for building robots with social-emotional-inspired capabilities is to develop "relational robots" and their associated applications in diverse areas such as health, education, or work productivity where the human user derives performance benefit from establishing a kind of social rapport with the robot. We describe some of the future applications for such robots, provide a brief summary of the current capabilities of state-of-the-art socially interactive robots, present recent findings in human-computer interaction, and conclude with a few challenges that we would like to see addressed in future research. Much of what we describe in this chapter cannot be done by most machines today, but we are working to bring about research breakthroughs that will make such things possible. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {New York, NY, US},
  issn      = {0-19-517761-4 (Hardcover); 978-0-19-517761-9 (Hardcover)},
  journal   = {Neuroergonomics: The brain at work.},
  keywords  = {*Emotions, *Human Factors Engineering, *Neurosciences, *Robotics, Behavior, Cognitive Science, Human Computer Interaction, Virtual Reality},
  pages     = {275--292},
  publisher = {Oxford University Press},
  refid     = {2007-09206-018},
  series    = {Series in human-technology interaction.},
  url       = {https://www.researchgate.net/publication/289801203_The_Role_of_Emotion-Inspired_Abilities_in_Relational_Robots},
}

@Misc{Terada2017,
  author    = {Terada, Kazunori and Takeuchi, Chikara},
  title     = {Emotional expression in simple line drawings of a robot's face leads to higher offers in the ultimatum game.},
  year      = {2017},
  abstract  = {In the present study, we investigated whether expressing emotional states using a simple line drawing to represent a robot’s face can serve to elicit altruistic behavior from humans. An experimental investigation was conducted in which human participants interacted with a humanoid robot whose facial expression was shown on an LCD monitor that was mounted as its head (Study 1). Participants were asked to play the ultimatum game, which is usually used to measure human altruistic behavior. All participants were assigned to be the proposer and were instructed to decide their offer within 1 min by controlling a slider bar. The corners of the robot’s mouth, as indicated by the line drawing, simply moved upward, or downward depending on the position of the slider bar. The results suggest that the change in the facial expression depicted by a simple line drawing of a face significantly affected the participant’s final offer in the ultimatum game. The offers were increased by 13% when subjects were shown contingent changes of facial expression. The results were compared with an experiment in a teleoperation setting in which participants interacted with another person through a computer display showing the same line drawings used in Study 1 (Study 2). The results showed that offers were 15% higher if participants were shown a contingent facial expression change. Together, Studies 1 and 2 indicate that emotional expression in simple line drawings of a robot’s face elicits the same higher offer from humans as a human telepresence does. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Terada, Kazunori: terada@gifu-u.ac.jp},
  doi       = {10.3389/fpsyg.2017.00724},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Altruism, *Facial Expressions, *Human Computer Interaction, *Robotics, *Social Influences, Games, Human Robot Interaction},
  publisher = {Frontiers Media S.A.},
  refid     = {2017-28068-001},
  volume    = {8},
}

@Misc{Carpenter2014,
  author    = {Carpenter, Julie},
  title     = {The quiet professional: An investigation of U.S. military explosive ordnance disposal personnel interactions with everyday field robots.},
  year      = {2014},
  status    = {Rejeitado: Inacessivel},
  abstract  = {This research explores interactions between Explosive Ordnance Disposal (EOD) personnel and the robots used every day. It was designed to richly describe the nuances of these interactions, especially those related to operator emotion associated with the robots. In this study, the EOD human-robot dynamic was investigated by interviewing 23 EOD personnel, collecting demographic information, and using one-on-one semi-structured interviews. Study results suggest EOD personnel relationships among peers and team members showed distinct patterns in human-human relationships as part of a Human-Human Interaction Model (HHIM) in terms of expectations of performance, and beliefs, values, and actions, related to their work. Findings described here also suggest performance expectations and other factors of the HHIM of teamwork do not map onto EOD personnel human-robot interactions. However, in some cases there is a tendency for personnel to ascribe human traits to robots, creating nuanced human-technology relationships introduced here as the Robot Accommodation Dilemma (RAD). These findings have implications for future personnel training and the refinement of robot design considerations for EOD and other fields that rely on critical small group communication and decision-making skills. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {US},
  issn      = {0419-4217(Print)},
  keywords  = {*Decision Making, *Semi-Structured Interview, Human Technology Interaction},
  pages     = {No Pagination Specified--No Pagination Specified},
  publisher = {ProQuest Information & Learning},
  refid     = {2014-99160-517},
  volume    = {75},
}

@Misc{Ledwig2007,
  author    = {Ledwig, Marion},
  title     = {Review of Who needs emotions? The brain meets the robot.},
  year      = {2007},
  status    = {Rejeitado: Inacessivel},
  abstract  = {Reviews the book, Who needs emotions? The brain meets the robot by Jean-Marc Fellous and Michael A. Arbib (see record 2005-03475-000). This book is a cutting-edge research volume. The editors have chosen 12 stimulating new essays on the intersection of brains, emotions, and robotics research, which are divided into four parts: perspectives, brains, robots, and conclusions. I think that this collection of essays is an excellent anthology in terms of cutting-edge research on the brain, and also to a lesser degree in matters of artificial intelligence, I was surprised that the book's title question failed to be sufficiently addressed. Hence, one may get the impression that because most of the book addresses how the brain works, emotions will eventually be reduced to brain functions. So, in general I was a little bit perplexed by the book, because I expected something different from the content given the title. Although the chapters are readable, some of them use medical terminology to a high degree, so that one can expect graduate students to have at least some problems understanding the content; even emotion researchers may encounter some problems in this regard. Nevertheless, I would highly recommend this book because of all the interesting insights into how the brain works with regard to the emotions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Ledwig, Marion: University of Nevada, Las Vegas, Department of Philosophy, Box 455028, 4505 Maryland Parkway, Las Vegas, NV, US, 89154-5028, marion.ledwig@unlv.edu},
  issn      = {1465-394X(Electronic),0951-5089(Print)},
  journal   = {Philosophical Psychology},
  keywords  = {*Artificial Intelligence, *Brain, *Emotions, *Robotics, Insight},
  pages     = {551--555},
  publisher = {Taylor & Francis},
  refid     = {2007-12726-011},
  volume    = {20},
}

@Misc{Kedzierski2013,
  author    = {Kędzierski, Jan and Muszyński, Robert and Zoll, Carsten and Oleksy, Adam and Frontkiewicz, Mirela},
  title     = {EMYS--Emotive head of a social robot.},
  year      = {2013},
  abstract  = {This paper presents the design, control, and emotion expressions capabilities of the robotic head EMYS. The concept of motion control system based on FACS theory is proposed. On the basis of this control system six basics emotions are designed for EMYS head. The proposed head shapes are verified in experiments with participation of children aged 8-12. The results of the experiments, perception of the proposed design, and control system are discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Muszyński, Robert: Institute of Computer Engineering, Control and Robotics, Wroclaw University of Technology, ul. Janiszewskiego 11/17, Wroclaw, Poland, 50-372, robert.muszynski@pwr.wroc.pl},
  doi       = {10.1007/s12369-013-0183-1},
  issn      = {1875-4805(Electronic),1875-4791(Print)},
  journal   = {International Journal of Social Robotics},
  keywords  = {*Classroom Environment, *Emotional Control, *Emotionality (Personality), *Human Machine Systems Design, *Robotics, Facial Expressions, Motion Perception, Social Robotics},
  pages     = {237--249},
  publisher = {Springer},
  refid     = {2013-09573-001},
  volume    = {5},
}

@Misc{Wang2017,
  author    = {Wang, Luyao and Li, Chunlin and Wu, Jinglong},
  title     = {The status of research into intention recognition.},
  year      = {2017},
  abstract  = {In recent years, service robots have been widely used in many fields, especially for assisting the elderly and disabled. For example, the medical care of patients with Alzheimer’s disease has become a worldwide problem. Existing service robots with some intelligence quotient can perform actions that are programmed by a human. However, the robot cannot understand human intentions or communicate with people naturally. Understanding the intent of the service object could allow the robot to provide better service. Therefore, the most critical component of human-computer interactions is intention recognition. There are currently many methods by which intention recognition can be achieved, such as EMG, EOG and EEG. In addition, emotion is one of the important factors during intention recognition, and this has been a breakthrough notion. This chapter summarizes the current status of research into intention recognition and gives a brief description of the relationship between emotion and intention. We hope to provide more ideas for optimizing human-computer interactions. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Hershey, PA, US},
  doi       = {10.4018/978-1-5225-0925-7.ch010},
  issn      = {978-1-522-50925-7 (Hardcover); 978-1-522-50926-4 (Digital (undefined format))},
  journal   = {Improving the quality of life for dementia patients through progressive detection, treatment, and care.},
  keywords  = {*Automated Information Processing, *Emotions, *Experimentation, *Intention, *Robotics, Electroencephalography},
  pages     = {201--221},
  publisher = {Medical Information Science Reference/IGI Global},
  refid     = {2016-59877-010},
  series    = {Advances in psychology, mental health, and behavioral studies (APMHBS) book series.},
}

@Misc{Batliner2008,
  author    = {Batliner, Anton and Steidl, Stefan and Hacker, Christian and Nöth, Elmar},
  title     = {Private emotions versus social interaction: A data-driven approach towards analysing emotion in speech.},
  year      = {2008},
  abstract  = {The 'traditional' first two dimensions in emotion research are VALENCE and AROUSAL. Normally, they are obtained by using elicited, acted data. In this paper, we use realistic, spontaneous speech data from our 'AIBO' corpus (human-robot communication, children interacting with Sony's AIBO robot). The recordings were done in a Wizard-of-Oz scenario: the children believed that AIBO obeys their commands; in fact, AIBO followed a fixed script and often disobeyed. Five labellers annotated each word as belonging to one of eleven emotion-related states; seven of these states which occurred frequently enough are dealt with in this paper. The confusion matrices of these labels were used in a Non-Metrical Multi-dimensional Scaling to display two dimensions; the first we interpret as VALENCE, the second, however, not as AROUSAL but as INTERACTION, i.e., addressing oneself (angry, joyful) or the communication partner (motherese, reprimanding). We show that it depends on the specificity of the scenario and on the subjects' conceptualizations whether this new dimension can be observed, and discuss impacts on the practice of labelling and processing emotional data. Two-dimensional solutions based on acoustic and linguistic features that were used for automatic classification of these emotional states are interpreted along the same lines. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Batliner, Anton: Lehrstuhl fur Mustererkennung, FAU Erlangen-Nurnberg, Martensstr. 3, Erlangen, Germany, 91058, batliner@informatik.uni-erlangen.de},
  doi       = {10.1007/s11257-007-9039-4},
  issn      = {1573-1391(Electronic),0924-1868(Print)},
  journal   = {User Modeling and User-Adapted Interaction},
  keywords  = {*Communication, *Emotional States, *Emotions, *Social Interaction, Speech Development},
  pages     = {175--206},
  publisher = {Springer},
  refid     = {2008-04806-006},
  volume    = {18},
}

@Misc{Rocks2009,
  author    = {Rocks, Claire and Jenkins, Sarah and Studley, Matthew and McGoran, David},
  title     = {'Heart Robot', a public engagement project.},
  year      = {2009},
  abstract  = {Heart Robot was a public engagement project funded by the Engineering and Physical Sciences Research Council (EPSRC). The aim of the project was to challenge cultural perceptions of robots, and to stimulate thought and debate in members of the general public around research in the field of social and emotional robotics. Fusing the traditions of Bunraku puppetry, the technology of animatronics and the field of artificial emotion and social intelligence, Heart Robot presented a series of entertaining, thought-provoking, and moving performances at fourteen events in the south-west region of the UK between May and December 2008. This paper presents a summary of the independent evaluation of the project. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Rocks, Claire: University of the West of England, Science Communication Unit, Faculty of Health and Life Sciences, Frenchay Campus, Coldharbour Lane, Bristol, United Kingdom, BS16 1QY, Claire.Rocks@uwe.ac.uk},
  doi       = {10.1075/is.10.3.07roc},
  issn      = {1572-0381(Electronic),1572-0373(Print)},
  journal   = {Interaction Studies: Social Behaviour and Communication in Biological and Artificial Systems},
  keywords  = {*Computer Attitudes, *Robotics, *Sciences, Emotions, Engineering Psychology},
  pages     = {427--452},
  publisher = {John Benjamins},
  refid     = {2010-01354-007},
  volume    = {10},
}

@Misc{Kolnes2022,
  author    = {Kolnes, Martin and Gentsch, Kornelia and van Steenbergen, Henk and Uusberg, Andero},
  title     = {The mystery remains: Breadth of attention in flanker and navon tasks unaffected by affective states induced by an appraisal manipulation.},
  year      = {2022},
  abstract  = {Affective effects on breadth of attention have been related to aspects of different components of affective states such as the arousal and valence of affective experience and the motivational intensity of action tendency. As none of these explanations fully aligns with existing evidence, we hypothesised that affective effects on breadth of attention may arise from the appraisal component of affective states. Based on this reconceptualisation, we tested the effects of conduciveness and power appraisals on two measures of breadth of attention. In two web-based experiments, we manipulated these appraisals in a 2 × 2 design using a game-like arithmetic task where participants could (1) gain or lose rewards (goal conducive vs. obstructive) based on (2) either their action or the actions of a “robot” (high vs. low power). Breadth of attention was assessed using the flanker task (Experiment 1; n = 236) and the Navon task (Experiment 2; n = 215). We found that appraisals did not directly influence breadth of attention even though high power appraisal significantly improved the overall performance in both experiments indicating successful appraisal manipulation. We discuss ways in which these findings inform future efforts to explain the origins of affective effects on attentional breadth. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Kolnes, Martin: martin.kolnes@ut.ee},
  doi       = {10.1080/02699931.2022.2056580},
  issn      = {1464-0600(Electronic),0269-9931(Print)},
  journal   = {Cognition and Emotion},
  pages     = {No Pagination Specified--No Pagination Specified},
  publisher = {Taylor & Francis},
  refid     = {2022-51382-001},
}

@Misc{Adolphs2005,
  author    = {Adolphs, Ralph},
  title     = {Could a Robot Have Emotions? Theoretical Perspectives from Social Cognitive Neuroscience.},
  year      = {2005},
  abstract  = {Could a robot have emotions? I begin by dissecting the initial question, and propose that we should attribute emotions and feelings to a system only if it satisfies criteria in addition to mere behavioral duplication. Those criteria require in turn a theory of what emotions and feelings are. Some aspects of emotion depend only on how humans react to observing behavior, some depend additionally on a scientific account of adaptive behavior, and some depend also on how that behavior is internally generated. Roughly, these three aspects correspond to the social communicative, the adaptive/regulatory, and the experiential aspects of emotion. I summarize these aspects in subsequent sections. I conclude with the speculation that robots could certainly interact socially with humans within a restricted domain (they already do), but that correctly attributing emotions and feelings to them would require that robots are situated in the world and constituted internally in respects that are relevantly similar to humans. In particular, if robotics is to be a science that can actually tell us something new about what emotions are, we need to engineer an internal processing architecture that goes beyond merely fooling humans into judging that the robot has emotions. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Adolphs, Ralph: Division of Humanities and Social Sciences, California Institute of Technology, Pasadena, CA, US, 91125, radolphs@hss.caltech.edu},
  doi       = {10.1093/acprof:oso/9780195166194.001.0001},
  issn      = {0-19-516619-1 (Hardcover)},
  journal   = {Who needs emotions?: The brain meets the robot.},
  keywords  = {*Emotions, *Neurosciences, *Robotics, *Social Cognition, *Social Neuroscience, Cognitive Neuroscience},
  pages     = {9--25},
  publisher = {Oxford University Press},
  refid     = {2005-03475-002},
  series    = {Series in affective science.},
}

@Misc{Arkin2005,
  author    = {Arkin, Ronald C.},
  title     = {Moving Up the Food Chain: Motivation and Emotion in Behavior-Based Robots.},
  year      = {2005},
  abstract  = {This article investigates the relationship between motivations and emotions as evidenced by a broad range of animal models, including humans. Emotions constitute a subset of motivations that provide support for an agent's survival in a complex world. Both motivations and emotions affect behavioral performance, but motivation can additionally lead to the formulation of concrete goal-achieving behavior, whereas emotions are concerned with modulating existing behaviors in support of current activity. My focus is placed on how these models can have utility within the context of working robotic systems. Behavior-based control serves as the primary vehicle through which emotions and motivations are integrated into robots ranging from hexapods to wheeled robots to humanoids. In this framework, motivations and emotions dynamically affect the underlying control of a cybernetic system by altering its underlying behavioral parameters. I review actual robotic examples that have, each in their own way, provided useful environments where questions about emotions and motivations can be addressed. I start with a description of models of the sowbug that provided the first testbed for asking questions about the use of parallel streams of sensory information, goal-oriented behaviors, motivation and emotions, and developmental growth. I then move on in some detail to a model of the praying mantis, in which explicit motivational state variables such as fear, hunger, and sex affect the selection of motivated behaviors. Moving on to more-complex systems, I review the progress made in using attachment theory as a basis for robot exploration. I then describe the attempts at using canine ethology to design doglike robots that use their emotional and motivational states to bond with their human counterparts. Finally, I describe an ongoing modeling effort to address the issue of time varying affect-related phenomena such as personality traits, attitudes, moods, and emotions. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Arkin, Ronald C.: Mobile Robot Laboratory, College of Computing, Institute of Technology, Atlanta, GA, US, 30332-0280, arkin@cc.gatech.edu},
  doi       = {10.1093/acprof:oso/9780195166194.003.0009},
  issn      = {0-19-516619-1 (Hardcover)},
  journal   = {Who needs emotions?: The brain meets the robot.},
  keywords  = {*Animal Models, *Emotions, *Motivation, Robotics},
  pages     = {245--269},
  publisher = {Oxford University Press},
  refid     = {2005-03475-009},
  series    = {Series in affective science.},
}

@Misc{Breazeal2005,
  author    = {Breazeal, Cynthia and Brooks, Rodney},
  title     = {Robot emotion: A functional perspective.},
  year      = {2005},
  abstract  = {Robots are becoming more and more ubiquitous in human environments. The time has come when their ability to intelligently and effectively interact with us needs to match the level of technological sophistication they have already achieved, whether these robots are tools, avatars (human surrogates), partners (as in a team) or cyborg extensions (prostheses). Emotion-inspired mechanisms can improve the way autonomous robots operate in a human environment with people, and can improve the ability of these robots to effectively achieve their own goals. Such goals may be related to accomplishing tasks or satisfying motivations, and they may be achieved either autonomously or in cooperation with a person. In order to do this in a way that is natural for humans, the robot needs to be designed with a social model in mind. We illustrate this concept by describing in detail the design of Kismet, an anthropomorphic robot that interacts with a human in a social way, focusing on its facial and vocal expressions and gaze direction. Kismet's architecture includes a cognitive system that is tightly coupled to a separate emotive system. Each is designed as interacting networks of "specialists" that are activated when specific conditions are met. The cognitive component is responsible for perceiving and interpreting events, and for selecting among a hierarchy of goal-achieving behaviors, in accordance with its current motivational drives. It is primarily concerned with homeostasis and "well being." The emotive system implements eight basic emotions that are proposed to exist across species. It detects those internal and external events that have affective value, and motivates either task-based or communicative behavior to pursue beneficial interactions and to avoid those that are not beneficial by modulating the operation of the cognitive component. When placed in a realistic social setting, these two systems interact to achieve lifelike attention bias, flexible decision making, goal prioritization and persistence, and effective communication where the robot interacts in a natural and intuitive way with the person to achieve its goal (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Breazeal, Cynthia: MIT Media Laboratory, 20 Ames Street, E1S-449, Cambridge, MA, US, 02139, cynthia@media.mit.edu},
  doi       = {10.1093/acprof:oso/9780195166194.003.0010},
  issn      = {0-19-516619-1 (Hardcover)},
  journal   = {Who needs emotions?: The brain meets the robot.},
  keywords  = {*Cognitive Processes, *Emotions, *Robotics, *Social Cognition, Artificial Intelligence, Interpersonal Communication},
  pages     = {271--310},
  publisher = {Oxford University Press},
  refid     = {2005-03475-010},
  series    = {Series in affective science.},
}

@Misc{Spekman2018,
  author    = {Spekman, Marloes L. C. and Konijn, Elly A. and Hoorn, Johan F.},
  title     = {Perceptions of healthcare robots as a function of emotion-based coping: The importance of coping appraisals and coping strategies.},
  year      = {2018},
  abstract  = {The urgent pressure on healthcare increases the need for understanding how new technology such as social robots may offer solutions. Many healthcare situations are emotionally charged, which likely affects people's perceptions of such robots in healthcare contexts. Thus far however, little attention has been paid to how people's prior emotions may influence their perceptions of the robot. Based on emotional appraisal theories and prior research, we assume that particularly emotional coping appraisals would influence healthcare-robot perceptions. Additionally, we tested effects of actual coping through the use of emotion-focused and problem-focused coping strategies. Hypotheses were tested in a 2 (sad vs. angry) × 2 (hard-to-cope-with vs. easy-to-cope-with) between-subjects experiment, also including a control group. Results (N = 132; age range 18-36) showed that manipulated coping potential indirectly affected perceptions of a healthcare robot via the appraisal of coping potential. Furthermore, positive emotion-focused coping affected perceptions of a healthcare robot positively. Thus, people's healthcare-robot perceptions were affected by how they cope or how they think they can cope with their emotions, rather than by the emotions as such. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Spekman, Marloes L. C.: De Boelelaan 1105, Amsterdam, Netherlands, 1081 HV, m.spekman@vu.nl},
  doi       = {10.1016/j.chb.2018.03.043},
  issn      = {1873-7692(Electronic),0747-5632(Print)},
  journal   = {Computers in Human Behavior},
  keywords  = {*Attitudes, *Coping Behavior, *Emotions, *Robotics, *Telemedicine, Evaluation},
  pages     = {308--318},
  publisher = {Elsevier Science},
  refid     = {2018-24910-032},
  volume    = {85},
}

@Misc{Waytz2014,
  author    = {Waytz, Adam and Norton, Michael I.},
  title     = {Botsourcing and outsourcing: Robot, British, Chinese, and German workers are for thinking--not feeling--jobs.},
  year      = {2014},
  abstract  = {Technological innovations have produced robots capable of jobs that, until recently, only humans could perform. The present research explores the psychology of “botsourcing”--the replacement of human jobs by robots--while examining how understanding botsourcing can inform the psychology of outsourcing--the replacement of jobs in one country by humans from other countries. We test four related hypotheses across six experiments: (1) Given people’s lay theories about the capacities for cognition and emotion for robots and humans, workers will express more discomfort with botsourcing when they consider losing jobs that require emotion versus cognition; (2) people will express more comfort with botsourcing when jobs are framed as requiring cognition versus emotion; (3) people will express more comfort with botsourcing for jobs that do require emotion if robots appear to convey more emotion; and (4) people prefer to outsource cognition- versus emotion-oriented jobs to other humans who are perceived as more versus less robotic. These results have theoretical implications for understanding social cognition about both humans and nonhumans and practical implications for the increasingly botsourced and outsourced economy. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Waytz, Adam: Northwestern University, 2001 Sheridan Road, Evanston, IL, US, 60208, a-waytz@kellogg.northwestern.edu},
  doi       = {10.1037/a0036054},
  issn      = {1931-1516(Electronic),1528-3542(Print)},
  journal   = {Emotion},
  keywords  = {*Human Computer Interaction, *Robotics, *Anthropomorphism, Emotions, Mind, Outsourcing},
  pages     = {434--444},
  publisher = {American Psychological Association},
  refid     = {2014-12471-006},
  volume    = {14},
}

@Misc{LewandowskaTomaszczyk2016,
  author    = {Lewandowska-Tomaszczyk, Barbara and Wilson, Paul A.},
  title     = {Physical and moral disgust in socially believable behaving systems in different cultures.},
  year      = {2016},
  abstract  = {The aim of the present study is to use the GRID, online emotions sorting and corpus methodologies to illuminate different types of disgust that an emotion-sensitive socially interacting robot would need to encode and decode in order to competently produce and recognise these and other types of physical, moral and aesthetic types of complex emotions in social settings. We argue that emotions in general, and different types of disgust as an instance of these, differ with respect to the amount of cognitive grounding they need in order to arise and social robots will successfully use such emotions provided they do not only recognise and produce physical, bodily manifestations of emotions, but also have access to large knowledge bases and are able to process situational context clues. The different types of disgust are identified and compared cross-culturally to provide an evaluation of their relative salience. The study also underscores the conceptual viewpoint of emotions as clusters of emotions rather than solitary, individual representations. We argue that such clustering should be at the heart of emotions modelling in social robots. In order to successfully use the emotion of disgust in their interactions with humans, robots need to be sensitive to possible within-culture and cross-culture differences pertaining to such emotions, exemplified by British English and Polish in the present study. Given the centrality of values to the emotion of disgust, robots need to have the capacity to update from a knowledge base and learn from the situational context the set of values for each significant human that they interact with. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Lewandowska-Tomaszczyk, Barbara: State University of Applied Sciences in Konin, Konin, Poland, blt@uni.lodz.pl},
  doi       = {10.1007/978-3-319-31056-5_7},
  issn      = {978-3-319-31055-8 (Hardcover); 978-3-319-31056-5 (Digital (undefined format))},
  journal   = {Toward robotic socially believable behaving systems: Modeling emotions, Vol. I},
  keywords  = {*Aesthetics, *Cross Cultural Differences, *Emotions, *Robotics, *Social Robotics, Behavior, Disgust, Language, Morality},
  pages     = {105--132},
  publisher = {Springer International Publishing AG},
  refid     = {2016-27240-007},
  series    = {Intelligent systems reference library.},
}

@Misc{Yang2017,
  author    = {Yang, Euijung and Dorneich, Michael C.},
  title     = {The emotional, cognitive, physiological, and performance effects of variable time delay in robotic teleoperation.},
  year      = {2017},
  abstract  = {The effects of intermittent and variable time delay were investigated to understand the cognitive and physical consequences of gaps between an input from an operator and the corresponding feedback response from the system. Time delay has been shown to disrupt task performance in various areas including psychology and telerobotics. Previous research in multiple domains has focused on the performance effects of time delay and overcoming technological limitations that cause time delay. However, robotics researchers have yet to study the effects of variable time delay on specific operator emotions, usability, and physiological activation in teleoperations. This study investigates the influence of variable time delay not only on task performance, but also operator emotions, physiological arousal, cognitive workload, and usability in teleoperation. Time delay was manipulated by introducing lag into the system feedback. Participants were asked to navigate a remote-control robot vehicle through mazes of differing levels of task complexity in a remote location and simultaneously identify targets. Results showed that operator frustration, anger, and workload increased while usability and task performance decreased when intermittent and variable feedback lag was introduced to a robotic navigation task. The effect of the variable time delay was greater than the effect of task complexity. Furthermore, results suggest that the effects are of time delay and task complexity are additive. A better understanding of the emotional experiences of human operators and the corresponding physiological signals is of crucial importance to designing affect-aware robotic systems that have the ability to appropriately mitigate negative operator emotional states. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Dorneich, Michael C.: Industrial and Manufacturing Systems Department, Iowa State University, 3004 Black Engineering Building, Ames, IA, US, 50011-2164, dorneich@iastate.edu},
  doi       = {10.1007/s12369-017-0407-x},
  issn      = {1875-4805(Electronic),1875-4791(Print)},
  journal   = {International Journal of Social Robotics},
  keywords  = {*Human Computer Interaction, *Robotics, *Human Robot Interaction, Constant Time Delay},
  pages     = {491--508},
  publisher = {Springer},
  refid     = {2017-21035-001},
  volume    = {9},
}

@Article{Kim2022,
  author    = {Kim, Hyun K. and Jeong, Heejin and Park, Jangwoon and Park, Jaehyun and Kim, Won-Seok and Kim, Nahyeong and Park, Subin and Paik, Nam-Jong},
  journal   = {International Journal of Human-Computer Interaction},
  title     = {Development of a comprehensive design guideline to evaluate the user experiences of meal-assistance robots considering human-machine social interactions.},
  year      = {2022},
  issn      = {1532-7590(Electronic),1044-7318(Print)},
  pages     = {No Pagination Specified--No Pagination Specified},
  abstract  = {Meal-assistance robots (MARs) help feed meals to users with disabilities or who need help consuming their meals. Although such MARs have been introduced in various fields, guidelines for evaluating MARs are limited. This study aims to develop comprehensive guidelines to assess the user experience (UX) of MAR designs considering its social interaction characteristics between humans and robots. Participants from three groups (patients, doctors, and caregivers) with different perspectives on MARs were recruited and a focus group interview was conducted to collect their UXs with MARs. The three groups showed different UXs with MARs in user interface design, robot arm motion, and safety and mobility. In addition, based on the literature review, eight UX features (usability, emotion, value, naturalness, assistance, acceptance, personality, and culture) are proposed to evaluate MAR interfaces. The proposed comprehensive design guideline will be particularly useful in evaluating and designing the UX of MARs. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Park, Jaehyun: jaehpark@inu.ac.kr},
  doi       = {10.1080/10447318.2021.2009672},
  publisher = {Taylor & Francis},
  refid     = {2022-23529-001},
}

@Misc{Broadbent2017,
  author    = {Broadbent, Elizabeth},
  title     = {Interactions with robots: The truths we reveal about ourselves.},
  year      = {2017},
  abstract  = {In movies, robots are often extremely humanlike. Although these robots are not yet reality, robots are currently being used in healthcare, education, and business. Robots provide benefits such as relieving loneliness and enabling communication. Engineers are trying to build robots that look and behave like humans and thus need comprehensive knowledge not only of technology but also of human cognition, emotion, and behavior. This need is driving engineers to study human behavior toward other humans and toward robots, leading to greater understanding of how humans think, feel, and behave in these contexts, including our tendencies for mindless social behaviors, anthropomorphism, uncanny feelings toward robots, and the formation of emotional attachments. However, in considering the increased use of robots, many people have concerns about deception, privacy, job loss, safety, and the loss of human relationships. Human-robot interaction is a fascinating field and one in which psychologists have much to contribute, both to the development of robots and to the study of human behavior. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Broadbent, Elizabeth: Department of Psychological Medicine, Faculty of Medical and Health Sciences, University of Auckland, Auckland, New Zealand, 1142, e.broadbent@auckland.ac.nz},
  doi       = {10.1146/annurev-psych-010416-043958},
  issn      = {1545-2085(Electronic),0066-4308(Print)},
  journal   = {Annual Review of Psychology},
  keywords  = {*Consumer Attitudes, *Human Computer Interaction, *Human Robot Interaction, Business, Education, Health Care Services, Robotics},
  pages     = {627--652},
  publisher = {Annual Reviews},
  refid     = {2017-01278-025},
  volume    = {68},
}

@Misc{Koch2017,
  author    = {Koch, Sarah A. and Stevens, Carl E. and Clesi, Christian D. and Lebersfeld, Jenna B. and Sellers, Alyssa G. and McNew, Myriah E. and Biasini, Fred J. and Amthor, Franklin R. and Hopkins, Maria I.},
  title     = {A feasibility study evaluating the emotionally expressive robot SAM.},
  year      = {2017},
  abstract  = {This two-part feasibility study evaluated the functionality and acceptability of Socially Animated Machine (SAM), a humanoid robotic monkey developed to elicit social interaction in children with Autism Spectrum Disorder (ASD). Socially Animated Machine was designed with an approachable, animal-like appearance, while preserving the essential features of a human face. The intent was to design a robot that would be interesting and engaging to children with ASD, yet maintain the capability to model facial expressions that convey emotional subtlety. Study 1 evaluated the accuracy of SAM’s emotional facial expressions. Typically developing children (N = 35 ) labeled and matched SAM’s expressions to photos of human expressions with moderate-to-substantial levels of agreement. Study 2 compared children’s level of social engagement across an interaction with SAM and an interaction with an adult experimenter. Children with ASD (N = 13) spent significantly more time attending to the partner’s face while interacting with SAM. When asked to rate their interaction with SAM, children with ASD reported high levels of happiness and comfort and requested additional interactions. These results suggest that SAM may serve as a useful tool in interventions to improve social skills, including emotion recognition, in children with ASD. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Koch, Sarah A.: Department of Psychology, University of Alabama at Birmingham, Campbell Hall, Suite 415, 1720 2nd Avenue South, Birmingham, AL, US, 35294, sakoch@uab.edu},
  doi       = {10.1007/s12369-017-0419-6},
  issn      = {1875-4805(Electronic),1875-4791(Print)},
  journal   = {International Journal of Social Robotics},
  keywords  = {*Autism Spectrum Disorders, *Human Computer Interaction, *Robotics, *Emotion Recognition, Human Robot Interaction, Social Robotics},
  pages     = {601--613},
  publisher = {Springer},
  refid     = {2017-28207-001},
  volume    = {9},
}

@Misc{So2020,
  author    = {So, Wing-Chee and Cheng, Chun-Ho and Lam, Wan-Yi and Huang, Ying and Ng, Ka-Ching and Tung, Hiu-Ching and Wong, Wing},
  title     = {A robot-based play-drama intervention may improve the joint attention and functional play behaviors of Chinese-speaking preschoolers with autism spectrum disorder: A pilot study.},
  year      = {2020},
  abstract  = {Children with autism spectrum disorder (ASD) have deficits in joint attention and play behaviors. We examined whether a robot-based play-drama intervention would promote these skills. Chinese-speaking preschool children were randomly assigned to an intervention group (N = 12) and a waitlist control group (N = 11). Children in the intervention group watched three robot dramas and engaged in role-plays with both robots and human experimenters over the course of 9 weeks. There were significant improvements in joint attention initiations and functional play behaviors in the intervention group. Parents of this group of children also reported less severe social impairments. It was therefore concluded that a robot-based play-drama intervention can enhance the joint attention and play behaviors of children with ASD. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  address   = {So, Wing-Chee: wingchee@cuhk.edu.hk},
  doi       = {10.1007/s10803-019-04270-z},
  issn      = {1573-3432(Electronic),0162-3257(Print)},
  journal   = {Journal of Autism and Developmental Disorders},
  keywords  = {*Autism Spectrum Disorders, *Childhood Play Behavior, *Early Intervention, *Robotics, *Group Intervention, Drama, Preschool Students, Role Playing},
  pages     = {467--481},
  publisher = {Springer},
  refid     = {2019-64935-001},
  volume    = {50},
}

@Misc{Wang2018,
  author    = {Wang, Xijing and Krumhuber, Eva G.},
  title     = {Mind perception of robots varies with their economic versus social function.},
  year      = {2018},
  abstract  = {While robots were traditionally built to achieve economic efficiency and financial profits, their roles are likely to change in the future with the aim to provide social support and companionship. In this research, we examined whether the robot’s proposed function (social vs. economic) impacts judgments of mind and moral treatment. Studies 1a and 1b demonstrated that robots with social function were perceived to possess greater ability for emotional experience, but not cognition, compared to those with economic function and whose function was not mentioned explicitly. Study 2 replicated this finding and further showed that low economic value reduced ascriptions of cognitive capacity, whereas high social value resulted in increased emotion perception. In Study 3, robots with high social value were more likely to be afforded protection from harm, and such effect was related to levels of ascribed emotional experience. Together, the findings demonstrate a dissociation between function type (social vs. economic) and ascribed mind (emotion vs. cognition). In addition, the two types of functions exert asymmetric influences on the moral treatment of robots. Theoretical and practical implications for the field of social psychology and human-computer interaction are discussed. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  address   = {Wang, Xijing: xijing.wang.13@ucl.ac.uk},
  doi       = {10.3389/fpsyg.2018.01230},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Economics, *Robotics, *Social Interaction, Mind},
  publisher = {Frontiers Media S.A.},
  refid     = {2018-36920-001},
  volume    = {9},
}

@Misc{Strait2017,
  author    = {Strait, Megan K. and Floerke, Victoria A. and Ju, Wendy and Maddox, Keith and Remedios, Jessica D. and Jung, Malte F. and Urry, Heather L.},
  title     = {Understanding the uncanny: Both atypical features and category ambiguity provoke aversion toward humanlike robots.},
  year      = {2017},
  abstract  = {Robots intended for social contexts are often designed with explicit humanlike attributes in order to facilitate their reception by (and communication with) people. However, observation of an “uncanny valley”--a phenomenon in which highly humanlike entities provoke aversion in human observers--has lead some to caution against this practice. Both of these contrasting perspectives on the anthropomorphic design of social robots find some support in empirical investigations to date. Yet, owing to outstanding empirical limitations and theoretical disputes, the uncanny valley and its implications for human-robot interaction remains poorly understood. We thus explored the relationship between human similarity and people's aversion toward humanlike robots via manipulation of the agents' appearances. To that end, we employed a picture-viewing task (Nagents = 60) to conduct an experimental test (Nparticipants = 72) of the uncanny valley's existence and the visual features that cause certain humanlike robots to be unnerving. Across the levels of human similarity, we further manipulated agent appearance on two dimensions, typicality (prototypic, atypical, and ambiguous) and agent identity (robot, person), and measured participants' aversion using both subjective and behavioral indices. Our findings were as follows: (1) Further substantiating its existence, the data show a clear and consistent uncanny valley in the current design space of humanoid robots. (2) Both category ambiguity, and more so, atypicalities provoke aversive responding, thus shedding light on the visual factors that drive people's discomfort. (3) Use of the Negative Attitudes toward Robots Scale did not reveal any significant relationships between people's pre-existing attitudes toward humanlike robots and their aversive responding--suggesting positive exposure and/or additional experience with robots is unlikely to affect the occurrence of an uncanny valley effect in humanoid robotics. This work furthers our understanding of both the uncanny valley, as well as the visual factors that contribute to an agent's uncanniness. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Strait, Megan K.: megan.strait@utrgv.edu},
  doi       = {10.3389/fpsyg.2017.01366},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Adult Attitudes, *Emotional Regulation, *Human Computer Interaction, *Robotics, *Anthropomorphism, Human Robot Interaction, Social Robotics},
  publisher = {Frontiers Media S.A.},
  refid     = {2017-39500-001},
  volume    = {8},
}

@Misc{Schein2015,
  author    = {Schein, Chelsea and Gray, Kurt},
  title     = {The eyes are the window to the uncanny valley: Mind perception, autism and missing souls.},
  year      = {2015},
  abstract  = {Horror movies have discovered an easy recipe for making people creepy: alter their eyes. Instead of normal eyes, zombies’ eyes are vacantly white, vampires’ eyes glow with the color of blood, and those possessed by demons are cavernously black. In the Academy Award winning Pan’s Labyrinth, director Guillermo del Toro created the creepiest of all creatures by entirely removing its eyes from its face, placing them instead in the palms of its hands. The unease induced by altering eyes may help to explain the uncanny valley, which is the eeriness of robots that are almost--but not quite--human (Mori, 1970). Much research has explored the uncanny valley, including the research reported by MacDorman & Entezari (in press), which focuses on individual differences that might predict the eeriness of humanlike robots. In their paper, they suggest that a full understanding of this phenomenon needs to synthesize individual differences with features of the robot. One theory that links these two concepts is mind perception, which past research highlights as essential to the uncanny valley (Gray & Wegner, 2012). Mind perception is linked to both individual differences--autism--and to features of the robot--the eyes--and can provide a deeper understanding of this arresting phenomenon. In this paper, we present original data that links uncanniness to the eyes through aberrant perceptions of mind. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {Gray, Kurt: Department of Psychology, University of North Carolina, Chapel Hill, Chapel Hill, NC, US, 27599, kurtgray@unc.edu},
  doi       = {10.1075/is.16.2.02sch},
  issn      = {1572-0381(Electronic),1572-0373(Print)},
  journal   = {Interaction Studies: Social Behaviour and Communication in Biological and Artificial Systems},
  keywords  = {*Individual Differences, *Mind, *Robotics, Autism Spectrum Disorders},
  pages     = {173--179},
  publisher = {John Benjamins},
  refid     = {2016-00627-002},
  volume    = {16},
}

@Misc{Santos2021,
  author    = {Santos, María and Egerstedt, Magnus},
  title     = {From motions to emotions: Can the fundamental emotions be expressed in a robot swarm?},
  year      = {2021},
  abstract  = {This paper explores the expressive capabilities of a swarm of miniature mobile robots within the context of inter-robot interactions and their mapping to the so-called fundamental emotions. In particular, we investigate how motion and shape descriptors that are psychologically associated with different emotions can be incorporated into different swarm behaviors for the purpose of artistic expositions. Based on these characterizations from social psychology, a set of swarm behaviors is created, where each behavior corresponds to a fundamental emotion. The effectiveness of these behaviors is evaluated in a survey in which the participants are asked to associate different swarm behaviors with the fundamental emotions. The results of the survey show that most of the research participants assigned to each video the emotion intended to be portrayed by design. These results confirm that abstract descriptors associated with the different fundamental emotions in social psychology provide useful motion characterizations that can be effectively transformed into expressive behaviors for a swarm of simple ground mobile robots. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Santos, María: maria.santos@gatech.edu},
  doi       = {10.1007/s12369-020-00665-6},
  issn      = {1875-4805(Electronic),1875-4791(Print)},
  journal   = {International Journal of Social Robotics},
  keywords  = {*Emotions, *Robotics, *Social Psychology, *Surveys, Social Robotics},
  pages     = {751--764},
  publisher = {Springer},
  refid     = {2020-49444-001},
  volume    = {13},
}

@Misc{Hirt2021,
  author    = {Hirt, Julian and Ballhausen, Nicola and Hering, Alexandra and Kliegel, Matthias and Beer, Thomas and Meyer, Gabriele},
  title     = {Social robot interventions for people with dementia: A systematic review on effects and quality of reporting.},
  year      = {2021},
  abstract  = {Background: Using non-pharmacological interventions is a current approach in dementia care to manage responsive behaviors, to maintain functional capacity, and to reduce emotional stress. Novel technologies such as social robot interventions might be useful to engage people with dementia in activities and interactions as well as to improve their cognitive, emotional, and physical status. Objective: Assessing the effects and the quality of reporting of social robot interventions for people with dementia. Methods: In our systematic review, we included quasi-experimental and experimental studies published in English, French, or German, irrespective of publication year. Searching CINAHL, Cochrane Library, MEDLINE, PsycINFO, and Web of Science Core Collection was supplemented by citation tracking and free web searching. To assess the methodological quality of included studies, we used tools provided by the Joanna Briggs Institute. To assess the reporting of the interventions, we applied CReDECI 2 and TIDieR. Results: We identified sixteen studies published between 2012 and 2018, including two to 415 participants with mostly non-defined type of dementia. Eight studies had an experimental design. The predominant robot types were pet robots (i.e., PARO). Most studies addressed behavioral, emotion-related, and functional outcomes with beneficial, non-beneficial, and mixed results. Predominantly, cognitive outcomes were not improved. Overall, studies were of moderate methodological quality. Conclusion: Heterogeneous populations, intervention characteristics, and measured outcomes make it difficult to generalize the results with regard to clinical practice. The impact of social robot interventions on behavioral, emotion-related, and functional outcomes should therefore be assessed considering the severity of dementia and intervention characteristics. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Hirt, Julian: Center for Dementia Care, Institute of Applied Nursing Sciences, Department of Health, University of Applied Sciences FHS St. Gallen, Rosenbergstrasse 59, St. Gallen, Switzerland, 9000, julian.hirt@ost.ch},
  doi       = {10.3233/JAD-200347},
  issn      = {1875-8908(Electronic),1387-2877(Print)},
  journal   = {Journal of Alzheimer's Disease},
  keywords  = {*Behavior Therapy, *Dementia, *Intervention, *Treatment Outcomes, *Social Robotics, Robotics, Sciences, Technology, Tracking},
  pages     = {773--792},
  publisher = {IOS Press},
  refid     = {2021-41878-025},
  volume    = {79},
}

@Misc{KoryWestlund2017,
  author    = {Kory Westlund, Jacqueline M. and Jeong, Sooyeon and Park, Hae W. and Ronfard, Samuel and Adhikari, Aradhana and Harris, Paul L. and DeSteno, David and Breazeal, Cynthia L.},
  title     = {Flat vs. expressive storytelling: Young children’s learning and retention of a social robot’s narrative.},
  year      = {2017},
  abstract  = {Prior research with preschool children has established that dialogic or active book reading is an effective method for expanding young children’s vocabulary. In this exploratory study, we asked whether similar benefits are observed when a robot engages in dialogic reading with preschoolers. Given the established effectiveness of active reading, we also asked whether this effectiveness was critically dependent on the expressive characteristics of the robot. For approximately half the children, the robot’s active reading was expressive; the robot’s voice included a wide range of intonation and emotion (Expressive). For the remaining children, the robot read and conversed with a flat voice, which sounded similar to a classic text-to-speech engine and had little dynamic range (Flat). The robot’s movements were kept constant across conditions. We performed a verification study using Amazon Mechanical Turk (AMT) to confirm that the Expressive robot was viewed as significantly more expressive, more emotional, and less passive than the Flat robot. We invited 45 preschoolers with an average age of 5 years who were either English Language Learners (ELL), bilingual, or native English speakers to engage in the reading task with the robot. The robot narrated a story from a picture book, using active reading techniques and including a set of target vocabulary words in the narration. Children were post-tested on the vocabulary words and were also asked to retell the story to a puppet. A subset of 34 children performed a second story retelling 4-6 weeks later. Children reported liking and learning from the robot a similar amount in the Expressive and Flat conditions. However, as compared to children in the Flat condition, children in the Expressive condition were more concentrated and engaged as indexed by their facial expressions; they emulated the robot’s story more in their story retells; and they told longer stories during their delayed retelling. Furthermore, children who responded to the robot’s active reading questions were more likely to correctly identify the target vocabulary words in the Expressive condition than in the Flat condition. Taken together, these results suggest that children may benefit more from the expressive robot than from the flat robot. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Kory Westlund, Jacqueline M.: jakory@media.mit.edu},
  doi       = {10.3389/fnhum.2017.00295},
  issn      = {1662-5161(Electronic)},
  journal   = {Frontiers in Human Neuroscience},
  keywords  = {*Language Development, *Robotics, *Storytelling, *Social Robotics, Emotions, Narratives, Preschool Students},
  publisher = {Frontiers Media S.A.},
  refid     = {2017-27793-001},
  volume    = {11},
}

@Misc{Botto2018,
  author    = {Botto, Sara Valencia and Rochat, Philippe},
  title     = {Sensitivity to the evaluation of others emerges by 24 months.},
  year      = {2018},
  abstract  = {Although the human proclivity to engage in impression management and care for reputation is ubiquitous, the question of its developmental outset remains open. In 4 studies, we demonstrate that the sensitivity to the evaluation of others (i.e., evaluative audience perception) is manifest by 24 months. In a first study, 14- to 24-month-old children (N = 49) were tested in situations in which the attention of an audience was systematically manipulated. Results showed that when the experimenter was inattentive, as opposed to attentive, children were more likely to explore an attractive toy. A second study (N = 31) explored whether same-aged children would consider not only the attention of the experimenter but also the values the experimenter expressed for two different outcomes when exploring a toy. We found that children reproduced outcomes that were positively valued by the experimenter significantly more when the experimenter was attentive but were more likely to reproduce negatively valued outcomes when the experimenter was inattentive. A third control study (N = 30) showed that the significant effect of Study 2 disappeared in the absence of different values. Lastly, Study 4 (N = 34) replicated and extended the phenomenon by showing toddler’s propensity to modify their behavior in the presence of 2 different experimenters, depending on both the experimenter’s evaluation of an outcome and their attention. Overall, these data provide the first convergent demonstration of evaluative audience perception in young children that precedes the full-fledged normative, mentalizing, and strong conformity psychology documented in 4- to 5-year-old children. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  address   = {Botto, Sara Valencia: Department of Psychology, Emory University, 36 Eagle Row, Atlanta, GA, US, 30322, sara.botto@emory.edu},
  doi       = {10.1037/dev0000548},
  issn      = {1939-0599(Electronic),0012-1649(Print)},
  journal   = {Developmental Psychology},
  keywords  = {*Evaluation, *Impression Management, *Psychosocial Development, *Self-Perception, *Social Cognition, Attention, Audiences, Early Childhood Development, Experimenters, Perceptual Development, Test Construction, Values},
  pages     = {1723--1734},
  publisher = {American Psychological Association},
  refid     = {2018-41343-012},
  volume    = {54},
}

@Misc{Lee2011,
  author    = {Lee, Sau-lai and Lau, Ivy Yee-man and Hong, Ying-yi},
  title     = {Effects of appearance and functions on likability and perceived occupational suitability of robots.},
  year      = {2011},
  abstract  = {This article reports three experiments that examined the association between (a) appearances and perceived capabilities of robots, (b) appearance and capabilities of robots and liking for the robots, and (c) perceived capabilities of robots and judgments concerning their suitability for different occupations. In Experiment 1, the authors found that participants perceived human- and animal-like robots to have relatively more warmth-related (e.g., emotion) capabilities than machinelike robots have. In Experiment 2, the authors found that liking for robots was not affected by their human likeness or their having warmth or competence capabilities. In Experiment 3, participants generally thought that robots should have information-processing and communication capabilities more than sensory and emotion capabilities. More interestingly, participants considered robots with different capabilities to be suitable for different occupations, preferring robots with emotion capabilities more in occupations that require frequent interactions with humans than in occupations that do not. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  address   = {Lee, Sau-lai: Division of Psychology, Nanyang Technological University, 14 Nanyang Drive, Singapore, Singapore, 637332, slleeh@ntu.edu.sg},
  doi       = {10.1177/1555343411409829},
  issn      = {2169-5032(Electronic),1555-3434(Print)},
  journal   = {Journal of Cognitive Engineering and Decision Making},
  keywords  = {*Judgment, *Likability, *Robotics, Occupations},
  pages     = {232--250},
  publisher = {Sage Publications},
  refid     = {2011-13542-006},
  volume    = {5},
}

@Misc{Fritz2017,
  author    = {Fritz, Thomas Hans and Brummerloh, Berit and Urquijo, Maria and Wegner, Katharina and Reimer, Enrico and Gutekunst, Sven and Schneider, Lydia and Smallwood, Jonathan and Villringer, Arno},
  title     = {Blame it on the bossa nova: Transfer of perceived sexiness from music to touch.},
  year      = {2017},
  abstract  = {Emotion elicited through music transfers to subsequent processing of facial expressions. Music may accordingly function as a social technology by promoting social bonding. Here, we investigated whether music would cross-modally influence the perception of sensual touch, a behavior related to mating. A robot applied precisely controlled gentle touch to a group of healthy participants while they listened to music that varied with respect to its perceived sexiness. As the perceived sexiness of the music increased, so did the subjective sexiness of the touch stimulations. In short, the perception of sexiness transferred from music to touch. Because sensual touch is key to mating behavior and relates to procreation, this association has implications for the universality and evolutionary significance of music. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Fritz, Thomas Hans: Max Planck Institute for Human Cognitive and Brain Sciences, Stephanstrasse 1A, Leipzig, Germany, 04103, fritz@cbs.mpg.de},
  doi       = {10.1037/xge0000329},
  issn      = {1939-2222(Electronic),0096-3445(Print)},
  journal   = {Journal of Experimental Psychology: General},
  keywords  = {*Emotions, *Meaning, *Music Perception, *Sexual Attitudes, *Tactual Perception, Human Courtship, Intention, Music, Agency},
  pages     = {1360--1365},
  publisher = {American Psychological Association},
  refid     = {2017-36462-003},
  volume    = {146},
}

@Misc{Spatola2021,
  author    = {Spatola, Nicolas and Wudarczyk, Olga A.},
  title     = {Ascribing emotions to robots: Explicit and implicit attribution of emotions and perceived robot anthropomorphism.},
  year      = {2021},
  abstract  = {Despite growing literature on various aspects of robot perception by humans, little is still known about how robots' anthropomorphism is perceived at various degrees of conscious perception. In the current study, in two experiments, we assessed whether explicit and implicit attributions of primary and secondary emotions towards robots predict participants' perception of robots' anthropomorphism. In Experiment 1, participants explicitly evaluated whether robots were likely to experience different primary and secondary emotions, followed by the assessment of the level of attributed anthropomorphism towards robots. In Experiment 2, participants completed an adapted version of the Implicit Association Test (IAT), which aimed at implicit evaluation of participants' attribution of different primary and secondary emotions to robots, followed by the assessment of the level of attributed robot anthropomorphism. At the explicit level, attribution of secondary emotions to robots was linked to higher perception of robots' warmth and competence. At the implicit level, participants’ perception of conceptual similarity between robots and humans (i.e. lower difference between attribution of primary and secondary emotions to humans and robots), predicted more anthropomorphic warmth perception and less discomfort towards robots. The current results reveal that implicit and explicit attribution of primary/secondary emotions towards robots predict specific aspects of robot anthropomorphism, with the warmth dimension consistently being predicted across both levels of evaluation. These results shed light on a novel approach to predict various aspects of robot anthropomorphism, which in the future might be useful for evaluations of social robots for their suitability in human-robot interactions. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Spatola, Nicolas: Istituto Italiano di Tecnologia, Center for Human Technologies, Via Morego, 30, Genova, Italy, 16163, nicolas.spatola@iit.it},
  doi       = {10.1016/j.chb.2021.106934},
  issn      = {1873-7692(Electronic),0747-5632(Print)},
  journal   = {Computers in Human Behavior},
  keywords  = {*Emotions, *Anthropomorphism, *Human Robot Interaction, *Social Robotics, *Cognitive Computing, Attribution, Physical Comfort, Robotics, Implicit Attitudes, Online Surveys, Implicit Measures},
  publisher = {Elsevier Science},
  refid     = {2021-71085-001},
  volume    = {124},
}

@Misc{Broekens2015,
  author    = {Broekens, Joost and Jacobs, Elmer and Jonker, Catholijn M.},
  title     = {A reinforcement learning model of joy, distress, hope and fear.},
  year      = {2015},
  abstract  = {In this paper we computationally study the relation between adaptive behaviour and emotion. Using the reinforcement learning framework, we propose that learned state utility, V(s), models fear (negative) and hope (positive) based on the fact that both signals are about anticipation of loss or gain. Further, we propose that joy/distress is a signal similar to the error signal. We present agent-based simulation experiments that show that this model replicates psychological and behavioural dynamics of emotion. This work distinguishes itself by assessing the dynamics of emotion in an adaptive agent framework--coupling it to the literature on habituation, development, extinction and hope theory. Our results support the idea that the function of emotion is to provide a complex feedback signal for an organism to adapt its behaviour. Our work is relevant for understanding the relation between emotion and adaptation in animals, as well as for human-robot interaction, in particular how emotional signals can be used to communicate between adaptive agents and humans. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Broekens, Joost: joost.broekens@gmail.com},
  doi       = {10.1080/09540091.2015.1031081},
  issn      = {1360-0494(Electronic),0954-0091(Print)},
  journal   = {Connection Science},
  keywords  = {*Emotional States, *Learning, *Reinforcement, *Affective Valence, *Affective Computing, Distress, Fear, Happiness, Hope},
  pages     = {215--233},
  publisher = {Taylor & Francis},
  refid     = {2015-45612-002},
  volume    = {27},
}

@Misc{Tsiourti2019,
  author    = {Tsiourti, Christiana and Weiss, Astrid and Wac, Katarzyna and Vincze, Markus},
  title     = {Multimodal integration of emotional signals from voice, body, and context: Effects of (in)congruence on emotion recognition and attitudes towards robots.},
  year      = {2019},
  abstract  = {Humanoid social robots have an increasingly prominent place in today’s world. Their acceptance in social and emotional human-robot interaction (HRI) scenarios depends on their ability to convey well recognized and believable emotional expressions to their human users. In this article, we incorporate recent findings from psychology, neuroscience, human-computer interaction, and HRI, to examine how people recognize and respond to emotions displayed by the body and voice of humanoid robots, with a particular emphasis on the effects of incongruence. In a social HRI laboratory experiment, we investigated contextual incongruence (i.e., the conflict situation where a robot’s reaction is incongrous with the socio-emotional context of the interaction) and cross-modal incongruence (i.e., the conflict situation where an observer receives incongruous emotional information across the auditory (vocal prosody) and visual (whole-body expressions) modalities). Results showed that both contextual incongruence and cross-modal incongruence confused observers and decreased the likelihood that they accurately recognized the emotional expressions of the robot. This, in turn, gives the impression that the robot is unintelligent or unable to express “empathic” behaviour and leads to profoundly harmful effects on likability and believability. Our findings reinforce the need of proper design of emotional expressions for robots that use several channels to communicate their emotional states in a clear and effective way. We offer recommendations regarding design choices and discuss future research areas in the direction of multimodal HRI. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  address   = {Tsiourti, Christiana: Automation and Control Institute (ACIN), Vision4Robotics Group, TU Wien, Guhausstrae 27, Vienna, Austria, 1040, christiana.tsiourti@tuwien.ac.at},
  doi       = {10.1007/s12369-019-00524-z},
  issn      = {1875-4805(Electronic),1875-4791(Print)},
  journal   = {International Journal of Social Robotics},
  keywords  = {*Conflict, *Emotionality (Personality), *Robotics, *Social Integration, *Voice, Attitudes, Social Acceptance, Emotion Recognition},
  pages     = {555--573},
  publisher = {Springer},
  refid     = {2019-06836-001},
  volume    = {11},
}

@Misc{Argyropoulos2020,
  author    = {Argyropoulos, Georgios P. D. and van Dun, Kim and Adamaszek, Michael and Leggio, Maria and Manto, Mario and Masciullo, Marcella and Molinari, Marco and Stoodley, Catherine J. and Van Overwalle, Frank and Ivry, Richard B. and Schmahmann, Jeremy D.},
  title     = {The cerebellar cognitive affective/schmahmann syndrome: A task force paper.},
  year      = {2020},
  abstract  = {Sporadically advocated over the last two centuries, a cerebellar role in cognition and affect has been rigorously established in the past few decades. In the clinical domain, such progress is epitomized by the “cerebellar cognitive affective syndrome” (“CCAS”) or “Schmahmann syndrome.” Introduced in the late 1990s, CCAS reflects a constellation of cerebellar-induced sequelae, comprising deficits in executive function, visuospatial cognition, emotion-affect, and language, over and above speech. The CCAS thus offers excellent grounds to investigate the functional topography of the cerebellum, and, ultimately, illustrate the precise mechanisms by which the cerebellum modulates cognition and affect. The primary objective of this task force paper is thus to stimulate further research in this area. After providing an up-to-date overview of the fundamental findings on cerebellar neurocognition, the paper substantiates the concept of CCAS with recent evidence from different scientific angles, promotes awareness of the CCAS as a clinical entity, and examines our current insight into the therapeutic options available. The paper finally identifies topics of divergence and outstanding questions for further research. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  address   = {Argyropoulos, Georgios P. D.: georgios.argyropoulos@ndcn.ox.ac.uk},
  doi       = {10.1007/s12311-019-01068-8},
  issn      = {1473-4230(Electronic),1473-4222(Print)},
  journal   = {The Cerebellum},
  keywords  = {*Cerebellum, *Emotions, *Syndromes, *Executive Function, *Sequelae, Oral Communication, Topography, Visuospatial Ability},
  pages     = {102--125},
  publisher = {Springer},
  refid     = {2019-56218-001},
  volume    = {19},
}

@Misc{Desideri2019,
  author    = {Desideri, Lorenzo and Ottaviani, Cristina and Malavasi, Massimiliano and di Marzio, Roberto and Bonifacci, Paola},
  title     = {Emotional processes in human-robot interaction during brief cognitive testing.},
  year      = {2019},
  abstract  = {With the rapid rise in robot presence in a variety of life domains, understanding how robots influence people's emotions during human-robot interactions is important for ensuring their acceptance in society. Mental health care, in particular, is considered the field in which robotics technology will bring the most dramatic changes in the near future. In this context, the present study sought to determine whether a brief cognitive assessment conducted by a robot elicited different interaction-related emotional processes than a traditional assessment conducted by an expert clinician. A non-clinical sample of 29 young adults (17 females; M = 24.5, SD = 2.3 years) were asked to complete two cognitive tasks twice, in counterbalanced order, once administered by an expert clinician and once by an autonomous humanoid robot. Self-reported measures of affective states and assessment of physiological arousal did not reveal any difference in emotional processes between human-human and human-robot interactions. Similarly, cognitive performances and workload did not differ across conditions. Analysis of non-verbal behaviour, however, showed that participants spent more time looking at the robot (d = 1.3) and made fewer gaze aversions (d = 1.3) in interacting with the robot than with the human examiner. We argue that, far from being a trivial ‘cosmetic change’, using a social robot in place of traditional testing could be a potential way to open up the development of a new generation of tests for brief cognitive assessment. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Desideri, Lorenzo: Via Sant’Isaia, 90, Bologna, Italy, 40123, lorenzo.desideri2@unibo.it},
  doi       = {10.1016/j.chb.2018.08.013},
  issn      = {1873-7692(Electronic),0747-5632(Print)},
  journal   = {Computers in Human Behavior},
  keywords  = {*Cognitive Assessment, *Emotional Responses, *Human Computer Interaction, *Robotics, Human Robot Interaction, Social Robotics},
  pages     = {331--342},
  publisher = {Elsevier Science},
  refid     = {2018-40500-001},
  volume    = {90},
}

@Misc{Hertz2019,
  author    = {Hertz, Nicholas and Wiese, Eva},
  title     = {Good advice is beyond all price, but what if it comes from a machine?},
  year      = {2019},
  abstract  = {As nonhuman agents are integrated into the workforce, the question becomes to what extent advice seeking in technology-infused environments depends on the perceived fit between agent and task and whether humans are willing to consider advice from nonhuman agents. In this experiment, participants sought advice from human, robot, or computer agents when performing a social or analytical task, with the task being either known or unknown when selecting an agent. In the agent-1st condition, participants 1st chose an adviser and then got their task assignment; in the task-1st condition, participants 1st received the task assignment and then chose an adviser. In the agent-1st condition, we expected participants to prefer human to nonhuman advisers and to subsequently comply more with their advice when they were assigned the social as opposed to the analytical task. In the task-1st condition, we expected advice seeking and compliance to be guided by stereotypical assumptions regarding an agent’s task expertise. The findings indicate that the human was chosen more often than were the nonhuman agents in the agent-1st condition, whereas adviser choices were calibrated based on perceived agent-task fit in the task-1st condition. Compliance rates were not generally calibrated based on agent-task fit. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  address   = {Hertz, Nicholas: Psychology Department, George Mason University, 4400 University Drive, Fairfax, VA, US, nhertz@gmu.edu},
  doi       = {10.1037/xap0000205},
  issn      = {1939-2192(Electronic),1076-898X(Print)},
  journal   = {Journal of Experimental Psychology: Applied},
  keywords  = {*Computers, *Decision Making, *Human Computer Interaction, *Robotics, *Trust (Social Behavior), Compliance, Experience Level, Test Construction},
  pages     = {386--395},
  publisher = {American Psychological Association},
  refid     = {2019-04354-001},
  volume    = {25},
}

@Misc{Carr2017,
  author    = {Carr, Evan W. and Hofree, Galit and Sheldon, Kayla and Saygin, Ayse P. and Winkielman, Piotr},
  title     = {Is that a human? Categorization (dis)fluency drives evaluations of agents ambiguous on human-likeness.},
  year      = {2017},
  abstract  = {A fundamental and seemingly unbridgeable psychological boundary divides humans and nonhumans. Essentialism theories suggest that mixing these categories violates “natural kinds.” Perceptual theories propose that such mixing creates incompatible cues. Most theories suggest that mixed agents, with both human and nonhuman features, obligatorily elicit discomfort. In contrast, we demonstrate top-down, cognitive control of these effects--such that the discomfort with mixed agents is partially driven by disfluent categorization of ambiguous features that are pertinent to the agent. Three experiments tested this idea. Participants classified 3 different agents (humans, androids, and robots) either on the human-likeness or control dimension and then evaluated them. Classifying on the human-likeness dimensions made the mixed agent (android) more disfluent, and in turn, more disliked. Disfluency also mediated the negative affective reaction. Critically, devaluation only resulted from disfluency on human-likeness--and not from an equally disfluent color dimension. We argue that negative consequences on evaluations of mixed agents arise from integral disfluency (on features that are relevant to the judgment at-hand, like ambiguous human-likeness). In contrast, no negative effects stem from incidental disfluency (on features that do not bear on the current judgment, like ambiguous color backgrounds). Overall, these findings support a top-down account of why, when, and how mixed agents elicit conflict and discomfort. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  address   = {Carr, Evan W.: Columbia Business School, 3022 Broadway, 7L Uris, New York, NY, US, 10027, ewcarr@ucsd.edu},
  doi       = {10.1037/xhp0000304},
  issn      = {1939-1277(Electronic),0096-1523(Print)},
  journal   = {Journal of Experimental Psychology: Human Perception and Performance},
  keywords  = {*Cognitive Processes, *Emotions, *Human Computer Interaction, *Robotics, *Cognitive Control, Classification (Cognitive Process), Decision Making, Judgment},
  pages     = {651--666},
  publisher = {American Psychological Association},
  refid     = {2017-03602-001},
  volume    = {43},
}

@Misc{Destephe2015,
  author    = {Destephe, Matthieu and Brandao, Martim and Kishi, Tatsuhiro and Zecca, Massimiliano and Hashimoto, Kenji and Takanishi, Atsuo},
  title     = {Walking in the uncanny valley: Importance of the attractiveness on the acceptance of a robot as a working partner.},
  year      = {2015},
  abstract  = {The Uncanny valley hypothesis, which tells us that almost-human characteristics in a robot or a device could cause uneasiness in human observers, is an important research theme in the Human Robot Interaction (HRI) field. Yet, that phenomenon is still not well-understood. Many have investigated the external design of humanoid robot faces and bodies but only a few studies have focused on the influence of robot movements on our perception and feelings of the Uncanny valley. Moreover, no research has investigated the possible relation between our uneasiness feeling and whether or not we would accept robots having a job in an office, a hospital or elsewhere. To better understand the Uncanny valley, we explore several factors which might have an influence on our perception of robots, be it related to the subjects, such as culture or attitude toward robots, or related to the robot such as emotions and emotional intensity displayed in its motion. We asked 69 subjects (N = 69) to rate the motions of a humanoid robot (Perceived Humanity, Eeriness, and Attractiveness) and state where they would rather see the robot performing a task. Our results suggest that, among the factors we chose to test, the attitude toward robots is the main influence on the perception of the robot related to the Uncanny valley. Robot occupation acceptability was affected only by Attractiveness, mitigating any Uncanny valley effect. We discuss the implications of these findings for the Uncanny valley and the acceptability of a robotic worker in our society. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Destephe, Matthieu: Department of Modern Mechanical Engineering, Waseda University, Kikuicho Campus 41-5-3F-03/4, 17 Kikuicho, Shinjuku, Tokyo, Japan, matthieu@takanishi.mech.waseda.ac.jp},
  doi       = {10.3389/fpsyg.2015.00204},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Cross Cultural Differences, *Emotions, *Robotics, Physical Attractiveness},
  publisher = {Frontiers Media S.A.},
  refid     = {2016-24225-001},
  volume    = {6},
}

@Misc{Ferrey2015,
  author    = {Ferrey, Anne E. and Burleigh, Tyler J. and Fenske, Mark J.},
  title     = {Stimulus-category competition, inhibition, and affective devaluation: A novel account of the uncanny valley.},
  year      = {2015},
  abstract  = {Stimuli that resemble humans, but are not perfectly human-like, are disliked compared to distinctly human and non-human stimuli. Accounts of this “Uncanny Valley” effect often focus on how changes in human resemblance can evoke different emotional responses. We present an alternate account based on the novel hypothesis that the Uncanny Valley is not directly related to ‘human-likeness’ per se, but instead reflects a more general form of stimulus devaluation that occurs when inhibition is triggered to resolve conflict between competing stimulus-related representations. We consider existing support for this inhibitory-devaluation hypothesis and further assess its feasibility through tests of two corresponding predictions that arise from the link between conflict-resolving inhibition and aversive response: (1) that the pronounced disliking of Uncanny-type stimuli will occur for any image that strongly activates multiple competing stimulus representations, even in the absence of any human-likeness, and (2) that the negative peak of an ‘Uncanny Valley’ should occur at the point of greatest stimulus-related conflict and not (in the presence of human-likeness) always closer to the ‘human’ end of a perceptual continuum. We measured affective responses to a set of line drawings representing non-human animal-animal morphs, in which each continuum midpoint was a bistable image (Experiment 1), as well as to sets of human-robot and human-animal computer-generated morphs (Experiment 2). Affective trends depicting classic Uncanny Valley functions occurred for all continua, including the nonhuman stimuli. Images at continua midpoints elicited significantly more negative affect than images at endpoints, even when the continua included a human endpoint. This illustrates the feasibility of the inhibitory-devaluation hypothesis and the need for further research into the possibility that the strong dislike of Uncanny-type stimuli reflects the negative affective consequences of cognitive inhibition. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  address   = {Fenske, Mark J.: Department of Psychology, University of Guelph, Guelph, ON, Canada, N1G 2W1, mfenske@uoguelph.ca},
  doi       = {10.3389/fpsyg.2015.00249},
  issn      = {1664-1078(Electronic)},
  journal   = {Frontiers in Psychology},
  keywords  = {*Cognitive Dissonance, *Cognitive Processes, *Emotional Responses, *Perceptual Stimulation, *Visual Perception, Conflict},
  publisher = {Frontiers Media S.A.},
  refid     = {2015-34052-001},
  volume    = {6},
}

@Comment{jabref-meta: databaseType:bibtex;}
